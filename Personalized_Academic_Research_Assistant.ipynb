{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ccd810977aff4e36b22941f81a664764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_015c96fa392645b09ed3a5bff868dbce",
              "IPY_MODEL_bec78f1d2bf1400a91ff08c302ff4809",
              "IPY_MODEL_951a75647b4049c78a8f2aa1eaa952ba"
            ],
            "layout": "IPY_MODEL_8b5135dbace34a848660e7aa708606d4"
          }
        },
        "015c96fa392645b09ed3a5bff868dbce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_956c0a3e67d64f11b80d1cd86f6953d0",
            "placeholder": "​",
            "style": "IPY_MODEL_02eeb6a388524180af15734587914f18",
            "value": "modules.json: 100%"
          }
        },
        "bec78f1d2bf1400a91ff08c302ff4809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f610ba908674f9b823f44dd66941899",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5db03be6c0e0484796939a49b76342fd",
            "value": 349
          }
        },
        "951a75647b4049c78a8f2aa1eaa952ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e6e353fc2c4402996e0ece6c9f8e5f3",
            "placeholder": "​",
            "style": "IPY_MODEL_57bafc2f9d39427dbf4cdba75e328aba",
            "value": " 349/349 [00:00&lt;00:00, 20.3kB/s]"
          }
        },
        "8b5135dbace34a848660e7aa708606d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "956c0a3e67d64f11b80d1cd86f6953d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02eeb6a388524180af15734587914f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f610ba908674f9b823f44dd66941899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5db03be6c0e0484796939a49b76342fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e6e353fc2c4402996e0ece6c9f8e5f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57bafc2f9d39427dbf4cdba75e328aba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e9f6565775a43e2a260c933850fbe96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d39cbd9b4354b15853513e5e1b2c128",
              "IPY_MODEL_91b96ce1fd384a0ebcbefb54ec86164b",
              "IPY_MODEL_fc1ef955bdea49f0a7fa229dfddeea8c"
            ],
            "layout": "IPY_MODEL_a62873a12afe43c3a684b58960d4df14"
          }
        },
        "2d39cbd9b4354b15853513e5e1b2c128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f3e3e05b2fe4c538d7b1ef832c139f3",
            "placeholder": "​",
            "style": "IPY_MODEL_3f81c9efaf9f4db389cbd5fb0bc9c818",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "91b96ce1fd384a0ebcbefb54ec86164b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1ae7ef4189442c58573d511906b4953",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42d7ccdd24a947639c09ba8055eca79c",
            "value": 116
          }
        },
        "fc1ef955bdea49f0a7fa229dfddeea8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5723c3aba9043a89ec35039a93cb15b",
            "placeholder": "​",
            "style": "IPY_MODEL_9e0f6c0c9dab429395e874874ab30a0d",
            "value": " 116/116 [00:00&lt;00:00, 5.71kB/s]"
          }
        },
        "a62873a12afe43c3a684b58960d4df14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3e3e05b2fe4c538d7b1ef832c139f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f81c9efaf9f4db389cbd5fb0bc9c818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1ae7ef4189442c58573d511906b4953": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42d7ccdd24a947639c09ba8055eca79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5723c3aba9043a89ec35039a93cb15b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e0f6c0c9dab429395e874874ab30a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aba576a7e9e74d00995e9727b7d5fb9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7ca3d74b23a4169a00eff976ab4e0d4",
              "IPY_MODEL_741dfcf62025495884a7016975751661",
              "IPY_MODEL_45c93db61dcc44c4bee432046b2c699b"
            ],
            "layout": "IPY_MODEL_699165a630aa4b8e9cb9bc55a50d474d"
          }
        },
        "d7ca3d74b23a4169a00eff976ab4e0d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d9d5ae6998748378ac43f22abb9f00f",
            "placeholder": "​",
            "style": "IPY_MODEL_9ee242c7a84a4c3399e608ef308f670e",
            "value": "README.md: 100%"
          }
        },
        "741dfcf62025495884a7016975751661": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed30e22f807e4bb495ae6d65bca72e94",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fad340d07f174c46833ed4f75acebb55",
            "value": 10659
          }
        },
        "45c93db61dcc44c4bee432046b2c699b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6704ef11432c45369f87791edff59e97",
            "placeholder": "​",
            "style": "IPY_MODEL_1c0dbd967eda4f179923d70c7068963d",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 649kB/s]"
          }
        },
        "699165a630aa4b8e9cb9bc55a50d474d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d9d5ae6998748378ac43f22abb9f00f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ee242c7a84a4c3399e608ef308f670e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed30e22f807e4bb495ae6d65bca72e94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fad340d07f174c46833ed4f75acebb55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6704ef11432c45369f87791edff59e97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c0dbd967eda4f179923d70c7068963d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8538e490d4ea41fba1f10e1000c34c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1dabc0d0daf24fb7bcac405b160d3fb8",
              "IPY_MODEL_1aa3828f3aad4336a10a8f094a0a9cdb",
              "IPY_MODEL_1e9823864033440ab065b19a18dd6bdb"
            ],
            "layout": "IPY_MODEL_2784d4a33ba34117850edf485ea14460"
          }
        },
        "1dabc0d0daf24fb7bcac405b160d3fb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ca142aa8dd042958ff0b652cb90b528",
            "placeholder": "​",
            "style": "IPY_MODEL_f67e7b41da8b4cf29e21d6073426d13a",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "1aa3828f3aad4336a10a8f094a0a9cdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29631e282963437c8b0a09337a6000d7",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8385a88289c43588296fa7e7598b6d6",
            "value": 53
          }
        },
        "1e9823864033440ab065b19a18dd6bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fadef56086554e89bb8a18aaecbc8bd3",
            "placeholder": "​",
            "style": "IPY_MODEL_262a954787964d28943a148f1aabf78d",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.39kB/s]"
          }
        },
        "2784d4a33ba34117850edf485ea14460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ca142aa8dd042958ff0b652cb90b528": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f67e7b41da8b4cf29e21d6073426d13a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29631e282963437c8b0a09337a6000d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8385a88289c43588296fa7e7598b6d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fadef56086554e89bb8a18aaecbc8bd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "262a954787964d28943a148f1aabf78d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01be710bc46e4feb812204019152a181": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_163491a1435f49aebfe62a4c30cf588b",
              "IPY_MODEL_2078ed8477c74b549ad6ec31074ec6f6",
              "IPY_MODEL_c748bfdf1b354facbe1816a365bf298f"
            ],
            "layout": "IPY_MODEL_ecb06e81b4e446edae1c8fef6870a997"
          }
        },
        "163491a1435f49aebfe62a4c30cf588b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_875c3f35fb0f42808d1d9dc8f6c8c5da",
            "placeholder": "​",
            "style": "IPY_MODEL_8710b9fb793a47379e70c9915a59f63b",
            "value": "config.json: 100%"
          }
        },
        "2078ed8477c74b549ad6ec31074ec6f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_662f05663c3d460991414891e2bcf0a2",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d809c05b0ed47fa9eb12900cff541ac",
            "value": 612
          }
        },
        "c748bfdf1b354facbe1816a365bf298f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a42189b8fb884e47a0e0e7ca2a4dcac5",
            "placeholder": "​",
            "style": "IPY_MODEL_4c3ea191f02c4b31b3101a1bcd34199b",
            "value": " 612/612 [00:00&lt;00:00, 46.6kB/s]"
          }
        },
        "ecb06e81b4e446edae1c8fef6870a997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "875c3f35fb0f42808d1d9dc8f6c8c5da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8710b9fb793a47379e70c9915a59f63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "662f05663c3d460991414891e2bcf0a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d809c05b0ed47fa9eb12900cff541ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a42189b8fb884e47a0e0e7ca2a4dcac5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c3ea191f02c4b31b3101a1bcd34199b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98c340f017b64c5f9a81569163953e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14e3c40ac2494c24bef7eb1c5b925f5c",
              "IPY_MODEL_44402fbb61a64919be96d53508fd650e",
              "IPY_MODEL_2cafad12786944e2b7133e78e883daba"
            ],
            "layout": "IPY_MODEL_9c43340f53e94788adf2e166a069159f"
          }
        },
        "14e3c40ac2494c24bef7eb1c5b925f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8596f99da4e4bec900a72e984f2e7a1",
            "placeholder": "​",
            "style": "IPY_MODEL_f2283a2378f34cb9b62e8135b338bdd9",
            "value": "model.safetensors: 100%"
          }
        },
        "44402fbb61a64919be96d53508fd650e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a58308a67ac471d80bc0d54c3585f8e",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be675f5885ac43c28284e99279654476",
            "value": 90868376
          }
        },
        "2cafad12786944e2b7133e78e883daba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd65d563759c43bab8d28f6589399e86",
            "placeholder": "​",
            "style": "IPY_MODEL_ffedbbc5b3224b71a3f7e53241ff7f32",
            "value": " 90.9M/90.9M [00:04&lt;00:00, 37.2MB/s]"
          }
        },
        "9c43340f53e94788adf2e166a069159f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8596f99da4e4bec900a72e984f2e7a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2283a2378f34cb9b62e8135b338bdd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a58308a67ac471d80bc0d54c3585f8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be675f5885ac43c28284e99279654476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd65d563759c43bab8d28f6589399e86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffedbbc5b3224b71a3f7e53241ff7f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe7b3d78c3414474ad38346c0f3b1de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b7098322a8f4917baca2548d5de34d2",
              "IPY_MODEL_a73a1871dfe64e369d84d57eb5e3ecbe",
              "IPY_MODEL_c4ad9eb919d14505823ee5f5d6dccfb6"
            ],
            "layout": "IPY_MODEL_10bc5d87f19145f6819cc660f25f797b"
          }
        },
        "3b7098322a8f4917baca2548d5de34d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_112d9bd7abbc488b9490277f7ec5c608",
            "placeholder": "​",
            "style": "IPY_MODEL_936a004588264d92bc2b91c9e160b6ee",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "a73a1871dfe64e369d84d57eb5e3ecbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beebcd05cb244b03a58dc97557bb4d99",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8eb1f3ddc9334e9b924ec4ef7abc65e5",
            "value": 350
          }
        },
        "c4ad9eb919d14505823ee5f5d6dccfb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d9b1aa0f00d4d118c470bbabafa53c9",
            "placeholder": "​",
            "style": "IPY_MODEL_7f45308dec824f03a4f6e5da673c6594",
            "value": " 350/350 [00:00&lt;00:00, 20.1kB/s]"
          }
        },
        "10bc5d87f19145f6819cc660f25f797b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "112d9bd7abbc488b9490277f7ec5c608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "936a004588264d92bc2b91c9e160b6ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "beebcd05cb244b03a58dc97557bb4d99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eb1f3ddc9334e9b924ec4ef7abc65e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d9b1aa0f00d4d118c470bbabafa53c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f45308dec824f03a4f6e5da673c6594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1707c98fb9b54709a43c779a3b068daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bac89e8a09244410912f4a1f2b8858c8",
              "IPY_MODEL_12a77f5ac1f24861926dc58cc3cc3f97",
              "IPY_MODEL_9e5e1e7e80004cd2b4a07ec26a62fd64"
            ],
            "layout": "IPY_MODEL_4b34097c53cc41dba9c638a1ad68ea68"
          }
        },
        "bac89e8a09244410912f4a1f2b8858c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d3293d7a2324f73a8695e5d7a8995a3",
            "placeholder": "​",
            "style": "IPY_MODEL_25dc09fb82eb486f98dc22010ce24423",
            "value": "vocab.txt: 100%"
          }
        },
        "12a77f5ac1f24861926dc58cc3cc3f97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_672752a0a3014c11aeffd70897a650bb",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7160c25bce3a45409976457793e795a6",
            "value": 231508
          }
        },
        "9e5e1e7e80004cd2b4a07ec26a62fd64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbf167dbb15d4b77a8bb2409bdfdab7f",
            "placeholder": "​",
            "style": "IPY_MODEL_cca3ebddbd9f420ca425daf3bd8db5bb",
            "value": " 232k/232k [00:00&lt;00:00, 9.72MB/s]"
          }
        },
        "4b34097c53cc41dba9c638a1ad68ea68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d3293d7a2324f73a8695e5d7a8995a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25dc09fb82eb486f98dc22010ce24423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "672752a0a3014c11aeffd70897a650bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7160c25bce3a45409976457793e795a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbf167dbb15d4b77a8bb2409bdfdab7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cca3ebddbd9f420ca425daf3bd8db5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a18f5eeaa53411ab5ee709a834096b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e076d0594c7b415294b3a83344b92fdd",
              "IPY_MODEL_7690ec64ee5a404c988911c735de0f57",
              "IPY_MODEL_a424b15459d54475a500f8f9df0cf35e"
            ],
            "layout": "IPY_MODEL_1d301535b1e24f54b82c54b2050d5d81"
          }
        },
        "e076d0594c7b415294b3a83344b92fdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f99a798834d4f34b690282b040d0856",
            "placeholder": "​",
            "style": "IPY_MODEL_54c76d2ec1da450080770071581373c1",
            "value": "tokenizer.json: 100%"
          }
        },
        "7690ec64ee5a404c988911c735de0f57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d60c86f42c324740accba4e799abfecc",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d7a61e58ab04619ba1d470e41583b16",
            "value": 466247
          }
        },
        "a424b15459d54475a500f8f9df0cf35e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78a4197df4764132953e386fc3b5e648",
            "placeholder": "​",
            "style": "IPY_MODEL_6df0f4fd03eb44d699721ab35cee30ff",
            "value": " 466k/466k [00:00&lt;00:00, 14.6MB/s]"
          }
        },
        "1d301535b1e24f54b82c54b2050d5d81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f99a798834d4f34b690282b040d0856": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54c76d2ec1da450080770071581373c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d60c86f42c324740accba4e799abfecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d7a61e58ab04619ba1d470e41583b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78a4197df4764132953e386fc3b5e648": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6df0f4fd03eb44d699721ab35cee30ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83971072b55f43f1ae80c5de4328a22c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c06627c1d674a84b7126426423df22e",
              "IPY_MODEL_69db3b67c83d435ab6691fa85ef52dad",
              "IPY_MODEL_751416d1cee049a59c5235f73560613c"
            ],
            "layout": "IPY_MODEL_173536b69d514e1997bdecdeeb6023e1"
          }
        },
        "3c06627c1d674a84b7126426423df22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d48dd256cfb451baf5ef53e782d28e9",
            "placeholder": "​",
            "style": "IPY_MODEL_a2b052b6feea4a0fb35aa9afaa1fdaa7",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "69db3b67c83d435ab6691fa85ef52dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdd82cbcffe849c1903f67528b390a74",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3dd235972f34622bf0152a2b1bd86f1",
            "value": 112
          }
        },
        "751416d1cee049a59c5235f73560613c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96b953d6b9334198b16bfd56acfa4ce1",
            "placeholder": "​",
            "style": "IPY_MODEL_cbed670feaf24548985afba796845fba",
            "value": " 112/112 [00:00&lt;00:00, 9.50kB/s]"
          }
        },
        "173536b69d514e1997bdecdeeb6023e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d48dd256cfb451baf5ef53e782d28e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2b052b6feea4a0fb35aa9afaa1fdaa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdd82cbcffe849c1903f67528b390a74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3dd235972f34622bf0152a2b1bd86f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96b953d6b9334198b16bfd56acfa4ce1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbed670feaf24548985afba796845fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a0abd5e63804187a66544a21c04242d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad89408186cb423d8624af1ccd58437a",
              "IPY_MODEL_3cc5ff13d7f84098991a94df50b025e1",
              "IPY_MODEL_aa01140e575e4fb1adde1a97e563d193"
            ],
            "layout": "IPY_MODEL_0d4722646606439e9addfb3f29979d44"
          }
        },
        "ad89408186cb423d8624af1ccd58437a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25a522ee0ec54325b70c00f0d5397ffa",
            "placeholder": "​",
            "style": "IPY_MODEL_b8daa0553b624470b241db503cc5e3f6",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "3cc5ff13d7f84098991a94df50b025e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e263bd2d8f174dfb800fe0abdd2dff6d",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76880089511d41a8b779a5efa5118da7",
            "value": 190
          }
        },
        "aa01140e575e4fb1adde1a97e563d193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_646f94414de44ca08bcbc5a898bc001f",
            "placeholder": "​",
            "style": "IPY_MODEL_fd699c4e161b47dd8da518979df3e7f9",
            "value": " 190/190 [00:00&lt;00:00, 7.68kB/s]"
          }
        },
        "0d4722646606439e9addfb3f29979d44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25a522ee0ec54325b70c00f0d5397ffa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8daa0553b624470b241db503cc5e3f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e263bd2d8f174dfb800fe0abdd2dff6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76880089511d41a8b779a5efa5118da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "646f94414de44ca08bcbc5a898bc001f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd699c4e161b47dd8da518979df3e7f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "378f09a0c24e4ad7b1a9b7a9cb830cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_818eb89c1a2a47a28814376264f4e398",
              "IPY_MODEL_eb8e7640ae2044feb652241d903cfb37",
              "IPY_MODEL_adeea763bb024c62b1145ae06dc470d5"
            ],
            "layout": "IPY_MODEL_a24a9c37a8634e6e8887d5db7da465c6"
          }
        },
        "818eb89c1a2a47a28814376264f4e398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8cdb0b859694d048417ebda64dfa903",
            "placeholder": "​",
            "style": "IPY_MODEL_c91a069c6bc14080b2781977e38006e5",
            "value": "config.json: 100%"
          }
        },
        "eb8e7640ae2044feb652241d903cfb37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9888e3d93a34b6ca6b66dfba142711f",
            "max": 1208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5271dd63aee64460b663720017dc71a5",
            "value": 1208
          }
        },
        "adeea763bb024c62b1145ae06dc470d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cecddcef45684b2d934d58c7b77e0166",
            "placeholder": "​",
            "style": "IPY_MODEL_cc98b649a49e4193a5a464c58eb7c93b",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 22.2kB/s]"
          }
        },
        "a24a9c37a8634e6e8887d5db7da465c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8cdb0b859694d048417ebda64dfa903": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c91a069c6bc14080b2781977e38006e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9888e3d93a34b6ca6b66dfba142711f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5271dd63aee64460b663720017dc71a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cecddcef45684b2d934d58c7b77e0166": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc98b649a49e4193a5a464c58eb7c93b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b96e9329e0f540a0af652b8fd9558710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c850a350678e4d75aee994d7d0542001",
              "IPY_MODEL_4b0b07aadcaf41d498bc05424ece1cad",
              "IPY_MODEL_aadea8868d8348618eb220fdad71530f"
            ],
            "layout": "IPY_MODEL_ea607c52e4454f4283745c042b1c724f"
          }
        },
        "c850a350678e4d75aee994d7d0542001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d72cf60824447b5bfd7fb67ee833907",
            "placeholder": "​",
            "style": "IPY_MODEL_06fb4db5bc1f40a0af04791be0a72197",
            "value": "spiece.model: 100%"
          }
        },
        "4b0b07aadcaf41d498bc05424ece1cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5ae7e3a71154fb09b3ad9137f78bc84",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1973fe281f4f4b70a6d48b8bb3c587ea",
            "value": 791656
          }
        },
        "aadea8868d8348618eb220fdad71530f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdf44c22b1094dd1955a088cce438509",
            "placeholder": "​",
            "style": "IPY_MODEL_5339e3edab084d5db73bac717ad7ba62",
            "value": " 792k/792k [00:00&lt;00:00, 4.80MB/s]"
          }
        },
        "ea607c52e4454f4283745c042b1c724f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d72cf60824447b5bfd7fb67ee833907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06fb4db5bc1f40a0af04791be0a72197": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5ae7e3a71154fb09b3ad9137f78bc84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1973fe281f4f4b70a6d48b8bb3c587ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdf44c22b1094dd1955a088cce438509": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5339e3edab084d5db73bac717ad7ba62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39340122f257475fb385c09400d96ec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_159d5cbeae514a64adee19eb3be7ef3e",
              "IPY_MODEL_97a672aa9ac4438aa79b951a490b7127",
              "IPY_MODEL_46b9d2bf19264da3bfe23914a6a8ee07"
            ],
            "layout": "IPY_MODEL_90b5ecaff4ca4c7ea85bfdaf90db2462"
          }
        },
        "159d5cbeae514a64adee19eb3be7ef3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b67437a93fc4472e8205c9c1e96ce195",
            "placeholder": "​",
            "style": "IPY_MODEL_f3636bd920994340a1496a68fe0f9e2b",
            "value": "tokenizer.json: 100%"
          }
        },
        "97a672aa9ac4438aa79b951a490b7127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_674061a1b100474ba1500c47520b2215",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2666dfc96355414a9f89839e5fbf693e",
            "value": 1389353
          }
        },
        "46b9d2bf19264da3bfe23914a6a8ee07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db8dcc13d46a40908a64bd8b06a6c7bd",
            "placeholder": "​",
            "style": "IPY_MODEL_2dcf4dcc1d2b4e75bb1ba348ea653996",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 6.29MB/s]"
          }
        },
        "90b5ecaff4ca4c7ea85bfdaf90db2462": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b67437a93fc4472e8205c9c1e96ce195": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3636bd920994340a1496a68fe0f9e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "674061a1b100474ba1500c47520b2215": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2666dfc96355414a9f89839e5fbf693e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db8dcc13d46a40908a64bd8b06a6c7bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dcf4dcc1d2b4e75bb1ba348ea653996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdc0996dc31248bba002cb5269c02df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c3a9596957e48cdbe8808ac762ad7e7",
              "IPY_MODEL_eaa48e8fdd9045f78f0bd177996b194e",
              "IPY_MODEL_343858b0afd445fd8b5bf2ab2b0347b1"
            ],
            "layout": "IPY_MODEL_b210d6fe81084b839d2593e80fa995b8"
          }
        },
        "8c3a9596957e48cdbe8808ac762ad7e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4966132c2523425eb5da497b9f66a61c",
            "placeholder": "​",
            "style": "IPY_MODEL_3ce58743e6644489a0c308c9063c45e5",
            "value": "model.safetensors: 100%"
          }
        },
        "eaa48e8fdd9045f78f0bd177996b194e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b69ef220262a4b82866114b652917c9b",
            "max": 891646390,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5105e058932942958f1c09a162237ef2",
            "value": 891646390
          }
        },
        "343858b0afd445fd8b5bf2ab2b0347b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ac0b2eee03a4588b1634263d92eb2d9",
            "placeholder": "​",
            "style": "IPY_MODEL_4b56a6221e2f4f76a537e0d0f746b7a6",
            "value": " 892M/892M [00:12&lt;00:00, 191MB/s]"
          }
        },
        "b210d6fe81084b839d2593e80fa995b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4966132c2523425eb5da497b9f66a61c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ce58743e6644489a0c308c9063c45e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b69ef220262a4b82866114b652917c9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5105e058932942958f1c09a162237ef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ac0b2eee03a4588b1634263d92eb2d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b56a6221e2f4f76a537e0d0f746b7a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d4c233461b74d38a810fa1ad83a54be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b95f711b50df44a3a607ef70219224ae",
              "IPY_MODEL_7fdf0bcf1bcc49d0826d0a61ad1c6e6a",
              "IPY_MODEL_3bd8e69b77f1447b99c4ec4714ed60c8"
            ],
            "layout": "IPY_MODEL_687bb6f07fc14bed945afcf130d7a012"
          }
        },
        "b95f711b50df44a3a607ef70219224ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7a52d54f461420cba7b8ad98b991975",
            "placeholder": "​",
            "style": "IPY_MODEL_ae57023746e940f9ab793ad5402883a7",
            "value": "generation_config.json: 100%"
          }
        },
        "7fdf0bcf1bcc49d0826d0a61ad1c6e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a34826ae523842be88b2e9bfe988df2f",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ebc0cd9d3af49b4a5232b571d06a8cd",
            "value": 147
          }
        },
        "3bd8e69b77f1447b99c4ec4714ed60c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67b084f8a3c547d4aee22eeb5ff45a2c",
            "placeholder": "​",
            "style": "IPY_MODEL_fdf47df7714c41f99365a4f24b38b6a2",
            "value": " 147/147 [00:00&lt;00:00, 9.23kB/s]"
          }
        },
        "687bb6f07fc14bed945afcf130d7a012": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7a52d54f461420cba7b8ad98b991975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae57023746e940f9ab793ad5402883a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a34826ae523842be88b2e9bfe988df2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ebc0cd9d3af49b4a5232b571d06a8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67b084f8a3c547d4aee22eeb5ff45a2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdf47df7714c41f99365a4f24b38b6a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the required libraries"
      ],
      "metadata": {
        "id": "maRiC6-Mrnea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjlQscb8qxd8",
        "outputId": "a96ad0ac-fc4f-49aa-8f88-73c1af2da1ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu, sentence-transformers\n",
            "Successfully installed faiss-cpu-1.8.0.post1 sentence-transformers-3.1.1\n",
            "Collecting scholarly\n",
            "  Downloading scholarly-1.7.11-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting arrow (from scholarly)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.12.3)\n",
            "Collecting bibtexparser (from scholarly)\n",
            "  Downloading bibtexparser-1.4.1.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated (from scholarly)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting fake-useragent (from scholarly)\n",
            "  Downloading fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting free-proxy (from scholarly)\n",
            "  Downloading free_proxy-1.1.2.tar.gz (5.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx (from scholarly)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting python-dotenv (from scholarly)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from scholarly) (2.32.3)\n",
            "Collecting selenium (from scholarly)\n",
            "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting sphinx-rtd-theme (from scholarly)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->scholarly) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->scholarly)\n",
            "  Downloading types_python_dateutil-2.9.0.20240906-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->scholarly) (2.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from bibtexparser->scholarly) (3.1.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->scholarly) (1.16.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from free-proxy->scholarly) (4.9.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx->scholarly)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->scholarly)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (2.2.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (1.7.1)\n",
            "Collecting trio~=0.17 (from selenium->scholarly)\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium->scholarly)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium->scholarly) (1.8.0)\n",
            "Requirement already satisfied: sphinx<8,>=5 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->scholarly) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->scholarly) (0.18.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->scholarly)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (3.1.4)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.16.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (24.1)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium->scholarly)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->scholarly)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.1.5)\n",
            "Downloading scholarly-1.7.11-py3-none-any.whl (39 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading types_python_dateutil-2.9.0.20240906-py3-none-any.whl (9.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: bibtexparser, free-proxy\n",
            "  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bibtexparser: filename=bibtexparser-1.4.1-py3-none-any.whl size=43253 sha256=04eee6314228fc1572598b7d44f4e4c6c66848212de0edf3a05ea17e2724eb92\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c6/c3/56e639fab68d1fdbf13ea147636d9795ccdbd3c1d3178d1332\n",
            "  Building wheel for free-proxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for free-proxy: filename=free_proxy-1.1.2-py3-none-any.whl size=5813 sha256=3c7683c11bf31db87330b46fc2d969318e3ee2a6d27458084b9d34b50f72a3da\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/db/d5/089b3eb5f67a2103a5136b9043156af7fb96fac4cae00fe006\n",
            "Successfully built bibtexparser free-proxy\n",
            "Installing collected packages: fake-useragent, types-python-dateutil, python-dotenv, outcome, h11, deprecated, bibtexparser, wsproto, trio, httpcore, free-proxy, arrow, trio-websocket, sphinxcontrib-jquery, httpx, sphinx-rtd-theme, selenium, scholarly\n",
            "Successfully installed arrow-1.3.0 bibtexparser-1.4.1 deprecated-1.2.14 fake-useragent-1.5.1 free-proxy-1.1.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 outcome-1.3.0.post0 python-dotenv-1.0.1 scholarly-1.7.11 selenium-4.25.0 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1 trio-0.26.2 trio-websocket-0.11.1 types-python-dateutil-2.9.0.20240906 wsproto-1.2.0\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.10-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.24.10 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.10-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.10-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.10-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.10 PyMuPDFb-1.24.10\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers faiss-cpu sentence-transformers pandas numpy\n",
        "!pip install scholarly\n",
        "!pip install PyMuPDF\n",
        "!pip install nltk\n",
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import fitz\n",
        "from scholarly import scholarly\n",
        "import re\n"
      ],
      "metadata": {
        "id": "NNw0lU2rq8mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GhOzshQq90J",
        "outputId": "2c78d33a-7c4b-4ff3-837e-4afc7436128c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_dirs = ['data/raw', 'data/processed', 'models', 'outputs']\n",
        "\n",
        "for dir in project_dirs:\n",
        "    os.makedirs(dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "XNcBTuAwq_rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration parameters\n",
        "EMBEDDING_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'  # Efficient for semantic search\n",
        "GENERATION_MODEL_NAME = 't5-base'\n",
        "TOP_K = 5  # Number of documents to retrieve\n",
        "CHUNK_SIZE = 500  # Number of words per document chunk\n",
        "DATA_SOURCES = ['arxiv', 'pubmed']\n"
      ],
      "metadata": {
        "id": "u-KY7Vz7rCX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection and Preprocessing"
      ],
      "metadata": {
        "id": "oRp3QN_Vrk_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HbaMovirI6S",
        "outputId": "c402a2c4-9090-4495-9d3e-eb43ca2abdd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2024.8.30)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=f1548b2033aac7250825e20c0048b37b5cdb0d48f6c59ab74c2d2a8466cc001a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv"
      ],
      "metadata": {
        "id": "G2Rzdp_QruwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define search parameters\n",
        "ARXIV_QUERY = \"machine learning\"\n",
        "MAX_RESULTS_ARXIV = 100  # Number of papers to fetch\n"
      ],
      "metadata": {
        "id": "atHPdG3ErxEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_arxiv_papers(query, max_results=100):\n",
        "    search = arxiv.Search(\n",
        "        query=query,\n",
        "        max_results=max_results,\n",
        "        sort_by=arxiv.SortCriterion.Relevance\n",
        "    )\n",
        "    papers = []\n",
        "    for result in search.results():\n",
        "        paper = {\n",
        "            'title': result.title,\n",
        "            'authors': [author.name for author in result.authors],\n",
        "            'abstract': result.summary,\n",
        "            'published': result.published.strftime('%Y-%m-%d'),\n",
        "            'pdf_url': result.pdf_url,\n",
        "            'categories': result.categories,\n",
        "            'id': result.get_short_id()\n",
        "        }\n",
        "        papers.append(paper)\n",
        "    return papers\n",
        "\n",
        "# Fetch ArXiv papers\n",
        "arxiv_papers = fetch_arxiv_papers(ARXIV_QUERY, MAX_RESULTS_ARXIV)\n",
        "print(f\"Fetched {len(arxiv_papers)} papers from ArXiv.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQrFAryfrzNx",
        "outputId": "b7f903cb-2d57-4777-cd9e-38529cac2ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-7769a2cad309>:8: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 100 papers from ArXiv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save ArXiv metadata to JSON\n",
        "with open('data/raw/arxiv_papers.json', 'w') as f:\n",
        "    json.dump(arxiv_papers, f, indent=2)\n"
      ],
      "metadata": {
        "id": "bCLrcyaNr4Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import requests\n"
      ],
      "metadata": {
        "id": "-oqGyYx_s6vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_pdf(url, save_path):\n",
        "    try:\n",
        "        response = requests.get(url, stream=True, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        with open(save_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(1024):\n",
        "                f.write(chunk)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download PDF from {url}: {e}\")\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "3oMJdgs9s-eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            text = \"\"\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract text from {pdf_path}: {e}\")\n",
        "        return \"\"\n"
      ],
      "metadata": {
        "id": "I_SK4kcAtAG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_arxiv_papers(papers, download_dir='data/raw/arxiv_pdfs/'):\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "    for paper in papers:\n",
        "        pdf_url = paper['pdf_url']\n",
        "        paper_id = paper['id']\n",
        "        pdf_path = os.path.join(download_dir, f\"{paper_id}.pdf\")\n",
        "\n",
        "        # Download PDF if not already downloaded\n",
        "        if not os.path.exists(pdf_path):\n",
        "            success = download_pdf(pdf_url, pdf_path)\n",
        "            if not success:\n",
        "                continue\n",
        "\n",
        "        # Extract text\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        if text:\n",
        "            paper['full_text'] = text\n",
        "        else:\n",
        "            paper['full_text'] = \"\"\n",
        "\n",
        "    return papers\n",
        "\n",
        "# Process ArXiv papers\n",
        "arxiv_papers = process_arxiv_papers(arxiv_papers)\n",
        "print(\"Completed downloading and extracting PDFs from ArXiv.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukAuNJgStBnB",
        "outputId": "ca86803f-c144-4ac6-c183-9e0964cd2380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download PDF from http://arxiv.org/pdf/2206.07090v2: 404 Client Error: Not Found for url: http://arxiv.org/pdf/2206.07090v2\n",
            "Failed to download PDF from http://arxiv.org/pdf/1607.02450v2: 500 Server Error: Internal Server Error for url: http://arxiv.org/pdf/1607.02450v2\n",
            "Failed to download PDF from http://arxiv.org/pdf/1207.4676v2: 500 Server Error: Internal Server Error for url: http://arxiv.org/pdf/1207.4676v2\n",
            "Completed downloading and extracting PDFs from ArXiv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save enriched ArXiv data to JSON\n",
        "with open('data/raw/arxiv_papers_full.json', 'w') as f:\n",
        "    json.dump(arxiv_papers, f, indent=2)\n"
      ],
      "metadata": {
        "id": "lLo04YKAtD6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ArXiv enriched data\n",
        "with open('data/raw/arxiv_papers_full.json', 'r') as f:\n",
        "    arxiv_papers_full = json.load(f)\n",
        "\n",
        "# Display the first ArXiv paper's information\n",
        "print(\"Sample ArXiv Paper:\")\n",
        "print(json.dumps(arxiv_papers_full[0], indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsiwfOTovURI",
        "outputId": "cf3bc107-7346-4bc9-c0cd-efd478b0df24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample ArXiv Paper:\n",
            "{\n",
            "  \"title\": \"Lecture Notes: Optimization for Machine Learning\",\n",
            "  \"authors\": [\n",
            "    \"Elad Hazan\"\n",
            "  ],\n",
            "  \"abstract\": \"Lecture notes on optimization for machine learning, derived from a course at\\nPrinceton University and tutorials given in MLSS, Buenos Aires, as well as\\nSimons Foundation, Berkeley.\",\n",
            "  \"published\": \"2019-09-08\",\n",
            "  \"pdf_url\": \"http://arxiv.org/pdf/1909.03550v1\",\n",
            "  \"categories\": [\n",
            "    \"cs.LG\",\n",
            "    \"stat.ML\"\n",
            "  ],\n",
            "  \"id\": \"1909.03550v1\",\n",
            "  \"full_text\": \"lecture notes:\\nOptimization for Machine Learning\\nversion 0.57\\nAll rights reserved.\\nElad Hazan 1\\n1www.cs.princeton.edu/~ehazan\\narXiv:1909.03550v1  [cs.LG]  8 Sep 2019\\nii\\nPreface\\nThis text was written to accompany a series of lectures given at the Machine\\nLearning Summer School Buenos Aires, following a lecture series at the\\nSimons Center for Theoretical Computer Science, Berkeley. It was extended\\nfor the course COS 598D - Optimization for Machine Learning, Princeton\\nUniversity, Spring 2019.\\nI am grateful to Paula Gradu for proofreading parts of this manuscript.\\nI\\u2019m also thankful for the help of the following students and colleagues for\\ncorrections and suggestions to this text: Udaya Ghai, John Hallman, No\\u00b4e\\nPion, Xinyi Chen.\\niii\\niv\\nPreface\\nFigure 1:\\nProfessor Arkadi Nemirovski, Pioneer of mathematical optimiza-\\ntion\\nContents\\nPreface\\niii\\n1\\nIntroduction\\n3\\n1.1\\nExamples of optimization problems in machine learning\\n. . .\\n4\\n1.1.1\\nEmpirical Risk Minimization\\n. . . . . . . . . . . . . .\\n4\\n1.1.2\\nMatrix completion and recommender systems . . . . .\\n6\\n1.1.3\\nLearning in Linear Dynamical Systems\\n. . . . . . . .\\n7\\n1.2\\nWhy is mathematical programming hard? . . . . . . . . . . .\\n8\\n1.2.1\\nThe computational model . . . . . . . . . . . . . . . .\\n8\\n1.2.2\\nHardness of constrained mathematical programming .\\n9\\n2\\nBasic concepts in optimization and analysis\\n11\\n2.1\\nBasic de\\ufb01nitions and the notion of convexity . . . . . . . . . .\\n11\\n2.1.1\\nProjections onto convex sets . . . . . . . . . . . . . . .\\n13\\n2.1.2\\nIntroduction to optimality conditions . . . . . . . . . .\\n14\\n2.1.3\\nSolution concepts for non-convex optimization\\n. . . .\\n15\\n2.2\\nPotentials for distance to optimality\\n. . . . . . . . . . . . . .\\n16\\n2.3\\nGradient descent and the Polyak stepsize\\n. . . . . . . . . . .\\n18\\n2.4\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n2.5\\nBibliographic remarks\\n. . . . . . . . . . . . . . . . . . . . . .\\n23\\n3\\nStochastic Gradient Descent\\n25\\n3.1\\nTraining feedforward neural networks\\n. . . . . . . . . . . . .\\n25\\n3.2\\nGradient descent for smooth optimization . . . . . . . . . . .\\n27\\n3.3\\nStochastic gradient descent\\n. . . . . . . . . . . . . . . . . . .\\n29\\n3.4\\nBibliographic remarks\\n. . . . . . . . . . . . . . . . . . . . . .\\n31\\n4\\nGeneralization and Non-Smooth Optimization\\n33\\n4.1\\nA note on non-smooth optimization\\n. . . . . . . . . . . . . .\\n34\\n4.2\\nMinimizing Regret . . . . . . . . . . . . . . . . . . . . . . . .\\n35\\nv\\nvi\\nCONTENTS\\n4.3\\nRegret implies generalization\\n. . . . . . . . . . . . . . . . . .\\n35\\n4.4\\nOnline gradient descent\\n. . . . . . . . . . . . . . . . . . . . .\\n36\\n4.5\\nLower bounds . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n4.6\\nOnline gradient descent for strongly convex functions . . . . .\\n39\\n4.7\\nOnline Gradient Descent implies SGD . . . . . . . . . . . . .\\n41\\n4.8\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n4.9\\nBibliographic remarks\\n. . . . . . . . . . . . . . . . . . . . . .\\n45\\n5\\nRegularization\\n47\\n5.1\\nMotivation: prediction from expert advice . . . . . . . . . . .\\n47\\n5.1.1\\nThe weighted majority algorithm . . . . . . . . . . . .\\n49\\n5.1.2\\nRandomized weighted majority . . . . . . . . . . . . .\\n51\\n5.1.3\\nHedge . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n5.2\\nThe Regularization framework\\n. . . . . . . . . . . . . . . . .\\n53\\n5.2.1\\nThe RFTL algorithm\\n. . . . . . . . . . . . . . . . . .\\n54\\n5.2.2\\nMirrored Descent . . . . . . . . . . . . . . . . . . . . .\\n55\\n5.2.3\\nDeriving online gradient descent\\n. . . . . . . . . . . .\\n56\\n5.2.4\\nDeriving multiplicative updates . . . . . . . . . . . . .\\n57\\n5.3\\nTechnical background: regularization functions . . . . . . . .\\n57\\n5.4\\nRegret bounds for Mirrored Descent\\n. . . . . . . . . . . . . .\\n59\\n5.5\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n5.6\\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . .\\n63\\n6\\nAdaptive Regularization\\n65\\n6.1\\nAdaptive Learning Rates: Intuition . . . . . . . . . . . . . . .\\n65\\n6.2\\nA Regularization Viewpoint\\n. . . . . . . . . . . . . . . . . .\\n66\\n6.3\\nTools from Matrix Calculus . . . . . . . . . . . . . . . . . . .\\n66\\n6.4\\nThe AdaGrad Algorithm and Its Analysis . . . . . . . . . . .\\n67\\n6.5\\nDiagonal AdaGrad . . . . . . . . . . . . . . . . . . . . . . . .\\n71\\n6.6\\nState-of-the-art: from Adam to Shampoo and beyond\\n. . . .\\n72\\n6.7\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n73\\n6.8\\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . .\\n74\\n7\\nVariance Reduction\\n75\\n7.1\\nVariance reduction: Intuition . . . . . . . . . . . . . . . . . .\\n75\\n7.2\\nSetting and de\\ufb01nitions . . . . . . . . . . . . . . . . . . . . . .\\n76\\n7.3\\nThe variance reduction advantage . . . . . . . . . . . . . . . .\\n77\\n7.4\\nA simple variance-reduced algorithm . . . . . . . . . . . . . .\\n78\\n7.5\\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . .\\n80\\nCONTENTS\\nvii\\n8\\nNesterov Acceleration\\n81\\n8.1\\nAlgorithm and implementation . . . . . . . . . . . . . . . . .\\n81\\n8.2\\nAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n82\\n8.3\\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . .\\n84\\n9\\nThe conditional gradient method\\n85\\n9.1\\nReview: relevant concepts from linear algebra . . . . . . . . .\\n85\\n9.2\\nMotivation: matrix completion and recommendation systems\\n86\\n9.3\\nThe Frank-Wolfe method\\n. . . . . . . . . . . . . . . . . . . .\\n88\\n9.4\\nProjections vs. linear optimization . . . . . . . . . . . . . . .\\n90\\n9.5\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n93\\n9.6\\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . .\\n94\\n10 Second order methods for machine learning\\n95\\n10.1 Motivating example: linear regression\\n. . . . . . . . . . . . .\\n95\\n10.2 Self-Concordant Functions . . . . . . . . . . . . . . . . . . . .\\n96\\n10.3 Newton\\u2019s method for self-concordant functions\\n. . . . . . . .\\n97\\n10.4 Linear-time second-order methods\\n. . . . . . . . . . . . . . . 100\\n10.4.1 Estimators for the Hessian Inverse . . . . . . . . . . . 100\\n10.4.2 Incorporating the estimator . . . . . . . . . . . . . . . 101\\n10.5 Exercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\\n10.6 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . 104\\n11 Hyperparameter Optimization\\n105\\n11.1 Formalizing the problem . . . . . . . . . . . . . . . . . . . . . 105\\n11.2 Hyperparameter optimization algorithms . . . . . . . . . . . . 106\\n11.3 A Spectral Method . . . . . . . . . . . . . . . . . . . . . . . . 107\\n11.3.1 Background: Compressed Sensing\\n. . . . . . . . . . . 108\\n11.3.2 The Spectral Algorithm . . . . . . . . . . . . . . . . . 110\\n11.4 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . 111\\nviii\\nCONTENTS\\nNotation\\nWe use the following mathematical notation in this writeup:\\n\\u2022 d-dimensional Euclidean space is denoted Rd.\\n\\u2022 Vectors are denoted by boldface lower-case letters such as x \\u2208Rd. Co-\\nordinates of vectors are denoted by underscore notation xi or regular\\nbrackets x(i).\\n\\u2022 Matrices are denoted by boldface upper-case letters such as X \\u2208Rm\\u00d7n.\\nTheir coordinates by X(i, j), or Xij.\\n\\u2022 Functions are denoted by lower case letters f : Rd 7\\u2192R.\\n\\u2022 The k-th di\\ufb00erential of function f is denoted by \\u2207kf \\u2208Rdk.\\nThe\\ngradient is denoted without the superscript, as \\u2207f.\\n\\u2022 We use the mathcal macro for sets, such as K \\u2286Rd.\\n\\u2022 We denote the gradient at point xt as \\u2207xt, or simply \\u2207t.\\n\\u2022 We denote the global or local optima of functions by x\\u22c6.\\n\\u2022 We denote distance to optimality for iterative algorithms by ht =\\nf(xt) \\u2212f(x\\u22c6).\\n\\u2022 Euclidean distance to optimality is denoted dt = \\u2225xt \\u2212x\\u22c6\\u2225.\\n1\\n2\\nCONTENTS\\nChapter 1\\nIntroduction\\nThe topic of this lecture series is the mathematical optimization approach\\nto machine learning.\\nIn standard algorithmic theory, the burden of designing an e\\ufb03cient al-\\ngorithm for solving a problem at hand is on the algorithm designer. In the\\ndecades since in the introduction of computer science, elegant algorithms\\nhave been designed for tasks ranging from \\ufb01nding the shortest path in a\\ngraph, computing the optimal \\ufb02ow in a network, compressing a computer\\n\\ufb01le containing an image captured by digital camera, and replacing a string\\nin a text document.\\nThe design approach, while useful to many tasks, falls short of more\\ncomplicated problems, such as identifying a particular person in an image\\nin bitmap format, or translating text from English to Hebrew. There may\\nvery well be an elegant algorithm for the above tasks, but the algorithmic\\ndesign scheme does not scale.\\nAs Turing promotes in his paper [83], it is potentially easier to teach a\\ncomputer to learn how to solve a task, rather than teaching it the solution\\nfor the particular tasks. In e\\ufb00ect, that\\u2019s what we do at school, or in this\\nlecture series...\\nThe machine learning approach to solving problems is to have an au-\\ntomated mechanism for learning an algorithm.\\nConsider the problem of\\nclassifying images into two categories: those containing cars and those con-\\ntaining chairs (assuming there are only two types of images in the world).\\nIn ML we train (teach) a machine to achieve the desired functionality. The\\nsame machine can potentially solve any algorithmic task, and di\\ufb00ers from\\ntask to task only by a set of parameters that determine the functionality of\\nthe machine. This is much like the wires in a computer chip determine its\\n3\\n4\\nCHAPTER 1. INTRODUCTION\\nfunctionality. Indeed, one of the most popular machines are arti\\ufb01cial neural\\nnetworks.\\nThe mathematical optimization approach to machine learning is to view\\nthe process of machine training as an optimization problem. If we let w \\u2208Rd\\nbe the parameters of our machine (a.k.a. model), that are constrained to\\nbe in some set K \\u2286Rd, and f the function measuring success in mapping\\nexamples to their correct label, then the problem we are interested in is\\ndescribed by the mathematical optimization problem of\\nmin\\nw\\u2208K f(w)\\n(1.1)\\nThis is the problem that the lecture series focuses on, with particular em-\\nphasis on functions that arise in machine learning and have special structure\\nthat allows for e\\ufb03cient algorithms.\\n1.1\\nExamples of optimization problems in machine\\nlearning\\n1.1.1\\nEmpirical Risk Minimization\\nMachine learning problems exhibit special structure. For example, one of\\nthe most basic optimization problems in supervised learning is that of \\ufb01tting\\na model to data, or examples, also known as the optimization problem of\\nEmpirical Risk Minimization (ERM). The special structure of the problems\\narising in such formulations is separability across di\\ufb00erent examples into\\nindividual losses.\\nAn example of such formulation is the supervised learning paradigm of\\nlinear classi\\ufb01cation. In this model, the learner is presented with positive and\\nnegative examples of a concept. Each example, denoted by ai, is represented\\nin Euclidean space by a d dimensional feature vector. For example, a com-\\nmon representation for emails in the spam-classi\\ufb01cation problem are binary\\nvectors in Euclidean space, where the dimension of the space is the number of\\nwords in the language. The i\\u2019th email is a vector ai whose entries are given\\nas ones for coordinates corresponding to words that appear in the email,\\nand zero otherwise1. In addition, each example has a label bi \\u2208{\\u22121, +1},\\ncorresponding to whether the email has been labeled spam/not spam. The\\n1Such a representation may seem na\\u00a8\\u0131ve at \\ufb01rst as it completely ignores the words\\u2019 order\\nof appearance and their context. Extensions to capture these features are indeed studied\\nin the Natural Language Processing literature.\\n1.1. EXAMPLES OF OPTIMIZATION PROBLEMS IN MACHINE LEARNING5\\ngoal is to \\ufb01nd a hyperplane separating the two classes of vectors: those with\\npositive labels and those with negative labels. If such a hyperplane, which\\ncompletely separates the training set according to the labels, does not ex-\\nist, then the goal is to \\ufb01nd a hyperplane that achieves a separation of the\\ntraining set with the smallest number of mistakes.\\nMathematically speaking, given a set of m examples to train on, we seek\\nx \\u2208Rd that minimizes the number of incorrectly classi\\ufb01ed examples, i.e.\\nmin\\nx\\u2208Rd\\n1\\nm\\nX\\ni\\u2208[m]\\n\\u03b4(sign(x\\u22a4ai) \\u0338= bi)\\n(1.2)\\nwhere sign(x) \\u2208{\\u22121, +1} is the sign function, and \\u03b4(z) \\u2208{0, 1} is the\\nindicator function that takes the value 1 if the condition z is satis\\ufb01ed and\\nzero otherwise.\\nThe mathematical formulation of the linear classi\\ufb01cation above is a spe-\\ncial case of mathematical programming (1.1), in which\\nf(x) = 1\\nm\\nX\\ni\\u2208[m]\\n\\u03b4(sign(x\\u22a4ai) \\u0338= bi) =\\nE\\ni\\u223c[m][\\u2113i(x)],\\nwhere we make use of the expectation operator for simplicity, and denote\\n\\u2113i(x) = \\u03b4(sign(x\\u22a4ai) \\u0338= bi) for brevity. Since the program above is non-\\nconvex and non-smooth, it is common to take a convex relaxation and replace\\n\\u2113i with convex loss functions. Typical choices include the means square error\\nfunction and the hinge loss, given by\\n\\u2113ai,bi(x) = max{0, 1 \\u2212bi \\u00b7 x\\u22a4ai}.\\nThis latter loss function in the context of binary classi\\ufb01cation gives rise\\nto the popular soft-margin SVM problem.\\nAnother important optimization problem is that of training a deep neural\\nnetwork for binary classi\\ufb01cation. For example, consider a dataset of images,\\nrepresented in bitmap format and denoted by {ai \\u2208Rd|i \\u2208[m]}, i.e. m\\nimages over n pixels. We would like to \\ufb01nd a mapping from images to the\\ntwo categories, {bi \\u2208{0, 1}} of cars and chairs. The mapping is given by a\\nset of parameters of a machine class, such as weights in a neural network,\\nor values of a support vector machine.\\nWe thus try to \\ufb01nd the optimal\\nparameters that match ai to b, i..e\\nmin\\nw\\u2208Rd f(w) = E\\nai,bi [\\u2113(fw(ai), bi)] .\\n6\\nCHAPTER 1. INTRODUCTION\\n1.1.2\\nMatrix completion and recommender systems\\nMedia recommendations have changed signi\\ufb01cantly with the advent of the\\nInternet and rise of online media stores. The large amounts of data collected\\nallow for e\\ufb03cient clustering and accurate prediction of users\\u2019 preferences\\nfor a variety of media.\\nA well-known example is the so called \\u201cNet\\ufb02ix\\nchallenge\\u201d\\u2014a competition of automated tools for recommendation from a\\nlarge dataset of users\\u2019 motion picture preferences.\\nOne of the most successful approaches for automated recommendation\\nsystems, as proven in the Net\\ufb02ix competition, is matrix completion. Perhaps\\nthe simplest version of the problem can be described as follows.\\nThe entire dataset of user-media preference pairs is thought of as a\\npartially-observed matrix. Thus, every person is represented by a row in\\nthe matrix, and every column represents a media item (movie). For sim-\\nplicity, let us think of the observations as binary\\u2014a person either likes or\\ndislikes a particular movie. Thus, we have a matrix M \\u2208{0, 1, \\u2217}n\\u00d7m where\\nn is the number of persons considered, m is the number of movies at our\\nlibrary, and 0/1 and \\u2217signify \\u201cdislike\\u201d, \\u201clike\\u201d and \\u201cunknown\\u201d respectively:\\nMij =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n0,\\nperson i dislikes movie j\\n1,\\nperson i likes movie j\\n\\u2217,\\npreference unknown\\n.\\nThe natural goal is to complete the matrix, i.e. correctly assign 0 or 1 to\\nthe unknown entries. As de\\ufb01ned so far, the problem is ill-posed, since any\\ncompletion would be equally good (or bad), and no restrictions have been\\nplaced on the completions.\\nThe common restriction on completions is that the \\u201ctrue\\u201d matrix has\\nlow rank. Recall that if a matrix X \\u2208Rn\\u00d7m has rank k \\u2264\\u03c1 = min{n, m}\\nthen it can be written as\\nX = UV , U \\u2208Rn\\u00d7k, V \\u2208Rk\\u00d7m.\\nThe intuitive interpretation of this property is that each entry in M\\ncan be explained by only k numbers.\\nIn matrix completion this means,\\nintuitively, that there are only k factors that determine a persons preference\\nover movies, such as genre, director, actors and so on.\\nNow the simplistic matrix completion problem can be well-formulated\\nas in the following mathematical program. Denote by \\u2225\\u00b7 \\u2225OB the Euclidean\\n1.1. EXAMPLES OF OPTIMIZATION PROBLEMS IN MACHINE LEARNING7\\nnorm only on the observed (non starred) entries of M, i.e.,\\n\\u2225X\\u22252\\nOB =\\nX\\nMij\\u0338=\\u2217\\nX2\\nij.\\nThe mathematical program for matrix completion is given by\\nmin\\nX\\u2208Rn\\u00d7m\\n1\\n2\\u2225X \\u2212M\\u22252\\nOB\\ns.t.\\nrank(X) \\u2264k.\\n1.1.3\\nLearning in Linear Dynamical Systems\\nMany learning problems require memory, or the notion of state. This is\\ncaptured by the paradigm of reinforcement learning, as well of the special\\ncase of control in Linear Dynamical Systems (LDS).\\nLDS model a variety of control and robotics problems in continuous\\nvariables. The setting is that of a time series, with following parameters:\\n1. Inputs to the system, also called controls, denoted by u1, ..., uT \\u2208Rn.\\n2. Outputs from the system, also called observations, denoted y1, ..., yT \\u2208\\nRm.\\n3. The state of the system, which may either be observed or hidden,\\ndenoted xt, ..., xT \\u2208Rd.\\n4. The system parameters, which are transformations matrices A, B, C, D\\nin appropriate dimensions.\\nIn the online learning problem of LDS, the learner iteratively observes\\nut, yt, and has to predict \\u02c6yt+1. The actual yt is generated according to the\\nfollowing dynamical equations:\\nxt+1 = Axt + But + \\u03b5t\\nyt+1 = Cxt+1 + Dut + \\u03b6t,\\nwhere \\u03b5t, \\u03b6t are noise which is distributed as a Normal random variable.\\nConsider an online sequence in which the states are visible. At time t,\\nall system states, inputs and outputs are visible up to this time step. The\\nlearner has to predict yt+1, and only afterwards observes ut+1.xt+1, yt+1.\\n8\\nCHAPTER 1. INTRODUCTION\\nOne reasonable way to predict yt+1 based upon past observations is to\\ncompute the system, and use the computed transformations to predict. This\\namounts to solving the following mathematical program:\\nmin\\nA,B, \\u02c6C, \\u02c6D\\n(X\\n\\u03c4<t\\n(x\\u03c4+1 \\u2212Ax\\u03c4 + Bu\\u03c4)2 + (y\\u03c4+1 \\u2212\\u02c6Cx\\u03c4 + \\u02c6Du\\u03c4)2\\n)\\n,\\nand then predicting \\u02c6yt+1 = \\u02c6C \\u02c6A(xt + But) + \\u02c6Dut.\\n1.2\\nWhy is mathematical programming hard?\\nThe general formulation (1.1) is NP hard. To be more precise, we have to\\nde\\ufb01ne the computational model we are working in as well as and the access\\nmodel to the function.\\nBefore we give a formal proof, the intuition to what makes mathematical\\noptimization hard is simple to state. In one line: it is the fact that global\\noptimality cannot be veri\\ufb01ed on the basis of local properties.\\nMost, if not all, e\\ufb03cient optimization algorithms are iterative and based\\non a local improvement step. By this nature, any optimization algorithm\\nwill terminate when the local improvement is no longer possible, giving rise\\nto a proposed solution. However, the quality of this proposed solution may\\ndi\\ufb00er signi\\ufb01cantly, in general, from that of the global optimum.\\nThis intuition explains the need for a property of objectives for which\\nglobal optimality is locally veri\\ufb01able.\\nIndeed, this is exactly the notion\\nof convexity, and the reasoning above explains its utmost importance in\\nmathematical optimization.\\nWe now to prove that mathematical programming is NP-hard.\\nThis\\nrequires discussion of the computational model as well as access model to\\nthe input.\\n1.2.1\\nThe computational model\\nThe computational model we shall adopt throughout this manuscript is that\\nof a RAM machine equipped with oracle access to the objective function\\nf : Rd 7\\u2192R and constraints set K \\u2286Rd. The oracle model for the objective\\nfunction can be one of the following, depending on the speci\\ufb01c scenario:\\n1. Value oracle:\\ngiven a point x \\u2208Rd, oracle returns f(x) \\u2208R.\\n2. Gradient (\\ufb01rst-order) oracle:\\ngiven a point x \\u2208Rd, oracle returns\\nthe gradient \\u2207f(x) \\u2208Rd.\\n1.2. WHY IS MATHEMATICAL PROGRAMMING HARD?\\n9\\n3. k-th order di\\ufb00erential oracle:\\ngiven a point x \\u2208Rd, oracle returns\\nthe tensor \\u2207kf(x) \\u2208Rdk.\\nThe oracle model for the constraints set is a bit more subtle. We distin-\\nguish between the following oracles:\\n1. Membership oracle:\\ngiven a point x \\u2208Rd, oracle returns one if\\nx \\u2208K and zero otherwise.\\n2. Separating hyperplane oracle:\\ngiven a point x \\u2208Rd, oracle either\\nreturns \\u201dYes\\u201d if x \\u2208K, or otherwise returns a hyperplane h \\u2208Rd such\\nthat h\\u22a4x > 0 and \\u2200y \\u2208K , h\\u22a4y \\u22640.\\n3. Explicit sets:\\nthe most common scenario in machine learning is one\\nin which K is \\u201cnatural\\u201d, such as the Euclidean ball or hypercube, or\\nthe entire Euclidean space.\\n1.2.2\\nHardness of constrained mathematical programming\\nUnder this computational model, we can show:\\nLemma 1.1. Mathematical programming is NP-hard, even for a convex\\ncontinuous constraint set K and quadratic objective functions.\\nInformal sketch. Consider the MAX-CUT problem:\\ngiven a graph G =\\n(V, E), \\ufb01nd a subset of the vertices that maximizes the number of edges\\ncut. Let A be the negative adjacency matrix of the graph, i.e.\\nAij =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n\\u22121,\\n(i, j) \\u2208E\\n0,\\no/w\\nAlso suppose that Aii = 0.\\nNext, consider the mathematical program:\\nmin\\n\\u001a\\nfA(x) = 1\\n4(x\\u22a4Ax \\u22122|E|)\\n\\u001b\\n(1.3)\\n\\u2225x\\u2225\\u221e= 1 .\\nConsider the cut de\\ufb01ned by the solution of this program, namely\\nSx = {i \\u2208V |xi = 1},\\nfor x = x\\u22c6. Let C(S) denote the size of the cut speci\\ufb01ed by the subset of\\nedges S \\u2286E. Observe that the expression 1\\n2x\\u22a4Ax, is exactly equal to the\\n10\\nCHAPTER 1. INTRODUCTION\\nnumber of edges that are cut by Sx minus the number of edges that are\\nuncut. Thus, we have\\n1\\n2xAx = C(Sx) \\u2212(E \\u2212C(Sx)) = 2C(Sx) \\u2212E,\\nand hence f(x) = C(Sx). Therefore, maximizing f(x) is equivalent to the\\nMAX-CUT problem, and is thus NP-hard. We proceed to make the con-\\nstraint set convex and continuous. Consider the mathematical program\\nmin {fA(x)}\\n(1.4)\\n\\u2225x\\u2225\\u221e\\u22641 .\\nThis is very similar to the previous program, but we relaxed the equality\\nto be an inequality, consequently the constraint set is now the hypercube.\\nWe now claim that the solution is w.l.o.g. a vertex. To see that, consider\\ny(x) \\u2208{\\u00b11}d a rounding of x to the corners de\\ufb01ned by:\\nyi = y(x)i =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1,\\nw.p. 1+xi\\n2\\n\\u22121,\\nw.p. 1\\u2212xi\\n2\\nNotice that\\nE[y] = x , \\u2200i \\u0338= j . E[yiyj] = xixj,\\nand therefore E[y(x)\\u22a4Ay(x)] = x\\u22a4Ax. We conclude that the optimum of\\nmathematical program 1.4 is the same as that for 1.3, and both are NP-\\nhard.\\nChapter 2\\nBasic concepts in\\noptimization and analysis\\n2.1\\nBasic de\\ufb01nitions and the notion of convexity\\nWe consider minimization of a continuous function over a convex subset of\\nEuclidean space. We mostly consider objective functions that are convex. In\\nlater chapters we relax this requirement and consider non-convex functions\\nas well.\\nHenceforth, let K \\u2286Rd be a bounded convex and compact set in Eu-\\nclidean space. We denote by D an upper bound on the diameter of K:\\n\\u2200x, y \\u2208K, \\u2225x \\u2212y\\u2225\\u2264D.\\nA set K is convex if for any x, y \\u2208K, all the points on the line segment\\nconnecting x and y also belong to K, i.e.,\\n\\u2200\\u03b1 \\u2208[0, 1], \\u03b1x + (1 \\u2212\\u03b1)y \\u2208K.\\nA function f : K 7\\u2192R is convex if for any x, y \\u2208K\\n\\u2200\\u03b1 \\u2208[0, 1], f(\\u03b1x + (1 \\u2212\\u03b1)y) \\u2264\\u03b1f(x) + (1 \\u2212\\u03b1)f(y).\\nGradients and subgradients.\\nThe set of all subgradients of a function\\nf at x, denoted \\u2202f(x), is the set of all vectors u such that\\nf(y) \\u2265f(x) + u\\u22a4(y \\u2212x).\\nIt can be shown that the set of subgradients of a convex function is always\\nnon-empty.\\n11\\n12\\nCHAPTER 2. BASIC CONCEPTS\\nSuppose f is di\\ufb00erentiable, let \\u2207f(x)[i] =\\n\\u2202\\n\\u2202xi f(x) be the vector of\\npartial derivatives according to the variables, called the gradient.\\nIf the\\ngradient \\u2207f(x) exists, then \\u2207f(x) \\u2208\\u2202f(x) and \\u2200y \\u2208K\\nf(y) \\u2265f(x) + \\u2207f(x)\\u22a4(y \\u2212x).\\nHenceforth we shall denote by \\u2207f(x) the gradient, if it exists, or any member\\nof \\u2202f(x) otherwise.\\nWe denote by G > 0 an upper bound on the norm of the subgradients of\\nf over K, i.e., \\u2225\\u2207f(x)\\u2225\\u2264G for all x \\u2208K. The existence of Such an upper\\nbound implies that the function f is Lipschitz continuous with parameter\\nG, that is, for all x, y \\u2208K\\n|f(x) \\u2212f(y)| \\u2264G\\u2225x \\u2212y\\u2225.\\nSmoothness and strong convexity.\\nThe optimization and machine learn-\\ning literature studies special types of convex functions that admit useful\\nproperties, which in turn allow for more e\\ufb03cient optimization. Notably, we\\nsay that a function is \\u03b1-strongly convex if\\nf(y) \\u2265f(x) + \\u2207f(x)\\u22a4(y \\u2212x) + \\u03b1\\n2 \\u2225y \\u2212x\\u22252.\\nA function is \\u03b2-smooth if\\nf(y) \\u2264f(x) + \\u2207f(x)\\u22a4(y \\u2212x) + \\u03b2\\n2 \\u2225y \\u2212x\\u22252.\\nThe latter condition is implied by a slightly stronger Lipschitz condition\\nover the gradients, which is sometimes used to de\\ufb01ned smoothness, i.e.,\\n\\u2225\\u2207f(x) \\u2212\\u2207f(y)\\u2225\\u2264\\u03b2\\u2225x \\u2212y\\u2225.\\nIf the function is twice di\\ufb00erentiable and admits a second derivative,\\nknown as a Hessian for a function of several variables, the above conditions\\nare equivalent to the following condition on the Hessian, denoted \\u22072f(x):\\nSmoothness:\\n\\u2212\\u03b2I \\u227c\\u22072f(x) \\u227c\\u03b2I\\nStrong-convexity:\\n\\u03b1I \\u227c\\u22072f(x),\\nwhere A \\u227cB if the matrix B \\u2212A is positive semide\\ufb01nite.\\nWhen the function f is both \\u03b1-strongly convex and \\u03b2-smooth, we say\\nthat it is \\u03b3-well-conditioned where \\u03b3 is the ratio between strong convexity\\nand smoothness, also called the condition number of f\\n\\u03b3 = \\u03b1\\n\\u03b2 \\u22641\\n2.1. BASICS\\n13\\n2.1.1\\nProjections onto convex sets\\nIn the following algorithms we shall make use of a projection operation onto\\na convex set, which is de\\ufb01ned as the closest point inside the convex set to a\\ngiven point. Formally,\\n\\u03a0\\nK(y) \\u225carg min\\nx\\u2208K\\n\\u2225x \\u2212y\\u2225.\\nWhen clear from the context, we shall remove the K subscript. It is left as\\nan exercise to the reader to prove that the projection of a given point over\\na closed non-empty convex set exists and is unique.\\nThe computational complexity of projections is a subtle issue that de-\\npends much on the characterization of K itself. Most generally, K can be\\nrepresented by a membership oracle\\u2014an e\\ufb03cient procedure that is capable\\nof deciding whether a given x belongs to K or not. In this case, projections\\ncan be computed in polynomial time. In certain special cases, projections\\ncan be computed very e\\ufb03ciently in near-linear time.\\nA crucial property of projections that we shall make extensive use of is\\nthe Pythagorean theorem, which we state here for completeness:\\nFigure 2.1: Pythagorean theorem.\\nTheorem 2.1 (Pythagoras, circa 500 BC). Let K \\u2286Rd be a convex set,\\ny \\u2208Rd and x = \\u03a0K(y). Then for any z \\u2208K we have\\n\\u2225y \\u2212z\\u2225\\u2265\\u2225x \\u2212z\\u2225.\\n14\\nCHAPTER 2. BASIC CONCEPTS\\nWe note that there exists a more general version of the Pythagorean\\ntheorem. The above theorem and the de\\ufb01nition of projections are true and\\nvalid not only for Euclidean norms, but for projections according to other\\ndistances that are not norms. In particular, an analogue of the Pythagorean\\ntheorem remains valid with respect to Bregman divergences.\\n2.1.2\\nIntroduction to optimality conditions\\nThe standard curriculum of high school mathematics contains the basic facts\\nconcerning when a function (usually in one dimension) attains a local opti-\\nmum or saddle point. The KKT (Karush-Kuhn-Tucker) conditions general-\\nize these facts to more than one dimension, and the reader is referred to the\\nbibliographic material at the end of this chapter for an in-depth rigorous\\ndiscussion of optimality conditions in general mathematical programming.\\nFor our purposes, we describe only brie\\ufb02y and intuitively the main facts\\nthat we will require henceforth. We separate the discussion into convex and\\nnon-convex programming.\\nOptimality for convex optimization\\nA local minimum of a convex function is also a global minimum (see exercises\\nat the end of this chapter). We say that x\\u22c6is an \\u03b5-approximate optimum if\\nthe following holds:\\n\\u2200x \\u2208K . f(x\\u22c6) \\u2264f(x) + \\u03b5.\\nThe generalization of the fact that a minimum of a convex di\\ufb00erentiable\\nfunction on R is a point in which its derivative is equal to zero, is given by\\nthe multi-dimensional analogue that its gradient is zero:\\n\\u2207f(x) = 0\\n\\u21d0\\u21d2\\nx \\u2208arg min\\nx\\u2208Rn\\nf(x).\\nWe will require a slightly more general, but equally intuitive, fact for con-\\nstrained optimization: at a minimum point of a constrained convex function,\\nthe inner product between the negative gradient and direction towards the\\ninterior of K is non-positive. This is depicted in Figure 2.2, which shows that\\n\\u2212\\u2207f(x\\u22c6) de\\ufb01nes a supporting hyperplane to K. The intuition is that if the\\ninner product were positive, one could improve the objective by moving in\\nthe direction of the projected negative gradient. This fact is stated formally\\nin the following theorem.\\n2.1. BASICS\\n15\\nTheorem 2.2 (Karush-Kuhn-Tucker). Let K \\u2286Rd be a convex set, x\\u22c6\\u2208\\narg minx\\u2208K f(x). Then for any y \\u2208K we have\\n\\u2207f(x\\u22c6)\\u22a4(y \\u2212x\\u22c6) \\u22650.\\nFigure 2.2: Optimality conditions: negative (sub)gradient pointing out-\\nwards.\\n2.1.3\\nSolution concepts for non-convex optimization\\nWe have seen in the previous chapter that mathematical optimization is NP-\\nhard. This implies that \\ufb01nding global solutions for non-convex optimization\\nis NP-hard, even for smooth functions over very simple convex domains. We\\nthus consider other trackable concepts of solutions.\\nThe most common solution concept is that of \\ufb01rst-order optimality, a.k.a.\\nsaddle-points or stationary points. These are points that satisfy\\n\\u2225\\u2207f(x\\u22c6)\\u2225= 0.\\nUnfortunately, even \\ufb01nding such stationary points is NP-hard.\\nWe thus\\nsettle for approximate stationary points, which satisify\\n\\u2225\\u2207f(x\\u22c6)\\u2225\\u2264\\u03b5.\\n16\\nCHAPTER 2. BASIC CONCEPTS\\nFigure 2.3: First and second-order local optima.\\nA more stringent notion of optimality we may consider is obtained by\\nlooking at the second derivatives. We can require they behave as for global\\nminimum, see \\ufb01gure 2.3. Formally, we say that a point x\\u22c6is a second-order\\nlocal minimum if it satis\\ufb01es the two conditions:\\n\\u2225\\u2207f(x\\u22c6)\\u2225\\u2264\\u03b5 , \\u22072f(x\\u22c6) \\u2ab0\\u2212\\u221a\\u03b5I.\\nThe di\\ufb00erences in approximation criteria for \\ufb01rst and second derivatives is\\nnatural, as we shall explore in non-convex approximation algorithms hence-\\nforth.\\nWe note that it is possible to further de\\ufb01ne optimality conditions for\\nhigher order derivatives, although this is less useful in the context of machine\\nlearning.\\n2.2\\nPotentials for distance to optimality\\nWhen analyzing convergence of gradient methods, it is useful to use potential\\nfunctions in lieu of function distance to optimality, such as gradient norm\\nand/or Euclidean distance to optimality. The following relationships hold\\nbetween these quantities.\\nLemma 2.3. The following properties hold for \\u03b1-strongly-convex functions\\nand/or \\u03b2-smooth functions over Euclidean space Rd.\\n1.\\n\\u03b1\\n2 d2\\nt \\u2264ht\\n2. ht \\u2264\\u03b2\\n2 d2\\nt\\n2.2. POTENTIALS FOR DISTANCE TO OPTIMALITY\\n17\\n3.\\n1\\n2\\u03b2\\u2225\\u2207t\\u22252 \\u2264ht\\n4. ht \\u2264\\n1\\n2\\u03b1\\u2225\\u2207t\\u22252\\nProof.\\n1. ht \\u2265\\u03b1\\n2 d2\\nt :\\nBy strong convexity, we have\\nht\\n= f(xt) \\u2212f(x\\u22c6)\\n\\u2265\\u2207f(x\\u22c6)\\u22a4(xt \\u2212x\\u22c6) + \\u03b1\\n2 \\u2225xt \\u2212x\\u22c6\\u22252\\n= \\u03b1\\n2 \\u2225xt \\u2212x\\u22c6\\u22252\\nwhere the last inequality follows since the gradient at the global opti-\\nmum is zero.\\n2. ht \\u2264\\u03b2\\n2 d2\\nt :\\nBy smoothness,\\nht\\n= f(xt) \\u2212f(x\\u22c6)\\n\\u2264\\u2207f(x\\u22c6)\\u22a4(xt \\u2212x\\u22c6) + \\u03b2\\n2 \\u2225xt \\u2212x\\u22c6\\u22252\\n= \\u03b2\\n2 \\u2225xt \\u2212x\\u22c6\\u22252\\nwhere the last inequality follows since the gradient at the global opti-\\nmum is zero.\\n3. ht \\u2265\\n1\\n2\\u03b2\\u2225\\u2207t\\u22252: Using smoothness, and let xt+1 = xt \\u2212\\u03b7\\u2207t for \\u03b7 = 1\\n\\u03b2,\\nht =\\nf(xt) \\u2212f(x\\u22c6)\\n\\u2265f(xt) \\u2212f(xt+1)\\n\\u2265\\u2207f(xt)\\u22a4(xt \\u2212xt+1) \\u2212\\u03b2\\n2 \\u2225xt \\u2212xt+1\\u22252\\n= \\u03b7\\u2225\\u2207t\\u22252 \\u2212\\u03b2\\n2 \\u03b72\\u2225\\u2207t\\u22252\\n=\\n1\\n2\\u03b2\\u2225\\u2207t\\u22252.\\n4. ht \\u2264\\n1\\n2\\u03b1\\u2225\\u2207t\\u22252:\\nWe have for any pair x, y \\u2208Rd:\\nf(y) \\u2265f(x) + \\u2207f(x)\\u22a4(y \\u2212x) + \\u03b1\\n2 \\u2225x \\u2212y\\u22252\\n\\u2265min\\nz\\u2208Rd\\nn\\nf(x) + \\u2207f(x)\\u22a4(z \\u2212x) + \\u03b1\\n2 \\u2225x \\u2212z\\u22252o\\n= f(x) \\u22121\\n2\\u03b1\\u2225\\u2207f(x)\\u22252.\\nby taking z = x \\u22121\\n\\u03b1\\u2207f(x)\\n18\\nCHAPTER 2. BASIC CONCEPTS\\nIn particular, taking x = xt , y = x\\u22c6, we get\\nht = f(xt) \\u2212f(x\\u22c6) \\u22641\\n2\\u03b1\\u2225\\u2207t\\u22252.\\n(2.1)\\n2.3\\nGradient descent and the Polyak stepsize\\nThe simplest iterative optimization algorithm is gradient descent, as given\\nin Algorithm 1. We analyze GD with the Polyak stepsize, which has the\\nadvantage of not depending on the strong convexity and/or smoothness\\nparameters of the objective function.\\nAlgorithm 1 GD with the Polyak stepsize\\n1: Input: time horizon T, x0\\n2: for t = 0, . . . , T \\u22121 do\\n3:\\nSet \\u03b7t =\\nht\\n\\u2225\\u2207t\\u22252\\n4:\\nxt+1 = xt \\u2212\\u03b7t\\u2207t\\n5: end for\\n6: Return \\u00afx = arg minxt{f(xt)}\\nTo prove convergence bounds, assume \\u2225\\u2207t\\u2225\\u2264G, and de\\ufb01ne:\\nBT\\n=\\nmin\\n(\\nGd0\\n\\u221a\\nT\\n, 2\\u03b2d2\\n0\\nT\\n, 3G2\\n\\u03b1T , \\u03b2d2\\n0\\n\\u0012\\n1 \\u2212\\u03b1\\n4\\u03b2\\n\\u0013T )\\nTheorem 2.4. (GD with the Polyak Step Size) Algorithm 1 attains the\\nfollowing regret bound after T steps:\\nh(\\u00afx)\\n=\\nmin\\n0\\u2264t\\u2264T{ht} \\u2264BT\\nTheorem 2.4 directly follows from the following lemma. Let 0 \\u2264\\u03b3 \\u22641,\\nde\\ufb01ne RT,\\u03b3 as follows:\\nRT,\\u03b3 = min\\n(\\nGd0\\n\\u221a\\u03b3T , 2\\u03b2d2\\n0\\n\\u03b3T , 3G2\\n\\u03b3\\u03b1T , \\u03b2d2\\n0\\n\\u0012\\n1 \\u2212\\u03b3 \\u03b1\\n4\\u03b2\\n\\u0013T )\\n.\\nLemma 2.5. For 0 \\u2264\\u03b3 \\u22641, suppose that a sequence x0, . . . xt satis\\ufb01es:\\nd2\\nt+1 \\u2264d2\\nt \\u2212\\u03b3\\nh2\\nt\\n\\u2225\\u2207t\\u22252\\n(2.2)\\n2.3. GRADIENT DESCENT AND THE POLYAK STEPSIZE\\n19\\nthen for \\u00afx as de\\ufb01ned in the algorithm, we have:\\nh(\\u00afx) \\u2264RT,\\u03b3 .\\nProof. The proof analyzes di\\ufb00erent cases:\\n1. For convex functions with gradient bounded by G,\\nd2\\nt+1 \\u2212d2\\nt\\n\\u2264\\u2212\\u03b3h2\\nt\\n\\u2225\\u2207t\\u22252 \\u2264\\u2212\\u03b3h2\\nt\\nG2\\nSumming up over T iterations, and using Cauchy-Schwartz, we have\\n1\\nT\\nX\\nt\\nht\\n\\u2264\\n1\\n\\u221a\\nT\\nsX\\nt\\nh2\\nt\\n\\u2264\\nG\\n\\u221a\\u03b3T\\nsX\\nt\\n(d2\\nt \\u2212d2\\nt+1) \\u2264Gd0\\n\\u221a\\u03b3T .\\n2. For smooth functions whose gradient is bounded by G, Lemma 2.3\\nimplies:\\nd2\\nt+1 \\u2212d2\\nt \\u2264\\u2212\\u03b3h2\\nt\\n\\u2225\\u2207t\\u22252 \\u2264\\u2212\\u03b3ht\\n2\\u03b2 .\\nThis implies\\n1\\nT\\nX\\nt\\nht \\u22642\\u03b2d2\\n0\\n\\u03b3T\\n.\\n3. For strongly convex functions, Lemma 2.3 implies:\\nd2\\nt+1 \\u2212d2\\nt \\u2264\\u2212\\u03b3\\nh2\\nt\\n\\u2225\\u2207t\\u22252 \\u2264\\u2212\\u03b3 h2\\nt\\nG2 \\u2264\\u2212\\u03b3 \\u03b12d4\\nt\\n4G2 .\\nIn other words, d2\\nt+1 \\u2264d2\\nt (1 \\u2212\\u03b3 \\u03b12d2\\nt\\n4G2 ) . De\\ufb01ning at := \\u03b3 \\u03b12d2\\nt\\n4G2 , we have:\\nat+1 \\u2264at(1 \\u2212at) .\\n20\\nCHAPTER 2. BASIC CONCEPTS\\nThis implies that at \\u2264\\n1\\nt+1, which can be seen by induction1. The\\nproof is completed as follows2 :\\n1\\nT/2\\nT\\nX\\nt=T/2\\nh2\\nt\\n\\u2264\\n2G2\\n\\u03b3T\\nT\\nX\\nt=T/2\\n(d2\\nt \\u2212d2\\nt+1)\\n=\\n2G2\\n\\u03b3T (d2\\nT/2 \\u2212d2\\nT )\\n=\\n8G4\\n\\u03b32\\u03b12T (aT/2 \\u2212aT )\\n\\u2264\\n9G4\\n\\u03b32\\u03b12T 2 .\\nThus, there exists a t for which h2\\nt \\u2264\\n9G4\\n\\u03b32\\u03b12T 2 . Taking the square root\\ncompletes the claim.\\n4. For both strongly convex and smooth functions:\\nd2\\nt+1 \\u2212d2\\nt \\u2264\\u2212\\u03b3\\nh2\\nt\\n\\u2225\\u2207t\\u22252 \\u2264\\u2212\\u03b3ht\\n2\\u03b2 \\u2264\\u2212\\u03b3 \\u03b1\\n4\\u03b2 d2\\nt\\nThus,\\nhT \\u2264\\u03b2d2\\nT \\u2264\\u03b2d2\\n0\\n\\u0012\\n1 \\u2212\\u03b3 \\u03b1\\n4\\u03b2\\n\\u0013T\\n.\\nThis completes the proof of all cases.\\n1That a0 \\u22641 follows from Lemma 2.3. For t = 1, a1 \\u22641\\n2 since a1 \\u2264a0(1 \\u2212a0) and\\n0 \\u2264a0 \\u22641. For the induction step, at \\u2264at\\u22121(1 \\u2212at\\u22121) \\u22641\\nt (1 \\u22121\\nt ) = t\\u22121\\nt2 =\\n1\\nt+1( t2\\u22121\\nt2 ) \\u2264\\n1\\nt+1.\\n2This assumes T is even. T odd leads to the same constants.\\n2.4. EXERCISES\\n21\\n2.4\\nExercises\\n1. Write an explicit expression for the gradient and projection operation\\n(if needed) for each of the example optimization problems in the \\ufb01rst\\nchapter.\\n2. Prove that a di\\ufb00erentiable function f(x) : R \\u2192R is convex if and only\\nif for any x, y \\u2208R it holds that f(x) \\u2212f(y) \\u2264(x \\u2212y)f\\u2032(x).\\n3. Recall that we say that a function f : Rn \\u2192R has a condition number\\n\\u03b3 = \\u03b1/\\u03b2 over K \\u2286Rd if the following two inequalities hold for all\\nx, y \\u2208K:\\n(a) f(y) \\u2265f(x) + (y \\u2212x)\\u22a4\\u2207f(x) + \\u03b1\\n2 \\u2225x \\u2212y\\u22252\\n(b) f(y) \\u2264f(x) + (y \\u2212x)\\u22a4\\u2207f(x) + \\u03b2\\n2 \\u2225x \\u2212y\\u22252\\nFor matrices A, B \\u2208Rn\\u00d7n we denote A \\u227dB if A \\u2212B is positive\\nsemide\\ufb01nite. Prove that if f is twice di\\ufb00erentiable and it holds that\\n\\u03b2I \\u227d\\u22072f(x) \\u227d\\u03b1I for any x \\u2208K, then the condition number of f\\nover K is \\u03b1/\\u03b2.\\n4. Prove:\\n(a) The sum of convex functions is convex.\\n(b) Let f be \\u03b11-strongly convex and g be \\u03b12-strongly convex. Then\\nf + g is (\\u03b11 + \\u03b12)-strongly convex.\\n(c) Let f be \\u03b21-smooth and g be \\u03b22-smooth. Then f +g is (\\u03b21 +\\u03b22)-\\nsmooth.\\n5. Let K \\u2286Rd be closed, compact, non-empty and bounded. Prove that\\na necessary and su\\ufb03cient condition for \\u03a0K(x) to be a singleton, that\\nis for | \\u03a0K(x)| = 1, is for K to be convex.\\n6. Prove that for convex functions, \\u2207f(x) \\u2208\\u2202f(x), that is, the gradient\\nbelongs to the subgradient set.\\n7. Let f(x) : Rn \\u2192R be a convex di\\ufb00erentiable function and K \\u2286Rn be\\na convex set. Prove that x\\u22c6\\u2208K is a minimizer of f over K if and only\\nif for any y \\u2208K it holds that (y \\u2212x\\u22c6)\\u22a4\\u2207f(x\\u22c6) \\u22650.\\n8. Consider the n-dimensional simplex\\n\\u2206n = {x \\u2208Rn |\\nn\\nX\\ni=1\\nxi = 1, xi \\u22650 , \\u2200i \\u2208[n]}.\\n22\\nCHAPTER 2. BASIC CONCEPTS\\nGive an algorithm for computing the projection of a point x \\u2208Rn onto\\nthe set \\u2206n (a near-linear time algorithm exists).\\n2.5. BIBLIOGRAPHIC REMARKS\\n23\\n2.5\\nBibliographic remarks\\nThe reader is referred to dedicated books on convex optimization for much\\nmore in-depth treatment of the topics surveyed in this background chapter.\\nFor background in convex analysis see the texts [11, 68]. The classic text-\\nbook [12] gives a broad introduction to convex optimization with numerous\\napplications. For an adaptive analysis of gradient descent with the Polyak\\nstepsize see [33].\\n24\\nCHAPTER 2. BASIC CONCEPTS\\nChapter 3\\nStochastic Gradient Descent\\nThe most important optimization algorithm in the context of machine learn-\\ning is stochastic gradient descent (SGD), especially for non-convex optimiza-\\ntion and in the context of deep neural networks. In this chapter we spell\\nout the algorithm and analyze it up to tight \\ufb01nite-time convergence rates.\\n3.1\\nTraining feedforward neural networks\\nPerhaps the most common optimization problem in machine learning is that\\nof training feedforward neural networks. In this problem, we are given a set\\nof labelled data points, such as labelled images or text. Let {xi, yi} be the\\nset of labelled data points, also called the training data.\\nThe goal is to \\ufb01t the weights of an arti\\ufb01cial neural network in order to\\nminimize the loss over the data. Mathematically, the feedforward network\\nis a given weighted a-cyclic graph G = (V, E, W). Each node v is assigned\\nan activation function, which we assume is the same function for all nodes,\\ndenoted \\u03c3 : Rd 7\\u2192R. Using a biological analogy, an activation function \\u03c3\\nis a function that determines how strongly a neuron (i.e. a node) \\u2018\\ufb01res\\u2019 for\\na given input by mapping the result into the desired range, usually [0, 1] or\\n[\\u22121, 1] . Some popular examples include:\\n\\u2022 Sigmoid: \\u03c3(x) =\\n1\\n1+e\\u2212x\\n\\u2022 Hyperbolic tangent: tanh(x) = ex\\u2212e\\u2212x\\nex+e\\u2212x\\n\\u2022 Recti\\ufb01ed linear unit: ReLU(x) = max{0, x} (currently the most widely\\nused of the three)\\n25\\n26\\nCHAPTER 3. STOCHASTIC GRADIENT DESCENT\\nThe inputs to the input layer nodes is a given data point, while the\\ninputs to to all other nodes are the output of the nodes connected to it. We\\ndenote by \\u03c1(v) the set of input neighbors to node v. The top node output\\nis the input to the loss function, which takes its \\u201cprediction\\u201d and the true\\nlabel to form a loss.\\nFor an input node v, its output as a function of the graph weights and\\ninput example x (of dimension d), which we denote as\\nv(W, x) = \\u03c3\\n X\\ni\\u2208d\\nWv,ixi\\n!\\nThe output of an internal node v is a function of its inputs u \\u2208\\u03c1(v) and a\\ngiven example x, which we denote as\\nv(W, x) = \\u03c3\\n\\uf8eb\\n\\uf8edX\\nu\\u2208\\u03c1(v)\\nWuvu(W, x)\\n\\uf8f6\\n\\uf8f8\\nIf we denote the top node as v1, then the loss of the network over data point\\n(xi, yi) is given by\\n\\u2113(v1(W, xi), yi).\\nThe objective function becomes\\nf(W) = E\\nxi,yi\\n\\u0002\\n\\u2113(v1(W, xi), yi)\\n\\u0003\\nFor most commonly-used activation and loss functions, the above func-\\ntion is non-convex. However, it admits important computational properties.\\nThe most signi\\ufb01cant property is given in the following lemma.\\nLemma 3.1 (Backpropagation lemma). The gradient of f can be computed\\nin time O(|E|).\\nThe proof of this lemma is left as an exercise, but we sketch the main\\nideas. For every variable Wuv, we have by linearity of expectation that\\n\\u2202\\n\\u2202Wuv\\nf(W) = E\\nxi,yi\\n\\u0014\\n\\u2202\\n\\u2202Wuv\\n\\u2113(v1(W, xi), yi)\\n\\u0015\\n.\\nNext, using the chain rule, we claim that it su\\ufb03ces to know the partial\\nderivatives of each node w.r.t. its immediate daughters. To see this, let us\\n3.2. GRADIENT DESCENT FOR SMOOTH OPTIMIZATION\\n27\\nwrite the derivative w.r.t. Wuv using the chain rule:\\n\\u2202\\n\\u2202Wuv\\n\\u2113(v1(W, xi), yi) = \\u2202\\u2113\\n\\u2202v1 \\u00b7 \\u2202v1\\n\\u2202Wuv\\n= \\u2202\\u2113\\n\\u2202v1 \\u00b7\\nX\\nv2\\u2208\\u03c1(v1)\\n\\u2202v1\\n\\u2202v2 \\u00b7\\n\\u2202vj\\n\\u2202Wuv\\n= ...\\n= \\u2202\\u2113\\n\\u2202v1 \\u00b7\\nX\\nv2\\u2208\\u03c1(v1)\\n\\u2202v1\\n\\u2202v2 \\u00b7 ... \\u00b7\\nX\\nvk\\nj \\u2208\\u03c1(vk\\u22121)\\n\\u00b7 \\u2202vk\\n\\u2202Wuv\\nWe conclude that we only need to obtain the E partial derivatives along\\nthe edges in order to compute all partial derivatives of the function. The\\nactual product at each node can be computed by a dynamic program in\\nlinear time.\\n3.2\\nGradient descent for smooth optimization\\nBefore moving to stochastic gradient descent, we consider its determinis-\\ntic counterpart:\\ngradient descent, in the context of smooth non-convex\\noptimization.\\nOur notion of solution is a point with small gradient, i.e.\\n\\u2225\\u2207f(x)\\u2225\\u2264\\u03b5.\\nAs we prove below, this requires O( 1\\n\\u03b52 ) iterations, each requiring one gra-\\ndient computation. Recall that gradients can be computed e\\ufb03ciently, linear\\nin the number of edges, in feed forward neural networks. Thus, the time to\\nobtain a \\u03b5-approximate solution becomes O( |E|m\\n\\u03b52 ) for neural networks with\\nE edges and over m examples.\\nAlgorithm 2 Gradient descent\\n1: Input: f, T, initial point x1 \\u2208K, sequence of step sizes {\\u03b7t}\\n2: for t = 1 to T do\\n3:\\nLet yt+1 = xt \\u2212\\u03b7t\\u2207f(xt), xt+1 = \\u03a0K (yt+1)\\n4: end for\\n5: return xT+1\\nAlthough the choice of \\u03b7t can make a di\\ufb00erence in practice, in theory\\nthe convergence of the vanilla GD algorithm is well understood and given in\\nthe following theorem. Below we assume that the function is bounded such\\nthat |f(x)| \\u2264M.\\n28\\nCHAPTER 3. STOCHASTIC GRADIENT DESCENT\\nTheorem 3.2. For unconstrained minimization of \\u03b2-smooth functions and\\n\\u03b7t = 1\\n\\u03b2, GD Algorithm 2 converges as\\n1\\nT\\nX\\nt\\n\\u2225\\u2207t\\u22252 \\u22644M\\u03b2\\nT\\n.\\nProof. Denote by \\u2207t the shorthand for \\u2207f(xt), and ht = f(xt) \\u2212f(x\\u2217).\\nThe Descent Lemma is given in the following simple equation,\\nht+1 \\u2212ht = f(xt+1) \\u2212f(xt)\\n\\u2264\\u2207\\u22a4\\nt (xt+1 \\u2212xt) + \\u03b2\\n2 \\u2225xt+1 \\u2212xt\\u22252\\n\\u03b2-smoothness\\n= \\u2212\\u03b7t\\u2225\\u2207t\\u22252 + \\u03b2\\n2 \\u03b72\\nt \\u2225\\u2207t\\u22252\\nalgorithm defn.\\n= \\u22121\\n2\\u03b2 \\u2225\\u2207t\\u22252\\nchoice of \\u03b7t = 1\\n\\u03b2\\nThus, summing up over T iterations, we have\\n1\\n2\\u03b2\\nT\\nX\\nt=1\\n\\u2225\\u2207t\\u22252 \\u2264\\nX\\nt\\n(ht \\u2212ht+1) = h1 \\u2212hT+1 \\u22642M\\nFor convex functions, the above theorem implies convergence in function\\nvalue due to the following lemma,\\nLemma 3.3. A convex function satis\\ufb01es\\nht \\u2264D\\u2225\\u2207t\\u2225,\\nand an \\u03b1-strongly convex function satis\\ufb01es\\nht \\u22641\\n2\\u03b1\\u2225\\u2207t\\u22252.\\nProof. The gradient upper bound for convex functions gives\\nht \\u2264\\u2207t(x\\u2217\\u2212xt) \\u2264D\\u2225\\u2207t\\u2225\\nThe strongly convex case appears in Lemma 2.3.\\n3.3. STOCHASTIC GRADIENT DESCENT\\n29\\n3.3\\nStochastic gradient descent\\nIn the context of training feed forward neural networks, the key idea of\\nStochastic Gradient Descent is to modify the updates to be:\\nWt+1 = Wt \\u2212\\u03b7 e\\u2207t\\n(3.1)\\nwhere e\\u2207t is a random variable with E[e\\u2207t] = \\u2207f (Wt) and bounded second\\nmoment E[\\u2225e\\u2207t\\u22252\\n2] \\u2264\\u03c32.\\nLuckily, getting the desired e\\u2207t random variable is easy in the posed\\nproblem since the objective function is already in expectation form so:\\n\\u2207f(W) = \\u2207E\\nxi,yi[\\u2113(v1(W, xi), yi)] = E\\nxi,yi[\\u2207\\u2113(v1(W, xi), yi)].\\nTherefore, at iteration t we can take e\\u2207t = \\u2207\\u2113(v1(W, xi), yi) where i \\u2208\\n{1, ..., m} is picked uniformly at random. Based on the observation above,\\nchoosing e\\u2207t this way preserves the desired expectation. So, for each iteration\\nwe only compute the gradient w.r.t. to one random example instead of the\\nentire dataset, thereby drastically improving performance for every step. It\\nremains to analyze how this impacts convergence.\\nAlgorithm 3 Stochastic gradient descent\\n1: Input: f, T, initial point x1 \\u2208K, sequence of step sizes {\\u03b7t}\\n2: for t = 1 to T do\\n3:\\nLet yt+1 = xt \\u2212\\u03b7t\\u2207f(xt), xt+1 = \\u03a0K (yt+1)\\n4: end for\\n5: return xT+1\\nTheorem 3.4. For unconstrained minimization of \\u03b2-smooth functions and\\n\\u03b7t = \\u03b7 =\\nq\\nM\\n\\u03b2\\u03c32T , SGD Algorithm 3 converges as\\nE\\n\\\"\\n1\\nT\\nX\\nt\\n\\u2225\\u2207t\\u22252\\n#\\n\\u22642\\nr\\nM\\u03b2\\u03c32\\nT\\n.\\n30\\nCHAPTER 3. STOCHASTIC GRADIENT DESCENT\\nProof. Denote by \\u2207t the shorthand for \\u2207f(xt), and ht = f(xt) \\u2212f(x\\u2217).\\nThe stochastic descent lemma is given in the following equation,\\nE[ht+1 \\u2212ht] = E[f(xt+1) \\u2212f(xt)]\\n\\u2264E[\\u2207\\u22a4\\nt (xt+1 \\u2212xt) + \\u03b2\\n2 \\u2225xt+1 \\u2212xt\\u22252]\\n\\u03b2-smoothness\\n= \\u2212E[\\u03b7\\u2207\\u22a4\\nt \\u02dc\\u2207t] + \\u03b2\\n2 \\u03b72 E \\u2225\\u02dc\\u2207t\\u22252\\nalgorithm defn.\\n= \\u2212\\u03b7\\u2225\\u2207t\\u22252 + \\u03b2\\n2 \\u03b72\\u03c32\\nvariance bound.\\nThus, summing up over T iterations, we have for \\u03b7 =\\nq\\nM\\n\\u03b2\\u03c32T ,\\nE\\n\\\"\\n1\\nT\\nT\\nX\\nt=1\\n\\u2225\\u2207t\\u22252\\n#\\n\\u2264\\n1\\nT\\u03b7\\nP\\nt E [ht \\u2212ht+1] + \\u03b7 \\u03b2\\n2 \\u03c32 \\u2264M\\nT\\u03b7 + \\u03b7 \\u03b2\\n2 \\u03c32\\n=\\nq\\nM\\u03b2\\u03c32\\nT\\n+ 1\\n2\\nq\\nM\\u03b2\\u03c32\\nT\\n\\u22642\\nq\\nM\\u03b2\\u03c32\\nT\\n.\\nWe thus conclude that O( 1\\n\\u03b54 ) iterations are needed to \\ufb01nd a point with\\n\\u2225\\u2207f(x)\\u2225\\u2264\\u03b5, as opposed to O( 1\\n\\u03b52 ). However, each iteration takes O(|E|)\\ntime, instead of O(|E|m) time for gradient descent.\\nThis is why SGD is one of the most useful algorithms in machine learning.\\n3.4. BIBLIOGRAPHIC REMARKS\\n31\\n3.4\\nBibliographic remarks\\nFor in depth treatment of backpropagation and the role of deep neural net-\\nworks in machine learning the reader is referred to [25].\\nFor detailed rigorous convergence proofs of \\ufb01rst order methods, see lec-\\nture notes by Nesterov [57] and Nemirovskii [53, 54], as well as the recent\\ntext [13].\\n32\\nCHAPTER 3. STOCHASTIC GRADIENT DESCENT\\nChapter 4\\nGeneralization and\\nNon-Smooth Optimization\\nIn previous chapter we have introduced the framework of mathematical op-\\ntimization within the context of machine learning. We have described the\\nmathematical formulation of several machine learning problems, notably\\ntraining neural networks, as optimization problems. We then described as\\nwell as analyzed the most useful optimization method to solve such formu-\\nlations: stochastic gradient descent.\\nHowever, several important questions arise:\\n1. SGD was analyzed for smooth functions. Can we minimize non-smooth\\nobjectives?\\n2. Given an ERM problem (a.k.a. learning from examples, see \\ufb01rst chap-\\nter), what can we say about generalization to unseen examples? How\\ndoes it a\\ufb00ect optimization?\\n3. Are there faster algorithms than SGD in the context of ML?\\nIn this chapter we address the \\ufb01rst two, and devote the rest of this\\nmanuscript/course to the last question.\\nHow many examples are needed to learn a certain concept? This is a\\nfundamental question of statistical/computational learning theory that has\\nbeen studied for decades (see end of chapter for bibliographic references).\\nThe classical setting of learning from examples is statistical. It assumes\\nexamples are drawn i.i.d from a \\ufb01xed, arbitrary and unknown distribution.\\nThe mathematical optimization formulations that we have derived for the\\nERM problem assume that we have su\\ufb03ciently many examples, such that\\n33\\n34\\nCHAPTER 4. GENERALIZATION\\noptimizing a certain predictor/neural-network/machine on them will result\\nin a solution that is capable of generalizing to unseen examples. The number\\nof examples needed to generalize is called the sample complexity of the prob-\\nlem, and it depends on the concept we are learning as well as the hypothesis\\nclass over which we are trying to optimize.\\nThere are dimensionality notions in the literature, notably the VC-\\ndimension and related notions, that give precise bounds on the sample com-\\nplexity for various hypothesis classes. In this text we take an algorithmic\\napproach, which is also deterministic. Instead of studying sample complex-\\nity, which is non-algorithmic, we study algorithms for regret minimization.\\nWe will show that they imply generalization for a broad class of machines.\\n4.1\\nA note on non-smooth optimization\\nMinimization of a function that is both non-convex and non-smooth is in\\ngeneral hopeless, from an information theoretic perspective. The following\\nimage explains why. The depicted function on the interval [0, 1] has a single\\nlocal/global minimum, and if the crevasse is narrow enough, it cannot be\\nfound by any method other than extensive brute-force search, which can\\ntake arbitrarily long.\\n0\\n1\\nFigure 4.1: Intractability of nonsmooth optimization\\nSince non-convex and non-smooth optimization is hopeless, in the con-\\ntext of non-smooth functions we only consider convex optimization.\\n4.2. MINIMIZING REGRET\\n35\\n4.2\\nMinimizing Regret\\nThe setting we consider for the rest of this chapter is that of online (convex)\\noptimization. In this setting a learner iteratively predicts a point xt \\u2208K in\\na convex set K \\u2286Rd, and then receives a cost according to an adversarially\\nchosen convex function ft \\u2208F from family F.\\nThe goal of the algorithms introduced in this chapter is to minimize\\nworst-case regret, or di\\ufb00erence between total cost and that of best point in\\nhindsight:\\nregret =\\nsup\\nf1,...,fT \\u2208F\\n( T\\nX\\nt=1\\nft(xt) \\u2212min\\nx\\u2208K\\nT\\nX\\nt=1\\nft(x)\\n)\\n.\\nIn order to compare regret to optimization error it is useful to consider\\nthe average regret, or regret/T. Let \\u00afxT = 1\\nT\\nPT\\nt=1 xt be the average decision.\\nIf the functions ft are all equal to a single function f : K 7\\u2192R, then Jensen\\u2019s\\ninequality implies that f(\\u00afxT ) converges to f(x\\u22c6) if the average regret is\\nvanishing, since\\nf(\\u00afxT ) \\u2212f(x\\u22c6) \\u22641\\nT\\nT\\nX\\nt=1\\n[f(xt) \\u2212f(x\\u22c6)] = regret\\nT\\n4.3\\nRegret implies generalization\\nStatistical learning theory for learning from examples postulates that exam-\\nples from a certain concept are sampled i.i.d. from a \\ufb01xed and unknown\\ndistribution. The learners\\u2019 goal is to choose a hypothesis from a certain\\nhypothesis class that can generalize to unseen examples.\\nMore formally, let D be a distribution over labelled examples {ai \\u2208\\nRd, bi \\u2208R} \\u223cD. Let H = {x} , x : Rd 7\\u2192R be a hypothsis class over which\\nwe are trying to learn (such as linear separators, deep neural networks,\\netc.). The generalization error of a hypothesis is the expected error of a\\nhypothesis over randomly chosen examples according to a given loss function\\n\\u2113: R \\u00d7 R 7\\u2192R, which is applied to the prediction of the hypothesis and the\\ntrue label, \\u2113(x(ai), bi). Thus,\\nerror(x) =\\nE\\nai,bi\\u223cD[\\u2113(x(ai), bi)].\\nAn algorithm that attains sublinear regret over the hypothesis class H,\\nw.r.t. loss functions given by ft(x) = fa,b(x) = \\u2113(x(a), b), gives rise to a\\ngeneralizing hypothesis as follows.\\n36\\nCHAPTER 4. GENERALIZATION\\nLemma 4.1. Let \\u00afx = xt for t \\u2208[T] be chose uniformly at random from\\n{x1, ..., xT }.Then, with expectation taken over random choice of \\u00afx as well\\nas choices of ft \\u223cD,\\nE[error(\\u00afx)] \\u2264E[error(x\\u2217)] + regret\\nT\\nProof. By random choice of \\u00afx, we have\\nE[f(\\u00afx)] = E\\n\\\"\\n1\\nT\\nX\\nt\\nf(xt)\\n#\\nUsing the fact that ft \\u223cD, we have\\nE[error(\\u00afx)]\\n= Ef\\u223cD[f(\\u00afx)]\\n= Eft[ 1\\nT\\nP\\nt ft(xt)]\\n\\u2264Eft[ 1\\nT\\nP\\nt ft(x\\u22c6)] + regret\\nT\\n= Ef[f(x\\u22c6)] + regret\\nT\\n= Ef[error(x\\u22c6)] + regret\\nT\\n4.4\\nOnline gradient descent\\nPerhaps the simplest algorithm that applies to the most general setting of\\nonline convex optimization is online gradient descent. This algorithm is an\\nonline version of standard gradient descent for o\\ufb04ine optimization we have\\nseen in the previous chapter.\\nPseudo-code for the algorithm is given in\\nAlgorithm 4, and a conceptual illustration is given in Figure 4.2.\\nIn each iteration, the algorithm takes a step from the previous point in\\nthe direction of the gradient of the previous cost. This step may result in\\na point outside of the underlying convex set. In such cases, the algorithm\\nprojects the point back to the convex set, i.e. \\ufb01nds its closest point in the\\nconvex set. Despite the fact that the next cost function may be completely\\ndi\\ufb00erent than the costs observed thus far, the regret attained by the algo-\\nrithm is sublinear. This is formalized in the following theorem (recall the\\nde\\ufb01nition of G and D from the previous chapter).\\nTheorem 4.2. Online gradient descent with step sizes {\\u03b7t =\\nD\\nG\\n\\u221a\\nt, t \\u2208[T]}\\nguarantees the following for all T \\u22651:\\nregretT =\\nT\\nX\\nt=1\\nft(xt) \\u2212min\\nx\\u22c6\\u2208K\\nT\\nX\\nt=1\\nft(x\\u22c6) \\u22643GD\\n\\u221a\\nT\\n4.4. ONLINE GRADIENT DESCENT\\n37\\nFigure 4.2: Online gradient descent: the iterate xt+1 is derived by advancing\\nxt in the direction of the current gradient \\u2207t, and projecting back into K.\\nAlgorithm 4 online gradient descent\\n1: Input: convex set K, T, x1 \\u2208K, step sizes {\\u03b7t}\\n2: for t = 1 to T do\\n3:\\nPlay xt and observe cost ft(xt).\\n4:\\nUpdate and project:\\nyt+1 = xt \\u2212\\u03b7t\\u2207ft(xt)\\nxt+1 = \\u03a0\\nK(yt+1)\\n5: end for\\nProof. Let x\\u22c6\\u2208arg minx\\u2208K\\nPT\\nt=1 ft(x). De\\ufb01ne \\u2207t \\u225c\\u2207ft(xt). By convexity\\nft(xt) \\u2212ft(x\\u22c6) \\u2264\\u2207\\u22a4\\nt (xt \\u2212x\\u22c6)\\n(4.1)\\nWe \\ufb01rst upper-bound \\u2207\\u22a4\\nt (xt \\u2212x\\u22c6) using the update rule for xt+1 and The-\\norem 2.1 (the Pythagorean theorem):\\n\\u2225xt+1 \\u2212x\\u22c6\\u22252 =\\n\\r\\r\\r\\r\\u03a0\\nK(xt \\u2212\\u03b7t\\u2207t) \\u2212x\\u22c6\\n\\r\\r\\r\\r\\n2\\n\\u2264\\u2225xt \\u2212\\u03b7t\\u2207t \\u2212x\\u22c6\\u22252\\n(4.2)\\n38\\nCHAPTER 4. GENERALIZATION\\nHence,\\n\\u2225xt+1 \\u2212x\\u22c6\\u22252\\n\\u2264\\n\\u2225xt \\u2212x\\u22c6\\u22252 + \\u03b72\\nt \\u2225\\u2207t\\u22252 \\u22122\\u03b7t\\u2207\\u22a4\\nt (xt \\u2212x\\u22c6)\\n2\\u2207\\u22a4\\nt (xt \\u2212x\\u22c6)\\n\\u2264\\n\\u2225xt \\u2212x\\u22c6\\u22252 \\u2212\\u2225xt+1 \\u2212x\\u22c6\\u22252\\n\\u03b7t\\n+ \\u03b7tG2\\n(4.3)\\nSumming (4.1) and (4.3) from t = 1 to T, and setting \\u03b7t =\\nD\\nG\\n\\u221a\\nt (with\\n1\\n\\u03b70 \\u225c0):\\n2\\n T\\nX\\nt=1\\nft(xt) \\u2212ft(x\\u22c6)\\n!\\n\\u22642\\nT\\nX\\nt=1\\n\\u2207\\u22a4\\nt (xt \\u2212x\\u22c6)\\n\\u2264\\nT\\nX\\nt=1\\n\\u2225xt \\u2212x\\u22c6\\u22252 \\u2212\\u2225xt+1 \\u2212x\\u22c6\\u22252\\n\\u03b7t\\n+ G2\\nT\\nX\\nt=1\\n\\u03b7t\\n\\u2264\\nT\\nX\\nt=1\\n\\u2225xt \\u2212x\\u22c6\\u22252\\n\\u0012 1\\n\\u03b7t\\n\\u2212\\n1\\n\\u03b7t\\u22121\\n\\u0013\\n+ G2\\nT\\nX\\nt=1\\n\\u03b7t\\n1\\n\\u03b70\\n\\u225c0,\\n\\u2225xT+1 \\u2212x\\u2217\\u22252 \\u22650\\n\\u2264D2\\nT\\nX\\nt=1\\n\\u0012 1\\n\\u03b7t\\n\\u2212\\n1\\n\\u03b7t\\u22121\\n\\u0013\\n+ G2\\nT\\nX\\nt=1\\n\\u03b7t\\n\\u2264D2 1\\n\\u03b7T\\n+ G2\\nT\\nX\\nt=1\\n\\u03b7t\\ntelescoping series\\n\\u22643DG\\n\\u221a\\nT.\\nThe last inequality follows since \\u03b7t =\\nD\\nG\\n\\u221a\\nt and PT\\nt=1\\n1\\n\\u221a\\nt \\u22642\\n\\u221a\\nT.\\nThe online gradient descent algorithm is straightforward to implement,\\nand updates take linear time given the gradient. However, there is a projec-\\ntion step which may take signi\\ufb01cantly longer.\\n4.5\\nLower bounds\\nTheorem 4.3. Any algorithm for online convex optimization incurs \\u2126(DG\\n\\u221a\\nT)\\nregret in the worst case. This is true even if the cost functions are generated\\nfrom a \\ufb01xed stationary distribution.\\nWe give a sketch of the proof; \\ufb01lling in all details is left as an exercise\\nat the end of this chapter.\\n4.6. ONLINE GRADIENT DESCENT FOR STRONGLY CONVEX FUNCTIONS39\\nConsider an instance of OCO where the convex set K is the n-dimensional\\nhypercube, i.e.\\nK = {x \\u2208Rn , \\u2225x\\u2225\\u221e\\u22641}.\\nThere are 2n linear cost functions, one for each vertex v \\u2208{\\u00b11}n, de\\ufb01ned\\nas\\n\\u2200v \\u2208{\\u00b11}n , fv(x) = v\\u22a4x.\\nNotice that both the diameter of K and the bound on the norm of the cost\\nfunction gradients, denoted G, are bounded by\\nD \\u2264\\nv\\nu\\nu\\nt\\nn\\nX\\ni=1\\n22 = 2\\u221an, G =\\nv\\nu\\nu\\nt\\nn\\nX\\ni=1\\n(\\u00b11)2 = \\u221an\\nThe cost functions in each iteration are chosen at random, with uniform\\nprobability, from the set {fv, v \\u2208{\\u00b11}n}. Denote by vt \\u2208{\\u00b11}n the vertex\\nchosen in iteration t, and denote ft = fvt. By uniformity and independence,\\nfor any t and xt chosen online, Evt[ft(xt)] = Evt[v\\u22a4\\nt xt] = 0. However,\\nE\\nv1,...,vT\\n\\\"\\nmin\\nx\\u2208K\\nT\\nX\\nt=1\\nft(x)\\n#\\n= E\\n\\uf8ee\\n\\uf8f0min\\nx\\u2208K\\nX\\ni\\u2208[n]\\nT\\nX\\nt=1\\nvt(i) \\u00b7 xi\\n\\uf8f9\\n\\uf8fb\\n= n E\\n\\\"\\n\\u2212\\n\\f\\f\\f\\f\\f\\nT\\nX\\nt=1\\nvt(1)\\n\\f\\f\\f\\f\\f\\n#\\ni.i.d. coordinates\\n= \\u2212\\u2126(n\\n\\u221a\\nT).\\nThe last equality is left as exercise 3.\\nThe facts above nearly complete the proof of Theorem 4.3; see the exer-\\ncises at the end of this chapter.\\n4.6\\nOnline gradient descent for strongly convex\\nfunctions\\nThe \\ufb01rst algorithm that achieves regret logarithmic in the number of iter-\\nations is a twist on the online gradient descent algorithm, changing only\\nthe step size. The following theorem establishes logarithmic bounds on the\\nregret if the cost functions are strongly convex.\\n40\\nCHAPTER 4. GENERALIZATION\\nTheorem 4.4. For \\u03b1-strongly convex loss functions, online gradient descent\\nwith step sizes \\u03b7t = 1\\n\\u03b1t achieves the following guarantee for all T \\u22651\\nregretT \\u2264\\nT\\nX\\nt=1\\n1\\n\\u03b1t\\u2225\\u2207t\\u22252 \\u2264G2\\n2\\u03b1 (1 + log T).\\nProof. Let x\\u22c6\\u2208arg minx\\u2208K\\nPT\\nt=1 ft(x). Recall the de\\ufb01nition of regret\\nregretT =\\nT\\nX\\nt=1\\nft(xt) \\u2212\\nT\\nX\\nt=1\\nft(x\\u22c6).\\nDe\\ufb01ne \\u2207t \\u225c\\u2207ft(xt). Applying the de\\ufb01nition of \\u03b1-strong convexity to\\nthe pair of points xt,x\\u2217, we have\\n2(ft(xt) \\u2212ft(x\\u22c6))\\n\\u2264\\n2\\u2207\\u22a4\\nt (xt \\u2212x\\u22c6) \\u2212\\u03b1\\u2225x\\u22c6\\u2212xt\\u22252.\\n(4.4)\\nWe proceed to upper-bound \\u2207\\u22a4\\nt (xt \\u2212x\\u22c6). Using the update rule for xt+1\\nand the Pythagorean theorem 2.1, we get\\n\\u2225xt+1 \\u2212x\\u22c6\\u22252 = \\u2225\\u03a0\\nK(xt \\u2212\\u03b7t\\u2207t) \\u2212x\\u22c6\\u22252 \\u2264\\u2225xt \\u2212\\u03b7t\\u2207t \\u2212x\\u22c6\\u22252.\\nHence,\\n\\u2225xt+1 \\u2212x\\u22c6\\u22252\\n\\u2264\\n\\u2225xt \\u2212x\\u22c6\\u22252 + \\u03b72\\nt \\u2225\\u2207t\\u22252 \\u22122\\u03b7t\\u2207\\u22a4\\nt (xt \\u2212x\\u22c6)\\nand\\n2\\u2207\\u22a4\\nt (xt \\u2212x\\u22c6)\\n\\u2264\\n\\u2225xt \\u2212x\\u22c6\\u22252 \\u2212\\u2225xt+1 \\u2212x\\u22c6\\u22252\\n\\u03b7t\\n+ \\u03b7t\\u2225\\u2207t\\u22252.\\n(4.5)\\n4.7. ONLINE GRADIENT DESCENT IMPLIES SGD\\n41\\nSumming (4.5) from t = 1 to T, setting \\u03b7t =\\n1\\n\\u03b1t (de\\ufb01ne\\n1\\n\\u03b70 \\u225c0), and\\ncombining with (4.4), we have:\\n2\\nT\\nX\\nt=1\\n(ft(xt) \\u2212ft(x\\u22c6))\\n\\u2264\\nT\\nX\\nt=1\\n\\u2225xt \\u2212x\\u22c6\\u22252\\n\\u0012 1\\n\\u03b7t\\n\\u2212\\n1\\n\\u03b7t\\u22121\\n\\u2212\\u03b1\\n\\u0013\\n+\\nT\\nX\\nt=1\\n\\u03b7t\\u2225\\u2207t\\u22252\\nsince 1\\n\\u03b70\\n\\u225c0, \\u2225xT+1 \\u2212x\\u2217\\u22252 \\u22650\\n=\\n0 +\\nT\\nX\\nt=1\\n1\\n\\u03b1t\\u2225\\u2207t\\u22252\\n\\u2264\\nG2\\n\\u03b1 (1 + log T)\\n4.7\\nOnline Gradient Descent implies SGD\\nIn this section we notice that OGD and its regret bounds imply the SGD\\nbounds we have studied in the previous chapter. The main advantage are\\nthe guarantees for non-smooth stochastic optimization, and constrained op-\\ntimization.\\nRecall that in stochastic optimization, the optimizer attempts to mini-\\nmize a convex function over a convex domain as given by the mathematical\\nprogram:\\nmin\\nx\\u2208K f(x).\\nHowever, unlike standard o\\ufb04ine optimization, the optimizer is given access\\nto a noisy gradient oracle, de\\ufb01ned by\\nO(x) \\u225c\\u02dc\\u2207x s.t.\\nE[ \\u02dc\\u2207x] = \\u2207f(x) , E[\\u2225\\u02dc\\u2207x\\u22252] \\u2264G2\\nThat is, given a point in the decision set, a noisy gradient oracle returns\\na random vector whose expectation is the gradient at the point and whose\\nsecond moment is bounded by G2.\\nWe will show that regret bounds for OCO translate to convergence rates\\nfor stochastic optimization. As a special case, consider the online gradient\\n42\\nCHAPTER 4. GENERALIZATION\\ndescent algorithm whose regret is bounded by\\nregretT = O(DG\\n\\u221a\\nT)\\nApplying the OGD algorithm over a sequence of linear functions that are\\nde\\ufb01ned by the noisy gradient oracle at consecutive points, and \\ufb01nally re-\\nturning the average of all points along the way, we obtain the stochastic\\ngradient descent algorithm, presented in Algorithm 5.\\nAlgorithm 5 stochastic gradient descent\\n1: Input: f, K, T, x1 \\u2208K, step sizes {\\u03b7t}\\n2: for t = 1 to T do\\n3:\\nLet \\u02dc\\u2207t = O(xt) and de\\ufb01ne: ft(x) \\u225c\\u27e8\\u02dc\\u2207t, x\\u27e9\\n4:\\nUpdate and project:\\nyt+1 = xt \\u2212\\u03b7t \\u02dc\\u2207t\\nxt+1 = \\u03a0\\nK(yt+1)\\n5: end for\\n6: return \\u00afxT \\u225c1\\nT\\nPT\\nt=1 xt\\nTheorem 4.5. Algorithm 5 with step sizes \\u03b7t =\\nD\\nG\\n\\u221a\\nt guarantees\\nE[f(\\u00afxT )] \\u2264min\\nx\\u22c6\\u2208K f(x\\u22c6) + 3GD\\n\\u221a\\nT\\nProof. By the regret guarantee of OGD, we have\\nE[f(\\u00afxT )] \\u2212f(x\\u22c6)\\n\\u2264E[ 1\\nT\\nX\\nt\\nf(xt)] \\u2212f(x\\u22c6)\\nconvexity of f (Jensen)\\n\\u22641\\nT E[\\nX\\nt\\n\\u27e8\\u2207f(xt), xt \\u2212x\\u22c6\\u27e9]\\nconvexity again\\n= 1\\nT E[\\nX\\nt\\n\\u27e8\\u02dc\\u2207t, xt \\u2212x\\u22c6\\u27e9]\\nnoisy gradient estimator\\n= 1\\nT E[\\nX\\nt\\nft(xt) \\u2212ft(x\\u22c6)]\\nAlgorithm 5, line (3)\\n\\u2264regretT\\nT\\nde\\ufb01nition\\n\\u22643GD\\n\\u221a\\nT\\ntheorem 4.2\\n4.7. ONLINE GRADIENT DESCENT IMPLIES SGD\\n43\\nIt is important to note that in the proof above, we have used the fact\\nthat the regret bounds of online gradient descent hold against an adaptive\\nadversary. This need arises since the cost functions ft de\\ufb01ned in Algorithm\\n5 depend on the choice of decision xt \\u2208K.\\nIn addition, the careful reader may notice that by plugging in di\\ufb00erent\\nstep sizes (also called learning rates) and applying SGD to strongly convex\\nfunctions, one can attain \\u02dcO(1/T) convergence rates. Details of this deriva-\\ntion are left as exercise 1.\\n44\\nCHAPTER 4. GENERALIZATION\\n4.8\\nExercises\\n1. Prove that SGD for a strongly convex function can, with appropriate\\nparameters \\u03b7t, converge as \\u02dcO( 1\\nT ). You may assume that the gradient\\nestimators have Euclidean norms bounded by the constant G.\\n2. Design an OCO algorithm that attains the same asymptotic regret\\nbound as OGD, up to factors logarithmic in G and D, without knowing\\nthe parameters G and D ahead of time.\\n3. In this exercise we prove a tight lower bound on the regret of any\\nalgorithm for online convex optimization.\\n(a) For any sequence of T fair coin tosses, let Nh be the number of\\nhead outcomes and Nt be the number of tails. Give an asymp-\\ntotically tight upper and lower bound on E[|Nh \\u2212Nt|] (i.e., order\\nof growth of this random variable as a function of T, up to mul-\\ntiplicative and additive constants).\\n(b) Consider a 2-expert problem, in which the losses are inversely\\ncorrelated: either expert one incurs a loss of one and the second\\nexpert zero, or vice versa. Use the fact above to design a set-\\nting in which any experts algorithm incurs regret asymptotically\\nmatching the upper bound.\\n(c) Consider the general OCO setting over a convex set K. Design\\na setting in which the cost functions have gradients whose norm\\nis bounded by G, and obtain a lower bound on the regret as\\na function of G, the diameter of K, and the number of game\\niterations.\\n4.9. BIBLIOGRAPHIC REMARKS\\n45\\n4.9\\nBibliographic remarks\\nThe OCO framework was introduced by Zinkevich in [87], where the OGD\\nalgorithm was introduced and analyzed. Precursors to this algorithm, albeit\\nfor less general settings, were introduced and analyzed in [47]. Logarithmic\\nregret algorithms for Online Convex Optimization were introduced and an-\\nalyzed in [32]. For more detailed exposition on this prediction framework\\nand its applications see [31].\\nThe SGD algorithm dates back to Robbins and Monro [67]. Application\\nof SGD to soft-margin SVM training was explored in [74]. Tight conver-\\ngence rates of SGD for strongly convex and non-smooth functions were only\\nrecently obtained in [35],[62],[76].\\n46\\nCHAPTER 4. GENERALIZATION\\nChapter 5\\nRegularization\\nIn this chapter we consider a generalization of the gradient descent called\\nby di\\ufb00erent names in di\\ufb00erent communities (such as mirrored-descent, or\\nregularized-follow-the-leader). The common theme of this generalization is\\ncalled Regularization, a concept that is founded in generalization theory.\\nSince this course focuses on optimization rather than generalization, we\\nshall refer the reader to the generalization aspect of regularization, and\\nfocus hereby on optimization algorithms.\\nWe start by motivating this general family of methods using the funda-\\nmental problem of decision theory.\\n5.1\\nMotivation: prediction from expert advice\\nConsider the following fundamental iterative decision making problem:\\nAt each time step t = 1, 2, . . . , T, the decision maker faces a choice\\nbetween two actions A or B (i.e., buy or sell a certain stock). The decision\\nmaker has assistance in the form of N \\u201cexperts\\u201d that o\\ufb00er their advice. After\\na choice between the two actions has been made, the decision maker receives\\nfeedback in the form of a loss associated with each decision. For simplicity\\none of the actions receives a loss of zero (i.e., the \\u201ccorrect\\u201d decision) and\\nthe other a loss of one.\\nWe make the following elementary observations:\\n1. A decision maker that chooses an action uniformly at random each\\niteration, trivially attains a loss of T\\n2 and is \\u201ccorrect\\u201d 50% of the time.\\n47\\n48\\nCHAPTER 5. REGULARIZATION\\n2. In terms of the number of mistakes, no algorithm can do better in the\\nworst case! In a later exercise, we will devise a randomized setting in\\nwhich the expected number of mistakes of any algorithm is at least T\\n2 .\\nWe are thus motivated to consider a relative performance metric: can\\nthe decision maker make as few mistakes as the best expert in hindsight?\\nThe next theorem shows that the answer in the worst case is negative for a\\ndeterministic decision maker.\\nTheorem 5.1. Let L \\u2264T\\n2 denote the number of mistakes made by the best\\nexpert in hindsight. Then there does not exist a deterministic algorithm that\\ncan guarantee less than 2L mistakes.\\nProof. Assume that there are only two experts and one always chooses op-\\ntion A while the other always chooses option B. Consider the setting in\\nwhich an adversary always chooses the opposite of our prediction (she can\\ndo so, since our algorithm is deterministic). Then, the total number of mis-\\ntakes the algorithm makes is T. However, the best expert makes no more\\nthan T\\n2 mistakes (at every iteration exactly one of the two experts is mis-\\ntaken). Therefore, there is no algorithm that can always guarantee less than\\n2L mistakes.\\nThis observation motivates the design of random decision making algo-\\nrithms, and indeed, the OCO framework gracefully models decisions on a\\ncontinuous probability space. Henceforth we prove Lemmas 5.3 and 5.4 that\\nshow the following:\\nTheorem 5.2. Let \\u03b5 \\u2208(0, 1\\n2). Suppose the best expert makes L mistakes.\\nThen:\\n1. There is an e\\ufb03cient deterministic algorithm that can guarantee less\\nthan 2(1 + \\u03b5)L + 2 log N\\n\\u03b5\\nmistakes;\\n2. There is an e\\ufb03cient randomized algorithm for which the expected num-\\nber of mistakes is at most (1 + \\u03b5)L + log N\\n\\u03b5\\n.\\n5.1. MOTIVATION: PREDICTION FROM EXPERT ADVICE\\n49\\n5.1.1\\nThe weighted majority algorithm\\nThe weighted majority (WM) algorithm is intuitive to describe: each expert\\ni is assigned a weight Wt(i) at every iteration t. Initially, we set W1(i) = 1\\nfor all experts i \\u2208[N]. For all t \\u2208[T] let St(A), St(B) \\u2286[N] be the set of\\nexperts that choose A (and respectively B) at time t. De\\ufb01ne,\\nWt(A) =\\nX\\ni\\u2208St(A)\\nWt(i)\\nWt(B) =\\nX\\ni\\u2208St(B)\\nWt(i)\\nand predict according to\\nat =\\n(\\nA\\nif Wt(A) \\u2265Wt(B)\\nB\\notherwise.\\nNext, update the weights Wt(i) as follows:\\nWt+1(i) =\\n(\\nWt(i)\\nif expert i was correct\\nWt(i)(1 \\u2212\\u03b5)\\nif expert i was wrong ,\\nwhere \\u03b5 is a parameter of the algorithm that will a\\ufb00ect its performance.\\nThis concludes the description of the WM algorithm. We proceed to bound\\nthe number of mistakes it makes.\\nLemma 5.3. Denote by Mt the number of mistakes the algorithm makes\\nuntil time t, and by Mt(i) the number of mistakes made by expert i until\\ntime t. Then, for any expert i \\u2208[N] we have\\nMT \\u22642(1 + \\u03b5)MT (i) + 2 log N\\n\\u03b5\\n.\\nWe can optimize \\u03b5 to minimize the above bound. The expression on the\\nright hand side is of the form f(x) = ax + b/x, that reaches its minimum\\nat x =\\np\\nb/a. Therefore the bound is minimized at \\u03b5\\u22c6=\\np\\nlog N/MT (i).\\nUsing this optimal value of \\u03b5, we get that for the best expert i\\u22c6\\nMT \\u22642MT (i\\u22c6) + O\\n\\u0010p\\nMT (i\\u22c6) log N\\n\\u0011\\n.\\nOf course, this value of \\u03b5\\u22c6cannot be used in advance since we do not know\\nwhich expert is the best one ahead of time (and therefore we do not know the\\nvalue of MT (i\\u22c6)). However, we shall see later on that the same asymptotic\\nbound can be obtained even without this prior knowledge.\\nLet us now prove Lemma 5.3.\\n50\\nCHAPTER 5. REGULARIZATION\\nProof. Let \\u03a6t = PN\\ni=1 Wt(i) for all t \\u2208[T], and note that \\u03a61 = N.\\nNotice that \\u03a6t+1 \\u2264\\u03a6t. However, on iterations in which the WM algo-\\nrithm erred, we have\\n\\u03a6t+1 \\u2264\\u03a6t(1 \\u2212\\u03b5\\n2),\\nthe reason being that experts with at least half of total weight were wrong\\n(else WM would not have erred), and therefore\\n\\u03a6t+1 \\u22641\\n2\\u03a6t(1 \\u2212\\u03b5) + 1\\n2\\u03a6t = \\u03a6t(1 \\u2212\\u03b5\\n2).\\nFrom both observations,\\n\\u03a6t \\u2264\\u03a61(1 \\u2212\\u03b5\\n2)Mt = N(1 \\u2212\\u03b5\\n2)Mt.\\nOn the other hand, by de\\ufb01nition we have for any expert i that\\nWT (i) = (1 \\u2212\\u03b5)MT (i).\\nSince the value of WT (i) is always less than the sum of all weights \\u03a6T , we\\nconclude that\\n(1 \\u2212\\u03b5)MT (i) = WT (i) \\u2264\\u03a6T \\u2264N(1 \\u2212\\u03b5\\n2)MT .\\nTaking the logarithm of both sides we get\\nMT (i) log(1 \\u2212\\u03b5) \\u2264log N + MT log (1 \\u2212\\u03b5\\n2).\\nNext, we use the approximations\\n\\u2212x \\u2212x2 \\u2264log (1 \\u2212x) \\u2264\\u2212x\\n0 < x < 1\\n2,\\nwhich follow from the Taylor series of the logarithm function, to obtain that\\n\\u2212MT (i)(\\u03b5 + \\u03b52) \\u2264log N \\u2212MT\\n\\u03b5\\n2,\\nand the lemma follows.\\n5.1. MOTIVATION: PREDICTION FROM EXPERT ADVICE\\n51\\n5.1.2\\nRandomized weighted majority\\nIn the randomized version of the WM algorithm, denoted RWM, we choose\\nexpert i w.p. pt(i) = Wt(i)/ PN\\nj=1 Wt(j) at time t.\\nLemma 5.4. Let Mt denote the number of mistakes made by RWM until\\niteration t. Then, for any expert i \\u2208[N] we have\\nE[MT ] \\u2264(1 + \\u03b5)MT (i) + log N\\n\\u03b5\\n.\\nThe proof of this lemma is very similar to the previous one, where the factor\\nof two is saved by the use of randomness:\\nProof. As before, let \\u03a6t = PN\\ni=1 Wt(i) for all t \\u2208[T], and note that \\u03a61 = N.\\nLet \\u02dcmt = Mt \\u2212Mt\\u22121 be the indicator variable that equals one if the RWM\\nalgorithm makes a mistake on iteration t. Let mt(i) equal one if the i\\u2019th\\nexpert makes a mistake on iteration t and zero otherwise. Inspecting the\\nsum of the weights:\\n\\u03a6t+1 =\\nX\\ni\\nWt(i)(1 \\u2212\\u03b5mt(i))\\n= \\u03a6t(1 \\u2212\\u03b5\\nX\\ni\\npt(i)mt(i))\\npt(i) =\\nWt(i)\\nP\\nj Wt(j)\\n= \\u03a6t(1 \\u2212\\u03b5 E[ \\u02dcmt])\\n\\u2264\\u03a6te\\u2212\\u03b5E[ \\u02dcmt].\\n1 + x \\u2264ex\\nOn the other hand, by de\\ufb01nition we have for any expert i that\\nWT (i) = (1 \\u2212\\u03b5)MT (i)\\nSince the value of WT (i) is always less than the sum of all weights \\u03a6T , we\\nconclude that\\n(1 \\u2212\\u03b5)MT (i) = WT (i) \\u2264\\u03a6T \\u2264Ne\\u2212\\u03b5E[MT ].\\nTaking the logarithm of both sides we get\\nMT (i) log(1 \\u2212\\u03b5) \\u2264log N \\u2212\\u03b5 E[MT ]\\nNext, we use the approximation\\n\\u2212x \\u2212x2 \\u2264log (1 \\u2212x) \\u2264\\u2212x\\n,\\n0 < x < 1\\n2\\n52\\nCHAPTER 5. REGULARIZATION\\nto obtain\\n\\u2212MT (i)(\\u03b5 + \\u03b52) \\u2264log N \\u2212\\u03b5 E[MT ],\\nand the lemma follows.\\n5.1.3\\nHedge\\nThe RWM algorithm is in fact more general: instead of considering a dis-\\ncrete number of mistakes, we can consider measuring the performance of an\\nexpert by a non-negative real number \\u2113t(i), which we refer to as the loss\\nof the expert i at iteration t. The randomized weighted majority algorithm\\nguarantees that a decision maker following its advice will incur an average\\nexpected loss approaching that of the best expert in hindsight.\\nHistorically, this was observed by a di\\ufb00erent and closely related algorithm\\ncalled Hedge.\\nAlgorithm 6 Hedge\\n1: Initialize: \\u2200i \\u2208[N], W1(i) = 1\\n2: for t = 1 to T do\\n3:\\nPick it \\u223cR Wt, i.e., it = i with probability xt(i) =\\nWt(i)\\nP\\nj Wt(j)\\n4:\\nIncur loss \\u2113t(it).\\n5:\\nUpdate weights Wt+1(i) = Wt(i)e\\u2212\\u03b5\\u2113t(i)\\n6: end for\\nHenceforth, denote in vector notation the expected loss of the algorithm\\nby\\nE[\\u2113t(it)] =\\nN\\nX\\ni=1\\nxt(i)\\u2113t(i) = x\\u22a4\\nt \\u2113t\\nTheorem 5.5. Let \\u21132\\nt denote the N-dimensional vector of square losses,\\ni.e., \\u21132\\nt (i) = \\u2113t(i)2, let \\u03b5 > 0, and assume all losses to be non-negative. The\\nHedge algorithm satis\\ufb01es for any expert i\\u22c6\\u2208[N]:\\nT\\nX\\nt=1\\nx\\u22a4\\nt \\u2113t \\u2264\\nT\\nX\\nt=1\\n\\u2113t(i\\u22c6) + \\u03b5\\nT\\nX\\nt=1\\nx\\u22a4\\nt \\u21132\\nt + log N\\n\\u03b5\\n5.2. THE REGULARIZATION FRAMEWORK\\n53\\nProof. As before, let \\u03a6t = PN\\ni=1 Wt(i) for all t \\u2208[T], and note that \\u03a61 = N.\\nInspecting the sum of weights:\\n\\u03a6t+1\\n= P\\ni Wt(i)e\\u2212\\u03b5\\u2113t(i)\\n= \\u03a6t\\nP\\ni xt(i)e\\u2212\\u03b5\\u2113t(i)\\nxt(i) =\\nWt(i)\\nP\\nj Wt(j)\\n\\u2264\\u03a6t\\nP\\ni xt(i)(1 \\u2212\\u03b5\\u2113t(i) + \\u03b52\\u2113t(i)2))\\nfor x \\u22650,\\ne\\u2212x \\u22641 \\u2212x + x2\\n= \\u03a6t(1 \\u2212\\u03b5x\\u22a4\\nt \\u2113t + \\u03b52x\\u22a4\\nt \\u21132\\nt )\\n\\u2264\\u03a6te\\u2212\\u03b5x\\u22a4\\nt \\u2113t+\\u03b52x\\u22a4\\nt \\u21132\\nt .\\n1 + x \\u2264ex\\nOn the other hand, by de\\ufb01nition, for expert i\\u22c6we have that\\nWT (i\\u22c6) = e\\u2212\\u03b5 PT\\nt=1 \\u2113t(i\\u22c6)\\nSince the value of WT (i\\u22c6) is always less than the sum of all weights \\u03a6t, we\\nconclude that\\nWT (i\\u22c6) \\u2264\\u03a6T \\u2264Ne\\u2212\\u03b5 P\\nt x\\u22a4\\nt \\u2113t+\\u03b52 P\\nt x\\u22a4\\nt \\u21132\\nt .\\nTaking the logarithm of both sides we get\\n\\u2212\\u03b5\\nT\\nX\\nt=1\\n\\u2113t(i\\u22c6) \\u2264log N \\u2212\\u03b5\\nT\\nX\\nt=1\\nx\\u22a4\\nt \\u2113t + \\u03b52\\nT\\nX\\nt=1\\nx\\u22a4\\nt \\u21132\\nt\\nand the theorem follows by simplifying.\\n5.2\\nThe Regularization framework\\nIn the previous section we studied the multiplicative weights update method\\nfor decision making. A natural question is: couldn\\u2019t we have used online\\ngradient descent for the same exact purpose?\\nIndeed, the setting of prediction from expert advice naturally follows\\ninto the framework of online convex optimization. To see this, consider the\\nloss functions given by\\nft(x) = \\u2113\\u22a4\\nt x = E\\ni\\u223cx[\\u2113t(i)],\\nwhich capture the expected loss of choosing an expert from distribution\\nx \\u2208\\u2206n as a linear function.\\n54\\nCHAPTER 5. REGULARIZATION\\nThe regret guarantees we have studied for OGD imply a regret of\\nO(GD\\n\\u221a\\nT) = O(\\n\\u221a\\nnT).\\nHere we have used the fact that the Eucliean diameter of the simplex is two,\\nand that the losses are bounded by one, hence the Euclidean norm of the\\ngradient vector \\u2113t is bounded by \\u221an.\\nIn contrast, the Hedge algorithm attains regret of O(\\u221aT log n) for the\\nsame problem. How can we explain this discrepancy?!\\n5.2.1\\nThe RFTL algorithm\\nBoth OGD and Hedge are, in fact, instantiations of a more general meta-\\nalgorithm called RFTL (Regularized-Follow-The-Leader).\\nIn an OCO setting of regret minimization, the most straightforward\\napproach for the online player is to use at any time the optimal decision\\n(i.e., point in the convex set) in hindsight. Formally, let\\nxt+1 = arg min\\nx\\u2208K\\nt\\nX\\n\\u03c4=1\\nf\\u03c4(x).\\nThis \\ufb02avor of strategy is known as \\u201c\\ufb01ctitious play\\u201d in economics, and has\\nbeen named \\u201cFollow the Leader\\u201d (FTL) in machine learning. It is not hard\\nto see that this simple strategy fails miserably in a worst-case sense. That\\nis, this strategy\\u2019s regret can be linear in the number of iterations, as the\\nfollowing example shows: Consider K = [\\u22121, 1], let f1(x) = 1\\n2x, and let f\\u03c4\\nfor \\u03c4 = 2, . . . , T alternate between \\u2212x or x. Thus,\\nt\\nX\\n\\u03c4=1\\nf\\u03c4(x) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1\\n2x,\\nt is odd\\n\\u22121\\n2x,\\notherwise\\nThe FTL strategy will keep shifting between xt = \\u22121 and xt = 1, always\\nmaking the wrong choice.\\nThe intuitive FTL strategy fails in the example above because it is un-\\nstable. Can we modify the FTL strategy such that it won\\u2019t change decisions\\noften, thereby causing it to attain low regret?\\nThis question motivates the need for a general means of stabilizing the\\nFTL method. Such a means is referred to as \\u201cregularization\\u201d.\\n5.2. THE REGULARIZATION FRAMEWORK\\n55\\nAlgorithm 7 Regularized Follow The Leader\\n1: Input: \\u03b7 > 0, regularization function R, and a convex compact set K.\\n2: Let x1 = arg minx\\u2208K {R(x)}.\\n3: for t = 1 to T do\\n4:\\nPredict xt.\\n5:\\nObserve the payo\\ufb00function ft and let \\u2207t = \\u2207ft(xt).\\n6:\\nUpdate\\nxt+1 = arg min\\nx\\u2208K\\n(\\n\\u03b7\\nt\\nX\\ns=1\\n\\u2207\\u22a4\\ns x + R(x)\\n)\\n7: end for\\nThe generic RFTL meta-algorithm is de\\ufb01ned in Algorithm 7. The reg-\\nularization function R is assumed to be strongly convex, smooth, and twice\\ndi\\ufb00erentiable.\\n5.2.2\\nMirrored Descent\\nAn alternative view of this algorithm is in terms of iterative updates, which\\ncan be spelled out using the above de\\ufb01nition directly. The resulting algo-\\nrithm is called \\u201dMirrored Descent\\u201d.\\nOMD is an iterative algorithm that computes the current decision using a\\nsimple gradient update rule and the previous decision, much like OGD. The\\ngenerality of the method stems from the update being carried out in a \\u201cdual\\u201d\\nspace, where the duality notion is de\\ufb01ned by the choice of regularization:\\nthe gradient of the regularization function de\\ufb01nes a mapping from Rn onto\\nitself, which is a vector \\ufb01eld. The gradient updates are then carried out in\\nthis vector \\ufb01eld.\\nFor the RFTL algorithm the intuition was straightforward\\u2014the regular-\\nization was used to ensure stability of the decision. For OMD, regularization\\nhas an additional purpose: regularization transforms the space in which gra-\\ndient updates are performed. This transformation enables better bounds in\\nterms of the geometry of the space.\\nThe OMD algorithm comes in two \\ufb02avors: an agile and a lazy version.\\nThe lazy version keeps track of a point in Euclidean space and projects onto\\nthe convex decision set K only at decision time. In contrast, the agile version\\nmaintains a feasible point at all times, much like OGD.\\n56\\nCHAPTER 5. REGULARIZATION\\nAlgorithm 8 Online Mirrored Descent\\n1: Input: parameter \\u03b7 > 0, regularization function R(x).\\n2: Let y1 be such that \\u2207R(y1) = 0 and x1 = arg minx\\u2208K BR(x||y1).\\n3: for t = 1 to T do\\n4:\\nPlay xt.\\n5:\\nObserve the payo\\ufb00function ft and let \\u2207t = \\u2207ft(xt).\\n6:\\nUpdate yt according to the rule:\\n[Lazy version]\\n\\u2207R(yt+1) = \\u2207R(yt) \\u2212\\u03b7 \\u2207t\\n[Agile version]\\n\\u2207R(yt+1) = \\u2207R(xt) \\u2212\\u03b7 \\u2207t\\nProject according to BR:\\nxt+1 = arg min\\nx\\u2208K\\nBR(x||yt+1)\\n7: end for\\nA myriad of questions arise, but \\ufb01rst, let us see how does this algorithm\\ngive rise to both OGD.\\nWe note that there are other important special cases of the RFTL meta-\\nalgorithm: those are derived with matrix-norm regularization\\u2014namely, the\\nvon Neumann entropy function, and the log-determinant function, as well\\nas self-concordant barrier regularization. Perhaps most importantly for opti-\\nmization, also the AdaGrad algorithm is obtained via changing regularization\\u2014\\nwhich we shall explore in detail in the next chapter.\\n5.2.3\\nDeriving online gradient descent\\nTo derive the online gradient descent algorithm, we take R(x) =\\n1\\n2\\u2225x \\u2212\\nx0\\u22252\\n2 for an arbitrary x0 \\u2208K. Projection with respect to this divergence\\nis the standard Euclidean projection (left as an exercise), and in addition,\\n\\u2207R(x) = x\\u2212x0. Hence, the update rule for the OMD Algorithm 8 becomes:\\nxt = \\u03a0\\nK(yt), yt = yt\\u22121 \\u2212\\u03b7\\u2207t\\u22121\\nlazy version\\nxt = \\u03a0\\nK(yt), yt = xt\\u22121 \\u2212\\u03b7\\u2207t\\u22121\\nagile version\\nThe latter algorithm is exactly online gradient descent, as described in\\nAlgorithm 4 in Chapter 4. Furthermore, both variants are identical for the\\ncase in which K is the unit ball.\\n5.3. TECHNICAL BACKGROUND: REGULARIZATION FUNCTIONS57\\nWe later prove general regret bounds that will imply a O(GD\\n\\u221a\\nT) regret\\nfor OGD as a special case of mirrored descent.\\n5.2.4\\nDeriving multiplicative updates\\nLet R(x) = x log x = P\\ni xi log xi be the negative entropy function, where\\nlog x is to be interpreted elementwise. Then \\u2207R(x) = 1 + log x, and hence\\nthe update rules for the OMD algorithm become:\\nxt = arg min\\nx\\u2208K\\nBR(x||yt), log yt = log yt\\u22121 \\u2212\\u03b7\\u2207t\\u22121\\nlazy version\\nxt = arg min\\nx\\u2208K\\nBR(x||yt), log yt = log xt\\u22121 \\u2212\\u03b7\\u2207t\\u22121\\nagile version\\nWith this choice of regularizer, a notable special case is the experts\\nproblem we encountered in \\u00a75.1, for which the decision set K is the n-\\ndimensional simplex \\u2206n = {x \\u2208Rn\\n+ | P\\ni xi = 1}. In this special case, the\\nprojection according to the negative entropy becomes scaling by the \\u21131 norm\\n(left as an exercise), which implies that both update rules amount to the\\nsame algorithm:\\nxt+1(i) =\\nxt(i) \\u00b7 e\\u2212\\u03b7\\u2207t(i)\\nPn\\nj=1 xt(j) \\u00b7 e\\u2212\\u03b7\\u2207t(j) ,\\nwhich is exactly the Hedge algorithm! The general theorem we shall prove\\nhenceforth recovers the O(\\u221aT log n) bound for prediction from expert advice\\nfor this algorithm.\\n5.3\\nTechnical background: regularization functions\\nIn the rest of this chapter we analyze the mirrored descent algorithm. For\\nthis purpose, consider regularization functions, denoted R : K 7\\u2192R, which\\nare strongly convex and smooth (recall de\\ufb01nitions in \\u00a72.1).\\nAlthough it is not strictly necessary, we assume that the regularization\\nfunctions in this chapter are twice di\\ufb00erentiable over K and, for all points\\nx \\u2208int(K) in the interior of the decision set, have a Hessian \\u22072R(x) that\\nis, by the strong convexity of R, positive de\\ufb01nite.\\nWe denote the diameter of the set K relative to the function R as\\nDR =\\nr\\nmax\\nx,y\\u2208K{R(x) \\u2212R(y)}\\n58\\nCHAPTER 5. REGULARIZATION\\nHenceforth we make use of general norms and their dual. The dual norm\\nto a norm \\u2225\\u00b7 \\u2225is given by the following de\\ufb01nition:\\n\\u2225y\\u2225\\u2217\\u225cmax\\n\\u2225x\\u2225\\u22641\\u27e8x, y\\u27e9\\nA positive de\\ufb01nite matrix A gives rise to the matrix norm \\u2225x\\u2225A =\\n\\u221a\\nx\\u22a4Ax.\\nThe dual norm of a matrix norm is \\u2225x\\u2225\\u2217\\nA = \\u2225x\\u2225A\\u22121.\\nThe generalized Cauchy-Schwarz theorem asserts \\u27e8x, y\\u27e9\\u2264\\u2225x\\u2225\\u2225y\\u2225\\u2217and\\nin particular for matrix norms, \\u27e8x, y\\u27e9\\u2264\\u2225x\\u2225A\\u2225y\\u2225\\u2217\\nA.\\nIn our derivations, we usually consider matrix norms with respect to\\n\\u22072R(x), the Hessian of the regularization function R(x). In such cases, we\\nuse the notation\\n\\u2225x\\u2225y \\u225c\\u2225x\\u2225\\u22072R(y)\\nand similarly\\n\\u2225x\\u2225\\u2217\\ny \\u225c\\u2225x\\u2225\\u2207\\u22122R(y)\\nA crucial quantity in the analysis with regularization is the remainder\\nterm of the Taylor approximation of the regularization function, and es-\\npecially the remainder term of the \\ufb01rst order Taylor approximation. The\\ndi\\ufb00erence between the value of the regularization function at x and the value\\nof the \\ufb01rst order Taylor approximation is known as the Bregman divergence,\\ngiven by\\nDe\\ufb01nition 5.6. Denote by BR(x||y) the Bregman divergence with respect\\nto the function R, de\\ufb01ned as\\nBR(x||y) = R(x) \\u2212R(y) \\u2212\\u2207R(y)\\u22a4(x \\u2212y)\\nFor twice di\\ufb00erentiable functions, Taylor expansion and the mean-value\\ntheorem assert that the Bregman divergence is equal to the second derivative\\nat an intermediate point, i.e., (see exercises)\\nBR(x||y) = 1\\n2\\u2225x \\u2212y\\u22252\\nz,\\nfor some point z \\u2208[x, y], meaning there exists some \\u03b1 \\u2208[0, 1] such that\\nz = \\u03b1x+(1\\u2212\\u03b1)y. Therefore, the Bregman divergence de\\ufb01nes a local norm,\\nwhich has a dual norm. We shall denote this dual norm by\\n\\u2225\\u00b7 \\u2225\\u2217\\nx,y \\u225c\\u2225\\u00b7 \\u2225\\u2217\\nz.\\n5.4. REGRET BOUNDS FOR MIRRORED DESCENT\\n59\\nWith this notation we have\\nBR(x||y) = 1\\n2\\u2225x \\u2212y\\u22252\\nx,y.\\nIn online convex optimization, we commonly refer to the Bregman divergence\\nbetween two consecutive decision points xt and xt+1.\\nIn such cases, we\\nshorthand notation for the norm de\\ufb01ned by the Bregman divergence with\\nrespect to R on the intermediate point in [xt, xt+1] as \\u2225\\u00b7 \\u2225t \\u225c\\u2225\\u00b7 \\u2225xt,xt+1.\\nThe latter norm is called the local norm at iteration t. With this notation,\\nwe have BR(xt||xt+1) = 1\\n2\\u2225xt \\u2212xt+1\\u22252\\nt .\\nFinally, we consider below generalized projections that use the Bregman\\ndivergence as a distance instead of a norm. Formally, the projection of a\\npoint y according to the Bregman divergence with respect to function R is\\ngiven by\\narg min\\nx\\u2208K\\nBR(x||y)\\n5.4\\nRegret bounds for Mirrored Descent\\nIn this subsection we prove regret bounds for the agile version of the RFTL\\nalgorithm. The analysis is quite di\\ufb00erent than the one for the lazy version,\\nand of independent interest.\\nTheorem 5.7. The RFTL Algorithm 8 attains for every u \\u2208K the following\\nbound on the regret:\\nregretT \\u22642\\u03b7\\nT\\nX\\nt=1\\n\\u2225\\u2207t\\u2225\\u22172\\nt + R(u) \\u2212R(x1)\\n\\u03b7\\n.\\nIf an upper bound on the local norms is known, i.e. \\u2225\\u2207t\\u2225\\u2217\\nt \\u2264GR for all\\ntimes t, then we can further optimize over the choice of \\u03b7 to obtain\\nregretT \\u22642DRGR\\n\\u221a\\n2T.\\nProof. Since the functions ft are convex, for any x\\u2217\\u2208K,\\nft(xt) \\u2212ft(x\\u2217) \\u2264\\u2207ft(xt)\\u22a4(xt \\u2212x\\u2217).\\n60\\nCHAPTER 5. REGULARIZATION\\nThe following property of Bregman divergences follows easily from the def-\\ninition: for any vectors x, y, z,\\n(x \\u2212y)\\u22a4(\\u2207R(z) \\u2212\\u2207R(y)) = BR(x, y) \\u2212BR(x, z) + BR(y, z).\\nCombining both observations,\\n2(ft(xt) \\u2212ft(x\\u2217)) \\u22642\\u2207ft(xt)\\u22a4(xt \\u2212x\\u2217)\\n= 1\\n\\u03b7(\\u2207R(yt+1) \\u2212\\u2207R(xt))\\u22a4(x\\u2217\\u2212xt)\\n= 1\\n\\u03b7[BR(x\\u2217, xt) \\u2212BR(x\\u2217, yt+1) + BR(xt, yt+1)]\\n\\u22641\\n\\u03b7[BR(x\\u2217, xt) \\u2212BR(x\\u2217, xt+1) + BR(xt, yt+1)]\\nwhere the last inequality follows from the generalized Pythagorean inequality\\n(see [15] Lemma 11.3), as xt+1 is the projection w.r.t the Bregman divergence\\nof yt+1 and x\\u2217\\u2208K is in the convex set. Summing over all iterations,\\n2regret\\n\\u2264\\n1\\n\\u03b7[BR(x\\u2217, x1) \\u2212BR(x\\u2217, xT )] +\\nT\\nX\\nt=1\\n1\\n\\u03b7BR(xt, yt+1)\\n\\u2264\\n1\\n\\u03b7D2 +\\nT\\nX\\nt=1\\n1\\n\\u03b7BR(xt, yt+1)\\n(5.1)\\nWe proceed to bound BR(xt, yt+1). By de\\ufb01nition of Bregman divergence,\\nand the generalized Cauchy-Schwartz inequality,\\nBR(xt, yt+1) + BR(yt+1, xt) = (\\u2207R(xt) \\u2212\\u2207R(yt+1))\\u22a4(xt \\u2212yt+1)\\n= \\u03b7\\u2207ft(xt)\\u22a4(xt \\u2212yt+1)\\n\\u2264\\u03b7\\u2225\\u2207ft(xt)\\u2225\\u2217\\u2225xt \\u2212yt+1\\u2225\\n\\u22641\\n2\\u03b72G2\\n\\u2217+ 1\\n2\\u2225xt \\u2212yt+1\\u22252.\\nwhere in the last inequality follows from (a\\u2212b)2 \\u22650. Thus, by our assump-\\ntion BR(x, y) \\u22651\\n2\\u2225x \\u2212y\\u22252, we have\\nBR(xt, yt+1) \\u22641\\n2\\u03b72G2\\n\\u2217+ 1\\n2\\u2225xt \\u2212yt+1\\u22252 \\u2212BR(yt+1, xt) \\u22641\\n2\\u03b72G2\\n\\u2217.\\nPlugging back into Equation (5.1), and by non-negativity of the Bregman\\ndivergence, we get\\nregret \\u22641\\n2[1\\n\\u03b7D2 + 1\\n2\\u03b7TG2\\n\\u2217] \\u2264DG\\u2217\\n\\u221a\\nT ,\\n5.4. REGRET BOUNDS FOR MIRRORED DESCENT\\n61\\nby taking \\u03b7 =\\nD\\n2\\n\\u221a\\nTG\\u2217\\n62\\nCHAPTER 5. REGULARIZATION\\n5.5\\nExercises\\n1.\\n(a) Show that the dual norm to a matrix norm given by A \\u227b0\\ncorresponds to the matrix norm of A\\u22121.\\n(b) Prove the generalized Cauchy-Schwarz inequality for any norm,\\ni.e.,\\n\\u27e8x, y\\u27e9\\u2264\\u2225x\\u2225\\u2225y\\u2225\\u2217\\n2. Prove that the Bregman divergence is equal to the local norm at an\\nintermediate point, that is:\\nBR(x||y) = 1\\n2\\u2225x \\u2212y\\u22252\\nz,\\nwhere z \\u2208[x, y] and the interval [x, y] is de\\ufb01ned as\\n[x, y] = {v = \\u03b1x + (1 \\u2212\\u03b1)y , \\u03b1 \\u2208[0, 1]}\\n3. Let R(x) = 1\\n2\\u2225x\\u2212x0\\u22252 be the (shifted) Euclidean regularization func-\\ntion.\\nProve that the corresponding Bregman divergence is the Eu-\\nclidean metric. Conclude that projections with respect to this diver-\\ngence are standard Euclidean projections.\\n4. Prove that both agile and lazy versions of the OMD meta-algorithm\\nare equivalent in the case that the regularization is Euclidean and the\\ndecision set is the Euclidean ball.\\n5. For this problem the decision set is the n-dimensional simplex. Let\\nR(x) = x log x be the negative entropy regularization function. Prove\\nthat the corresponding Bregman divergence is the relative entropy,\\nand prove that the diameter DR of the n-dimensional simplex with\\nrespect to this function is bounded by log n. Show that projections\\nwith respect to this divergence over the simplex amounts to scaling by\\nthe \\u21131 norm.\\n6. \\u2217A set K \\u2286Rd is symmetric if x \\u2208K implies \\u2212x \\u2208K. Symmetric\\nsets gives rise to a natural de\\ufb01nition of a norm. De\\ufb01ne the function\\n\\u2225\\u00b7 \\u2225K : Rd 7\\u2192R as\\n\\u2225x\\u2225K = arg min\\n\\u03b1>0\\n\\u001a 1\\n\\u03b1x \\u2208K\\n\\u001b\\nProve that \\u2225\\u00b7 \\u2225K is a norm if and only if K is convex.\\n5.6. BIBLIOGRAPHIC REMARKS\\n63\\n5.6\\nBibliographic Remarks\\nRegularization in the context of online learning was \\ufb01rst studied in [26]\\nand [48]. The in\\ufb02uential paper of Kalai and Vempala [45] coined the term\\n\\u201cfollow-the-leader\\u201d and introduced many of the techniques that followed\\nin OCO. The latter paper studies random perturbation as a regularization\\nand analyzes the follow-the-perturbed-leader algorithm, following an early\\ndevelopment by [29] that was overlooked in learning for many years.\\nIn the context of OCO, the term follow-the-regularized-leader was coined\\nin [73, 71], and at roughly the same time an essentially identical algorithm\\nwas called \\u201cRFTL\\u201d in [1]. The equivalence of RFTL and Online Mirrored\\nDescent was observed by [34].\\n64\\nCHAPTER 5. REGULARIZATION\\nChapter 6\\nAdaptive Regularization\\nIn the previous chapter we have studied a geometric extension of online /\\nstochastic / determinisitic gradient descent. The technique to achieve it is\\ncalled regularization, and we have seen how for the problem of prediction\\nfrom expert advice, it can potentially given exponential improvements in\\nthe dependence on the dimension.\\nA natural question that arises is whether we can automatically learn the\\noptimal regularization, i.e. best algorithm from the mirrored-descent class,\\nfor the problem at hand?\\nThe answer is positive in a strong sense: it is theoretically possible to\\nlearn the optimal regularization online and in a data-speci\\ufb01c way. Not only\\nthat, the resulting algorithms exhibit the most signi\\ufb01cant speedups in train-\\ning deep neural networks from all accelerations studied thus far.\\n6.1\\nAdaptive Learning Rates: Intuition\\nThe intuition for adaptive regularization is simple: consider an optimization\\nproblem which is axis-aligned, in which each coordinate is independent of\\nthe rest. It is reasonable to \\ufb01ne tune the learning rate for each coordinate\\nseparately - to achieve optimal convergence in that particular subspace of\\nthe problem, independently of the rest.\\nThus, it is reasonable to change the SGD update rule from xt+1 \\u2190\\nxt \\u2212\\u03b7\\u2207t, to the more robust\\nxt+1 \\u2190xt \\u2212Dt\\u2207t,\\nwhere Dt is a diagonal matrix that contains in coordinate (i, i) the learning\\nrate for coordinate i in the gradient.\\nRecall from the previous sections\\n65\\n66\\nCHAPTER 6. ADAPTIVE REGULARIZATION\\nthat the optimal learning rate for stochastic non-convex optimization is of\\nthe order O( 1\\n\\u221a\\nt). More precisely, in Theorem 3.4, we have seen that this\\nlearning rate should be on the order of O(\\n1\\n\\u221a\\nt\\u03c32 ), where \\u03c32 is the variance of\\nthe stochastic gradients. The empirical estimator of the latter is P\\ni<t \\u2225\\u2207i\\u22252.\\nThus, the robust version of stochastic gradient descent for smooth non-\\nconvex optimization should behave as the above equation, with\\nDt(i, i) =\\n1\\npP\\ni<t \\u2207t(i)2 .\\nThis is exactly the diagonal version of the AdaGrad algorithm! We continue\\nto rigorously derive it and prove its performance guarantee.\\n6.2\\nA Regularization Viewpoint\\nIn the previous chapter we have introduced regularization as a general\\nmethodology for deriving online convex optimization algorithms. Theorem\\n5.7 bounds the regret of the Mirrored Descent algorithm for any strongly\\nconvex regularizer as\\nregretT \\u2264max\\nu\\u2208K\\ns\\n2\\nX\\nt\\n\\u2225\\u2207t\\u2225\\u22172\\nt BR(u||x1).\\nIn addition, we have seen how to derive the online gradient descent and the\\nmultiplicative weights algorithms as special cases of the RFTL methodology.\\nWe consider the following question: thus far we have thought of R as\\na strongly convex function. But which strongly convex function should we\\nchoose to minimize regret? This is a deep and di\\ufb03cult question which has\\nbeen considered in the optimization literature since its early developments.\\nThe ML approach is to learn the optimal regularization online. That is,\\na regularizer that adapts to the sequence of cost functions and is in a sense\\nthe \\u201coptimal\\u201d regularization to use in hindsight. We formalize this in the\\nnext section.\\n6.3\\nTools from Matrix Calculus\\nMany of the inequalities that we are familiar with for positive real numbers\\nhold for positive semi-de\\ufb01nite matrices as well.\\nWe henceforth need the\\nfollowing inequality, which is left as an exercise,\\n6.4. THE ADAGRAD ALGORITHM AND ITS ANALYSIS\\n67\\nProposition 6.1. For positive de\\ufb01nite matrices A \\u227dB \\u227b0:\\n2Tr((A \\u2212B)1/2) + Tr(A\\u22121/2B) \\u22642Tr(A1/2).\\nNext, we require a structural result which explicitly gives the optimal\\nregularization as a function of the gradients of the cost functions. For a\\nproof see the exercises.\\nProposition 6.2. Let A \\u227d0. The minimizer of the following minimization\\nproblem:\\nmin\\nX\\nTr(X\\u22121A)\\nsubject to X \\u227d0\\nTr(X) \\u22641,\\nis X = A1/2/Tr(A1/2), and the minimum objective value is Tr2(A1/2).\\n6.4\\nThe AdaGrad Algorithm and Its Analysis\\nTo be more formal, let us consider the set of all strongly convex regulariza-\\ntion functions with a \\ufb01xed and bounded Hessian in the set\\n\\u2200x \\u2208K . \\u22072R(x) = \\u22072 \\u2208H \\u225c{X \\u2208Rn\\u00d7n ; Tr(X) \\u22641 , X \\u227d0}\\nThe set H is a restricted class of regularization functions (which does not\\ninclude the entropic regularization). However, it is a general enough class\\nto capture online gradient descent along with any rotation of the Euclidean\\nregularization.\\nAlgorithm 9 AdaGrad (Full Matrix version)\\n1: Input: parameters \\u03b7, x1 \\u2208K.\\n2: Initialize: S0 = G0 = 0,\\n3: for t = 1 to T do\\n4:\\nPredict xt, su\\ufb00er loss ft(xt).\\n5:\\nUpdate:\\nSt = St\\u22121 + \\u2207t\\u2207\\u22a4\\nt , Gt = St1/2\\nyt+1 = xt \\u2212\\u03b7G\\u22121\\nt \\u2207t\\nxt+1 = arg min\\nx\\u2208K\\n\\u2225yt+1 \\u2212x\\u22252\\nGt\\n6: end for\\n68\\nCHAPTER 6. ADAPTIVE REGULARIZATION\\nThe problem of learning the optimal regularization has given rise to Algo-\\nrithm 9, known as the AdaGrad (Adaptive subGradient method) algorithm.\\nIn the algorithm de\\ufb01nition and throughout this chapter, the notation A\\u22121\\nrefers to the Moore-Penrose pseudoinverse of the matrix A. Perhaps sur-\\nprisingly, the regret of AdaGrad is at most a constant factor larger than the\\nminimum regret of all RFTL algorithm with regularization functions whose\\nHessian is \\ufb01xed and belongs to the class H. The regret bound on AdaGrad\\nis formally stated in the following theorem.\\nTheorem 6.3. Let {xt} be de\\ufb01ned by Algorithm 9 with parameters \\u03b7 = D,\\nwhere\\nD = max\\nu\\u2208K \\u2225u \\u2212x1\\u22252.\\nThen for any x\\u22c6\\u2208K,\\nregretT (AdaGrad) \\u22642D\\ns\\nmin\\nH\\u2208H\\nX\\nt\\n\\u2225\\u2207t\\u2225\\u22172\\nH .\\nBefore proving this theorem, notice that it delivers on one of the promised\\naccounts: comparing to the bound of Theorem 5.7 and ignoring the diameter\\nD and dimensionality, the regret bound is as good as the regret of RFTL\\nfor the class of regularization functions.\\nWe proceed to prove Theorem 6.3. First, a direct corollary of Proposition\\n6.2 is that\\nCorollary 6.4.\\ns\\nmin\\nH\\u2208H\\nX\\nt\\n\\u2225\\u2207t\\u2225\\u22172\\nH\\n=\\nq\\nminH\\u2208H Tr(H\\u22121 P\\nt \\u2207t\\u2207\\u22a4\\nt )\\n= Tr\\nqP\\nt \\u2207t\\u2207\\u22a4\\nt = Tr(GT )\\nHence, to prove Theorem 6.3, it su\\ufb03ces to prove the following lemma.\\nLemma 6.5.\\nregretT (AdaGrad) \\u22642DTr(GT ) = 2D\\ns\\nmin\\nH\\u2208H\\nX\\nt\\n\\u2225\\u2207t\\u2225\\u22172\\nH .\\nProof. By the de\\ufb01nition of yt+1:\\nyt+1 \\u2212x\\u22c6= xt \\u2212x\\u22c6\\u2212\\u03b7Gt\\u22121\\u2207t,\\n(6.1)\\n6.4. THE ADAGRAD ALGORITHM AND ITS ANALYSIS\\n69\\nand\\nGt(yt+1 \\u2212x\\u22c6) = Gt(xt \\u2212x\\u22c6) \\u2212\\u03b7\\u2207t.\\n(6.2)\\nMultiplying the transpose of (6.1) by (6.2) we get\\n(yt+1 \\u2212x\\u22c6)\\u22a4Gt(yt+1 \\u2212x\\u22c6) =\\n(xt\\u2212x\\u22c6)\\u22a4Gt(xt\\u2212x\\u22c6) \\u22122\\u03b7\\u2207\\u22a4\\nt (xt\\u2212x\\u22c6) + \\u03b72\\u2207\\u22a4\\nt G\\u22121\\nt \\u2207t.\\n(6.3)\\nSince xt+1 is the projection of yt+1 in the norm induced by Gt, we have (see\\n\\u00a72.1.1)\\n(yt+1 \\u2212x\\u22c6)\\u22a4Gt(yt+1 \\u2212x\\u22c6) = \\u2225yt+1 \\u2212x\\u22c6\\u22252\\nGt \\u2265\\u2225xt+1 \\u2212x\\u22c6\\u22252\\nGt.\\nThis inequality is the reason for using generalized projections as opposed\\nto standard projections, which were used in the analysis of online gradient\\ndescent (see \\u00a74.4 Equation (4.2)). This fact together with (6.3) gives\\n\\u2207\\u22a4\\nt (xt\\u2212x\\u22c6) \\u2264\\u03b7\\n2\\u2207\\u22a4\\nt G\\u22121\\nt \\u2207t + 1\\n2\\u03b7\\n\\u0000\\u2225xt \\u2212x\\u22c6\\u22252\\nGt \\u2212\\u2225xt+1 \\u2212x\\u22c6\\u22252\\nGt\\n\\u0001\\n.\\nNow, summing up over t = 1 to T we get that\\nT\\nX\\nt=1\\n\\u2207\\u22a4\\nt (xt \\u2212x\\u22c6) \\u2264\\u03b7\\n2\\nT\\nX\\nt=1\\n\\u2207\\u22a4\\nt G\\u22121\\nt \\u2207t + 1\\n2\\u03b7\\u2225x1 \\u2212x\\u22c6\\u22252\\nG0\\n(6.4)\\n+ 1\\n2\\u03b7\\nT\\nX\\nt=1\\n\\u0010\\n\\u2225xt \\u2212x\\u22c6\\u22252\\nGt \\u2212\\u2225xt \\u2212x\\u22c6\\u22252\\nGt\\u22121\\n\\u0011\\n\\u22121\\n2\\u03b7\\u2225xT+1 \\u2212x\\u22c6\\u22252\\nGT\\n\\u2264\\u03b7\\n2\\nT\\nX\\nt=1\\n\\u2207\\u22a4\\nt G\\u22121\\nt \\u2207t + 1\\n2\\u03b7\\nT\\nX\\nt=1\\n(xt\\u2212x\\u22c6)\\u22a4(Gt \\u2212Gt\\u22121)(xt\\u2212x\\u22c6).\\nIn the last inequality we use the fact that G0 = 0. We proceed to bound\\neach of the terms above separately.\\nLemma 6.6. With St, Gt as de\\ufb01ned in Algorithm 9,\\nT\\nX\\nt=1\\n\\u2207\\u22a4\\nt G\\u22121\\nt \\u2207t \\u22642\\nT\\nX\\nt=1\\n\\u2207\\u22a4\\nt G\\u22121\\nT \\u2207t \\u22642Tr(GT ).\\nProof. We prove the lemma by induction. The base case follows since\\n\\u2207\\u22a4\\n1 G\\u22121\\n1 \\u22071 = Tr(G\\u22121\\n1 \\u22071\\u2207\\u22a4\\n1 )\\n= Tr(G\\u22121\\n1 G2\\n1)\\n= Tr(G1).\\n70\\nCHAPTER 6. ADAPTIVE REGULARIZATION\\nAssuming the lemma holds for T \\u22121, we get by the inductive hypothesis\\nT\\nX\\nt=1\\n\\u2207\\u22a4\\nt G\\u22121\\nt \\u2207t \\u22642Tr(GT\\u22121) + \\u2207\\u22a4\\nT G\\u22121\\nT \\u2207T\\n= 2Tr((G2\\nT \\u2212\\u2207T \\u2207\\u22a4\\nT )1/2) + Tr(G\\u22121\\nT \\u2207T \\u2207\\u22a4\\nT )\\n\\u22642Tr(GT ).\\nHere, the last inequality is due to the matrix inequality 6.1.\\nLemma 6.7.\\nT\\nX\\nt=1\\n(xt\\u2212x\\u22c6)\\u22a4(Gt \\u2212Gt\\u22121)(xt\\u2212x\\u22c6) \\u2264D2Tr(GT ).\\nProof. By de\\ufb01nition St \\u227dSt\\u22121, and hence Gt \\u227dGt\\u22121. Thus,\\nT\\nX\\nt=1\\n(xt\\u2212x\\u22c6)\\u22a4(Gt \\u2212Gt\\u22121)(xt\\u2212x\\u22c6)\\n\\u2264\\nT\\nX\\nt=1\\nD2\\u03bbmax(Gt \\u2212Gt\\u22121)\\n\\u2264D2\\nT\\nX\\nt=1\\nTr(Gt \\u2212Gt\\u22121)\\nA \\u227d0 \\u21d2\\u03bbmax(A) \\u2264Tr(A)\\n= D2\\nT\\nX\\nt=1\\n(Tr(Gt) \\u2212Tr(Gt\\u22121))\\nlinearity of the trace\\n\\u2264D2Tr(GT ).\\nPlugging both lemmas into Equation (6.4), we obtain\\nT\\nX\\nt=1\\n\\u2207\\u22a4\\nt (xt \\u2212x\\u22c6) \\u2264\\u03b7Tr(GT ) + 1\\n2\\u03b7D2Tr(GT ) \\u22642DTr(GT ).\\n6.5. DIAGONAL ADAGRAD\\n71\\n6.5\\nDiagonal AdaGrad\\nThe AdaGrad algorithm maintains potentially dense matrices, and requires\\nthe computation of the square root of these matrices. This is usually pro-\\nhibitive in machine learning applications in which the dimension is very\\nlarge.\\nFortunately, the same ideas can be applied with almost no com-\\nputational overhead on top of vanilla SGD, using the diagonal version of\\nAdaGrad given by:\\nAlgorithm 10 AdaGrad (diagonal version)\\n1: Input: parameters \\u03b7, x1 \\u2208K.\\n2: Initialize: S0 = G0 = 0,\\n3: for t = 1 to T do\\n4:\\nPredict xt, su\\ufb00er loss ft(xt).\\n5:\\nUpdate:\\nSt = St\\u22121 + diag(\\u2207t\\u2207\\u22a4\\nt ), Gt = St1/2\\nyt+1 = xt \\u2212\\u03b7G\\u22121\\nt \\u2207t\\nxt+1 = arg min\\nx\\u2208K\\n\\u2225yt+1 \\u2212x\\u22252\\nGt\\n6: end for\\nIn contrast to the full-matrix version, this version can be implemented in\\nlinear time and space, since diagonal matrices can be manipulated as vectors.\\nThus, memory overhead is only a single d-dimensional vector, which is used\\nto represent the diagonal preconditioning (regularization) matrix, and the\\ncomputational overhead is a few vector manipulations per iteration.\\nVery similar to the full matrix case, the diagonal AdaGrad algorithm\\ncan be analyzed and the following performance bound obtained:\\nTheorem 6.8. Let {xt} be de\\ufb01ned by Algorithm 10 with parameters \\u03b7 =\\nD\\u221e, where\\nD\\u221e= max\\nu\\u2208K \\u2225u \\u2212x1\\u2225\\u221e,\\nand let diag(H) be the set of all diagonal matrices in H.\\nThen for any\\nx\\u22c6\\u2208K,\\nregretT (D-AdaGrad) \\u22642D\\u221e\\ns\\nmin\\nH\\u2208diag(H)\\nX\\nt\\n\\u2225\\u2207t\\u2225\\u22172\\nH .\\n72\\nCHAPTER 6. ADAPTIVE REGULARIZATION\\n6.6\\nState-of-the-art: from Adam to Shampoo and\\nbeyond\\nSince the introduction of the adaptive regularization technique in the con-\\ntext of regret minimization, several improvements were introduced that now\\ncompose state-of-the-art. A few notable advancements include:\\nAdaDelta: The algorithm keeps an exponential average of past gradients and uses\\nthat in the update step.\\nAdam: Adds a sliding window to AdaGrad, as well as adding a form of mo-\\nmentum via estimating the second moments of past gradients and\\nadjusting the update accordingly.\\nShampoo: Interpolates between full-matrix and diagonal adagrad in the context\\nof deep neural networks: use of the special layer structure to reduce\\nmemory constraints.\\nAdaFactor: Suggests a Shampoo-like approach to reduce memory footprint even\\nfurther, to allow the training of huge models.\\nGGT: While full-matrix AdaGrad is computationally slow due to the cost\\nof manipulating matrices, this algorithm uses recent gradients (a thin\\nmatrix G), and via linear algebraic manipulations reduces computa-\\ntion by never computing GG\\u22a4, but rather only G\\u22a4G, which is low\\ndimensional.\\nSM3 , ET: Diagonal AdaGrad requires an extra O(n) memory to store diag(Gt).\\nThese algorithms, inspired by AdaFactor, approximate Gt as a low\\nrank tensor to save memory and computation.\\n6.7. EXERCISES\\n73\\n6.7\\nExercises\\n1. \\u2217Prove that for positive de\\ufb01nite matrices A \\u227dB \\u227b0 it holds that\\n(a) A1/2 \\u227dB1/2\\n(b) 2Tr((A \\u2212B)1/2) + Tr(A\\u22121/2B) \\u22642Tr(A1/2).\\n2. \\u2217Consider the following minimization problem where A \\u227b0:\\nmin\\nX\\nTr(X\\u22121A)\\nsubject to\\nX \\u227b0\\nTr(X) \\u22641.\\nProve that its minimizer is given by X = A1/2/Tr(A1/2), and the\\nminimum is obtained at Tr2(A1/2).\\n74\\nCHAPTER 6. ADAPTIVE REGULARIZATION\\n6.8\\nBibliographic Remarks\\nThe AdaGrad algorithm was introduced in [19, 18], its diagonal version\\nwas also discovered in parallel in [52]. Adam [46] and RMSprop [39] are\\nwidely used methods based on adaptive regularization. A cleaner analysis\\nwas recently proposed in [27], see also [17].\\nAdaptive regularization has received much attention recently, see e.g.,\\n[60, 85]. Newer algorithmic developments on adaptive regularization include\\nShampoo [28], GGT [3], AdaFactor [77], Extreme Tensoring [16] and SM3\\n[6].\\nChapter 7\\nVariance Reduction\\nIn the previous chapter we have studied the \\ufb01rst of our three acceleration\\ntechniques over SGD, adaptive regularization, which is a geometric tool for\\nacceleration. In this chapter we introduce the second \\ufb01rst-order accelera-\\ntion technique, called variance reduction. This technique is probabilistic in\\nnature, and applies to more restricted settings of mathematical optimiza-\\ntion in which the objective function has a \\ufb01nite-sum structure. Namely, we\\nconsider optimization problems of the form\\nmin\\nx\\u2208K f(x) , f(x) = 1\\nm\\nm\\nX\\ni=1\\nfi(x) .\\n(7.1)\\nSuch optimization problems are canonical in training of ML models, con-\\nvex and non-convex. However, in the context of machine learning we should\\nremember that the ultimate goal is generalization rather than training.\\n7.1\\nVariance reduction: Intuition\\nThe intuition for variance reduction is simple, and comes from trying to\\nimprove the naive convergence bounds for SGD that we have covered in the\\n\\ufb01rst lesson.\\nRecall the SGD update rule xt+1 \\u2190xt\\u2212\\u03b7 \\u02c6\\u2207t, in which \\u02c6\\u2207t is an unbiased\\nestimator for the gradient such that\\nE[ \\u02c6\\u2207t] = \\u2207t , E[\\u2225\\u02c6\\u2207t\\u22252\\n2] \\u2264\\u03c32.\\nWe have seen in Theorem 3.4, that for this update rule,\\nE\\n\\\"\\n1\\nT\\nX\\nt\\n\\u2225\\u2207t\\u22252\\n#\\n\\u22642\\nr\\nM\\u03b2\\u03c32\\nT\\n.\\n75\\n76\\nCHAPTER 7. VARIANCE REDUCTION\\nThe convergence is proportional to the second moment of the gradient esti-\\nmator, and thus it makes sense to try to reduce this second moment. The\\nvariance reduction technique attempts to do so by using the average of all\\nprevious gradients, as we show next.\\n7.2\\nSetting and de\\ufb01nitions\\nWe consider the ERM optimization problem over an average of loss func-\\ntions. Before we begin, we need a few preliminaries and assumptions:\\n1. We denote distance to optimality according to function value as\\nht = f(xt) \\u2212f(x\\u2217),\\nand in the k\\u2019th epoch of an algorithm, we denote hk\\nt = f(xk\\nt ) \\u2212f(x\\u2217).\\n2. We denote \\u02dchk = max\\n\\b\\n4hk\\n0 , 8\\u03b1D2\\nk\\n\\t\\nover an epoch.\\n3. Assume all stochastic gradients have bounded second moments\\n\\u2225\\u02c6\\n\\u2207t\\u22252\\n2 \\u2264\\u03c32.\\n4. We will assume that the individual functions fi in formulation (7.1)\\nare also \\u02c6\\u03b2-smooth and have \\u02c6\\u03b2-Lipschitz gradient, namely\\n\\u2225\\u2207fi(x) \\u2212\\u2207fi(y)\\u2225\\u2264\\u02c6\\u03b2\\u2225x \\u2212y\\u2225.\\n5. We will use, proved in Lemma 2.3, that for \\u03b2-smooth and \\u03b1-strongly\\nconvex f we have\\nht \\u22651\\n2\\u03b2 \\u2225\\u2207t\\u22252\\nand\\n\\u03b1\\n2 d2\\nt = \\u03b1\\n2 \\u2225xt \\u2212x\\u2217\\u22252 \\u2264ht \\u22641\\n2\\u03b1\\u2225\\u2207t\\u22252.\\n6. Recall that a function f is \\u03b3-well-conditioned if it is \\u03b2-smooth, \\u03b1-\\nstrongly convex and \\u03b3 \\u2264\\u03b1\\n\\u03b2 .\\n7.3. THE VARIANCE REDUCTION ADVANTAGE\\n77\\n7.3\\nThe variance reduction advantage\\nConsider gradient descent for \\u03b3-well conditioned functions, and speci\\ufb01cally\\nused for ML training as in formulation (7.1) . It is well known that GD\\nattains linear convergence rate as we now prove for completeness:\\nTheorem 7.1. For unconstrained minimization of \\u03b3-well-conditioned func-\\ntions and \\u03b7t = 1\\n\\u03b2, the Gradient Descent Algorithm 2 converges as\\nht+1 \\u2264h1e\\u2212\\u03b3t.\\nProof.\\nht+1 \\u2212ht = f(xt+1) \\u2212f(xt)\\n\\u2264\\u2207\\u22a4\\nt (xt+1 \\u2212xt) + \\u03b2\\n2 \\u2225xt+1 \\u2212xt\\u22252\\n\\u03b2-smoothness\\n= \\u2212\\u03b7t\\u2225\\u2207t\\u22252 + \\u03b2\\n2 \\u03b72\\nt \\u2225\\u2207t\\u22252\\nalgorithm defn.\\n= \\u22121\\n2\\u03b2 \\u2225\\u2207t\\u22252\\nchoice of \\u03b7t = 1\\n\\u03b2\\n\\u2264\\u2212\\u03b1\\n\\u03b2 ht.\\nby (2.1)\\nThus,\\nht+1 \\u2264ht(1 \\u2212\\u03b1\\n\\u03b2 ) \\u2264\\u00b7 \\u00b7 \\u00b7 \\u2264h1(1 \\u2212\\u03b3)t \\u2264h1e\\u2212\\u03b3t\\nwhere the last inequality follows from 1 \\u2212x \\u2264e\\u2212x for all x \\u2208R.\\nHowever, what is the overall computational cost? Assuming that we can\\ncompute the gradient of each loss function corresponding to the individual\\ntraining examples in O(d) time, the overall running time to compute the\\ngradient is O(md).\\nIn order to attain approximation \\u03b5 to the objective, the algorithm re-\\nquires O( 1\\n\\u03b3 log 1\\n\\u03b5) iterations, as per the Theorem above. Thus, the overall\\nrunning time becomes O( md\\n\\u03b3 log 1\\n\\u03b5). As we show below, variance reduction\\ncan reduce this running time to be O((m+ 1\\n\\u02dc\\u03b32 )d log 1\\n\\u03b5), where \\u02dc\\u03b3 is a di\\ufb00erent\\ncondition number for the same problem, that is in general smaller than the\\noriginal. Thus, in one line, the variance reduction advantage can be sum-\\nmarized as:\\nmd\\n\\u03b3 log 1\\n\\u03b5 7\\u2192(m + 1\\n\\u02dc\\u03b32 )d log 1\\n\\u03b5 .\\n78\\nCHAPTER 7. VARIANCE REDUCTION\\n7.4\\nA simple variance-reduced algorithm\\nThe following simple variance-reduced algorithm illustrates the main ideas\\nof the technique.\\nThe algorithm is a stochastic gradient descent variant\\nwhich proceeds in epochs. Strong convexity implies that the distance to the\\noptimum shrinks with function value, so it is safe to decrease the distance\\nupper bound every epoch.\\nThe main innovation is in line 7, which constructs the gradient estimator.\\nInstead of the usual trick - which is to sample one example at random - here\\nthe estimator uses the entire gradient computed at the beginning of the\\ncurrent epoch.\\nAlgorithm 11 Epoch GD\\n1: Input: f, T, x1\\n0 \\u2208K, upper bound D1 \\u2265\\u2225x1\\n0 \\u2212x\\u2217\\u2225, step sizes {\\u03b7t}\\n2: for k = 1 to log 1\\n\\u03b5 do\\n3:\\nLet BDk(xk\\n0) be the ball of radius Dk around xk\\n0.\\n4:\\ncompute full gradient \\u2207k\\n0 = \\u2207f(xk\\n0)\\n5:\\nfor t = 1 to T do\\n6:\\nSample it \\u2208[m] uniformly at random, let ft = fit.\\n7:\\nconstruct stochastic gradient \\u02c6\\u2207k\\nt = \\u2207ft(xk\\nt ) \\u2212\\u2207ft(xk\\n0) + \\u2207k\\n0\\n8:\\nLet yk\\nt+1 = xk\\nt \\u2212\\u03b7t \\u02c6\\u2207k\\nt , xt+1 = \\u03a0BDk(xk\\n0) (yt+1)\\n9:\\nend for\\n10:\\nSet xk+1\\n0\\n= 1\\nT\\nPT\\nt=1 xk\\nt . Dk+1 \\u2190Dk/2.\\n11: end for\\n12: return x0\\nT+1\\nThe main guarantee for this algorithm is the following theorem, which\\ndelivers upon the aforementioned improvement,\\nTheorem 7.2. Algorithm 11 returns an \\u03b5-approximate solution to optimiza-\\ntion problem (7.1) in total time\\nO\\n\\u0012\\u0012\\nm + 1\\n\\u02dc\\u03b32\\n\\u0013\\nd log 1\\n\\u03b5\\n\\u0013\\n.\\nLet \\u02dc\\u03b3 = \\u03b1\\n\\u02c6\\u03b2 < \\u03b3. Then the proof of this theorem follows from the following\\nlemma.\\nLemma 7.3. For T = \\u02dcO\\n\\u0010\\n1\\n\\u02dc\\u03b32\\n\\u0011\\n, we have\\nE[\\u02dchk+1] \\u22641\\n2\\n\\u02dchk.\\n7.4. A SIMPLE VARIANCE-REDUCED ALGORITHM\\n79\\nProof. As a \\ufb01rst step, we bound the variance of the gradients. Due to the\\nfact that xk\\nt \\u2208BDk(xk\\n0), we have that for k\\u2032 > k, \\u2225xk\\nt \\u2212xk\\u2032\\nt \\u22252 \\u22644D2\\nk. Thus,\\n\\u2225\\u02c6\\u2207k\\nt \\u22252\\n= \\u2225\\u2207ft(xk\\nt ) \\u2212\\u2207ft(xk\\n0) + \\u2207f(xk\\n0)\\u22252\\nde\\ufb01nition\\n\\u22642\\u2225\\u2207ft(xk\\nt ) \\u2212\\u2207ft(xk\\n0)\\u22252 + 2\\u2225\\u2207f(xk\\n0)\\u22252\\n(a + b)2 \\u22642a2 + 2b2\\n\\u22642\\u02c6\\u03b22\\u2225xk\\nt \\u2212xk\\n0\\u22252 + 4\\u03b2hk\\n0\\nsmoothness\\n\\u22648\\u02c6\\u03b22D2\\nk + 4\\u03b2hk\\n0\\nprojection step\\n\\u2264\\u02c6\\u03b22 1\\n\\u03b1\\u02dchk + 4\\u03b2hk\\n0 \\u2264\\u02dchk(\\n\\u02c6\\u03b22\\n\\u03b1 + \\u03b2)\\nNext, using the regret bound for strongly convex functions, we have\\nE[hk+1\\n0\\n]\\n\\u2264E[ 1\\nT\\nP\\nt hk\\nt ]\\nJensen\\n\\u2264\\n1\\n\\u03b1T E[P\\nt\\n1\\nt \\u2225\\u02c6\\u2207k\\nt \\u22252]\\nTheorem 4.4\\n\\u2264\\n1\\n\\u03b1T\\nP\\nt\\n1\\nt \\u02dchk(\\n\\u02c6\\u03b22\\n\\u03b1 + \\u03b2)\\nabove\\n\\u2264log T\\nT \\u02dchk( 1\\n\\u02dc\\u03b32 + 1\\n\\u03b3 )\\n\\u02dc\\u03b3 = \\u03b1\\n\\u02c6\\u03b2\\nWhich implies the Lemma by choice of T, de\\ufb01nition of \\u02dchk = max\\n\\b\\n4hk\\n0 , 8\\u03b1D2\\nk\\n\\t\\n,\\nand exponential decrease of Dk.\\nThe expectation is over the stochastic gradient de\\ufb01nition, and is required\\nfor using Theorem 4.4.\\nTo obtain the theorem from the lemma above, we need to strengthen\\nit to a high probability statement using a martingale argument.\\nThis is\\npossible since the randomness in construction of the stochastic gradients is\\ni.i.d.\\nThe lemma now implies the theorem by noting that O(log 1\\n\\u03b5) epochs\\nsu\\ufb03ces to get \\u03b5-approximation. Each epoch requires the computation of one\\nfull gradient, in time O(md), and \\u02dcO( 1\\n\\u02dc\\u03b32 ) iterations that require stochastic\\ngradient computation, in time O(d).\\n80\\nCHAPTER 7. VARIANCE REDUCTION\\n7.5\\nBibliographic Remarks\\nThe variance reduction technique was \\ufb01rst introduced as part of the SAG\\nalgorithm [70]. Since then a host of algorithms were developed using the\\ntechnique. The simplest exposition of the technique was given in [44]. The\\nexposition in this chapter is developed from the Epoch GD algorithm [37],\\nwhich uses a related technique for stochastic strongly convex optimization,\\nas developed in [86].\\nChapter 8\\nNesterov Acceleration\\nIn previous chapters we have studied our bread and butter technique, SGD,\\nas well as two acceleration techniques of adaptive regularization and variance\\nreduction.\\nIn this chapter we study the historically earliest acceleration\\ntechnique, known as Nesterov acceleration, or simply \\u201cacceleration\\u201d.\\nFor smooth and convex functions, Nesterov acceleration improves the\\nconvergence rate to optimality to O( 1\\nT 2 ), a quadratic improvement over\\nvanilla gradient descent. Similar accelerations are possible when the func-\\ntion is also strongly convex: an accelerated rate of e\\u2212\\u221a\\u03b3T , where \\u03b3 is the\\ncondition number, vs. e\\u2212\\u03b3T of vanilla gradient descent. This improvement\\nis theoretically very signi\\ufb01cant.\\nHowever, in terms of applicability, Nesterov acceleration is theoretically\\nthe most restricted in the context of machine learning: it requires a smooth\\nand convex objective. More importantly, the learning rates of this method\\nare very brittle, and the method is not robust to noise. Since noise is pre-\\ndominant in machine learning, the theoretical guarantees in stochastic op-\\ntimization environments are very restricted.\\nHowever, the heuristic of momentum, which historically inspired acceler-\\nation, is extremely useful for non-convex stochastic optimization (although\\nnot known to yield signi\\ufb01cant improvements in theory).\\n8.1\\nAlgorithm and implementation\\nNesterov acceleration applies to the general setting of constrained smooth\\nconvex optimization:\\nmin\\nx\\u2208Rd f(x).\\n(8.1)\\n81\\n82\\nCHAPTER 8. NESTEROV ACCELERATION\\nFor simplicity of presentation, we restrict ourselves to the unconstrained\\nconvex and smooth case.\\nNevertheless, the method can be extended to\\nconstrained smooth convex, and potentially strongly convex, settings.\\nThe simple method presented in Algorithm 12 below is computationally\\nequivalent to gradient descent.\\nThe only overhead is saving three state\\nvectors (that can be reduced to two) instead of one for gradient descent.\\nThe following simple accelerated algorithm illustrates the main ideas of the\\ntechnique.\\nAlgorithm 12 Simpli\\ufb01ed Nesterov Acceleration\\n1: Input: f, T, initial point x0, parameters \\u03b7, \\u03b2, \\u03c4.\\n2: for t = 1 to T do\\n3:\\nSet xt+1 = \\u03c4zt + (1 \\u2212\\u03c4)yt, and denote \\u2207t+1 = \\u2207f(xt+1).\\n4:\\nLet yt+1 = xt+1 \\u22121\\n\\u03b2\\u2207t+1\\n5:\\nLet zt+1 = zt \\u2212\\u03b7\\u2207t+1\\n6: end for\\n7: return \\u00afx = 1\\nT\\nP\\nt xt\\n8.2\\nAnalysis\\nThe main guarantee for this algorithm is the following theorem.\\nTheorem 8.1. Algorithm 12 converges to an \\u03b5-approximate solution to op-\\ntimization problem (8.1) in O( 1\\n\\u221a\\u03b5) iterations.\\nThe proof starts with the following lemma which follows from our earlier\\nstandard derivations.\\nLemma 8.2.\\n\\u03b7\\u2207\\u22a4\\nt+1(zt \\u2212x\\u2217) \\u22642\\u03b72\\u03b2(f(xt+1) \\u2212f(yt+1)) +\\n\\u0002\\n\\u2225zt \\u2212x\\u2217\\u22252 \\u2212\\u2225zt+1 \\u2212x\\u2217\\u22252\\u0003\\n.\\nProof. The proof is very similar to that of Theorem 4.2. By de\\ufb01nition of zt,\\n1\\n\\u2225zt+1 \\u2212x\\u2217\\u22252\\n= \\u2225zt \\u2212\\u03b7\\u2207t+1 \\u2212x\\u2217\\u22252\\n= \\u2225zt \\u2212x\\u2217\\u22252 \\u2212\\u03b7\\u2207\\u22a4\\nt+1(zt \\u2212x\\u2217) + \\u03b72\\u2225\\u2207t+1\\u22252\\n\\u2264\\u2225zt \\u2212x\\u2217\\u22252 \\u2212\\u03b7\\u2207\\u22a4\\nt+1(zt \\u2212x\\u2217) + 2\\u03b72\\u03b2(f(xt+1) \\u2212f(yt+1))\\nLemma 2.3 part 3\\n1Henceforth we use Lemma 2.3 part 3.\\nThis proof of this Lemma shows that for\\ny = x \\u22121\\n\\u03b2 \\u2207f(x), it holds that f(x) \\u2212f(y) \\u2265\\n1\\n2\\u03b2 \\u2225\\u2207f(x)\\u22252.\\n8.2. ANALYSIS\\n83\\nLemma 8.3. For 2\\u03b7\\u03b2 = 1\\u2212\\u03c4\\n\\u03c4 , we have that\\n\\u03b7\\u2207\\u22a4\\nt+1(xt+1 \\u2212x\\u2217) \\u22642\\u03b72\\u03b2(f(yt) \\u2212f(yt+1)) +\\n\\u0002\\n\\u2225zt \\u2212x\\u2217\\u22252 \\u2212\\u2225zt+1 \\u2212x\\u2217\\u22252\\u0003\\n.\\nProof.\\n\\u03b7\\u2207\\u22a4\\nt+1(xt+1 \\u2212x\\u2217) \\u2212\\u03b7\\u2207\\u22a4\\nt+1(zt \\u2212x\\u2217)\\n= \\u03b7\\u2207\\u22a4\\nt+1(xt+1 \\u2212zt)\\n= (1\\u2212\\u03c4)\\u03b7\\n\\u03c4\\n\\u2207\\u22a4\\nt+1(yt \\u2212xt+1)\\n\\u03c4(xt+1 \\u2212zt) = (1 \\u2212\\u03c4)(yt \\u2212xt+1)\\n\\u2264(1\\u2212\\u03c4)\\u03b7\\n\\u03c4\\n(f(yt) \\u2212f(xt+1)).\\nconvexity\\nThus, in combination with Lemma 8.2, and the condition of the Lemma, we\\nget the inequality.\\nWe can now sketch the proof of the main theorem.\\nProof. Telescope Lemma 8.3 for all iterations to obtain:\\nThT\\n= T(f(\\u00afx) \\u2212f(x\\u2217))\\n\\u2264P\\nt \\u2207\\u22a4\\nt (xt \\u2212x\\u2217)\\n\\u22642\\u03b7\\u03b2 P\\nt(f(yt) \\u2212f(yt+1)) + 1\\n\\u03b7\\nP\\nt\\n\\u0002\\n\\u2225zt \\u2212x\\u2217\\u22252 \\u2212\\u2225zt+1 \\u2212x\\u2217\\u22252\\u0003\\n\\u22642\\u03b7\\u03b2(f(y1) \\u2212f(yT+1)) + 1\\n\\u03b7\\n\\u0002\\n\\u2225z1 \\u2212x\\u2217\\u22252 \\u2212\\u2225zT+1 \\u2212x\\u2217\\u22252\\u0003\\n\\u2264\\u221a2\\u03b2h1D,\\noptimizing \\u03b7\\nwhere h1 is an upper bound on the distance f(y1) \\u2212f(x\\u2217), and D bounds\\nthe Euclidean distance of zt to the optimum. Thus, we get a recurrence of\\nthe form\\nhT \\u2264\\n\\u221ah1\\nT\\n.\\nRestarting Algorithm 12 and adapting the learning rate according to hT\\ngives a rate of convergence of O( 1\\nT 2 ) to optimality.\\n84\\nCHAPTER 8. NESTEROV ACCELERATION\\n8.3\\nBibliographic Remarks\\nAccelerated rates of order O( 1\\nT 2 ) were obtained by Nemirovski as early as\\nthe late seventies. The \\ufb01rst practically e\\ufb03cient accelerated algorithm is due\\nto Nesterov [56] , see also [57]. The simpli\\ufb01ed proof presented hereby is due\\nto [5].\\nChapter 9\\nThe conditional gradient\\nmethod\\nIn many computational and learning scenarios the main bottleneck of opti-\\nmization, both online and o\\ufb04ine, is the computation of projections onto the\\nunderlying decision set (see \\u00a72.1.1). In this chapter we discuss projection-free\\nmethods in convex optimization, and some of their applications in machine\\nlearning.\\nThe motivating example throughout this chapter is the problem of ma-\\ntrix completion, which is a widely used and accepted model in the con-\\nstruction of recommendation systems. For matrix completion and related\\nproblems, projections amount to expensive linear algebraic operations and\\navoiding them is crucial in big data applications.\\nHenceforth we describe the conditional gradient algorithm, also known as\\nthe Frank-Wolfe algorithm. Afterwards, we describe problems for which lin-\\near optimization can be carried out much more e\\ufb03ciently than projections.\\nWe conclude with an application to exploration in reinforcement learning.\\n9.1\\nReview: relevant concepts from linear algebra\\nThis chapter addresses rectangular matrices, which model applications such\\nas recommendation systems naturally. Consider a matrix X \\u2208Rn\\u00d7m. A\\nnon-negative number \\u03c3 \\u2208R+ is said to be a singular value for X if there are\\ntwo vectors u \\u2208Rn, v \\u2208Rm such that\\nX\\u22a4u = \\u03c3v,\\nXv = \\u03c3u.\\n85\\n86\\nCHAPTER 9. THE CONDITIONAL GRADIENT METHOD\\nThe vectors u, v are called the left and right singular vectors respectively.\\nThe non-zero singular values are the square roots of the eigenvalues of the\\nmatrix XX\\u22a4(and X\\u22a4X). The matrix X can be written as\\nX = U\\u03a3V \\u22a4, U \\u2208Rn\\u00d7\\u03c1 , V \\u22a4\\u2208R\\u03c1\\u00d7m,\\nwhere \\u03c1 = min{n, m}, the matrix U is an orthogonal basis of the left singular\\nvectors of X, the matrix V is an orthogonal basis of right singular vectors,\\nand \\u03a3 is a diagonal matrix of singular values. This form is called the singular\\nvalue decomposition for X.\\nThe number of non-zero singular values for X is called its rank, which\\nwe denote by k \\u2264\\u03c1. The nuclear norm of X is de\\ufb01ned as the \\u21131 norm of its\\nsingular values, and denoted by\\n\\u2225X\\u2225\\u2217=\\n\\u03c1\\nX\\ni=1\\n\\u03c3i\\nIt can be shown (see exercises) that the nuclear norm is equal to the trace\\nof the square root of the matrix times its transpose, i.e.,\\n\\u2225X\\u2225\\u2217= Tr(\\n\\u221a\\nX\\u22a4X)\\nWe denote by A \\u2022 B the inner product of two matrices as vectors in Rn\\u00d7m,\\nthat is\\nA \\u2022 B =\\nn\\nX\\ni=1\\nm\\nX\\nj=1\\nAijBij = Tr(AB\\u22a4)\\n9.2\\nMotivation: matrix completion and recommen-\\ndation systems\\nMedia recommendations have changed signi\\ufb01cantly with the advent of the\\nInternet and rise of online media stores. The large amounts of data collected\\nallow for e\\ufb03cient clustering and accurate prediction of users\\u2019 preferences\\nfor a variety of media.\\nA well-known example is the so called \\u201cNet\\ufb02ix\\nchallenge\\u201d\\u2014a competition of automated tools for recommendation from a\\nlarge dataset of users\\u2019 motion picture preferences.\\nOne of the most successful approaches for automated recommendation\\nsystems, as proven in the Net\\ufb02ix competition, is matrix completion. Perhaps\\nthe simplest version of the problem can be described as follows.\\nThe entire dataset of user-media preference pairs is thought of as a\\npartially-observed matrix. Thus, every person is represented by a row in\\n9.2. MOTIVATION\\n87\\nthe matrix, and every column represents a media item (movie). For sim-\\nplicity, let us think of the observations as binary\\u2014a person either likes or\\ndislikes a particular movie. Thus, we have a matrix M \\u2208{0, 1, \\u2217}n\\u00d7m where\\nn is the number of persons considered, m is the number of movies at our\\nlibrary, and 0/1 and \\u2217signify \\u201cdislike\\u201d, \\u201clike\\u201d and \\u201cunknown\\u201d respectively:\\nMij =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n0,\\nperson i dislikes movie j\\n1,\\nperson i likes movie j\\n\\u2217,\\npreference unknown\\n.\\nThe natural goal is to complete the matrix, i.e. correctly assign 0 or 1 to\\nthe unknown entries. As de\\ufb01ned so far, the problem is ill-posed, since any\\ncompletion would be equally good (or bad), and no restrictions have been\\nplaced on the completions.\\nThe common restriction on completions is that the \\u201ctrue\\u201d matrix has\\nlow rank. Recall that a matrix X \\u2208Rn\\u00d7m has rank k < \\u03c1 = min{n, m} if\\nand only if it can be written as\\nX = UV , U \\u2208Rn\\u00d7k, V \\u2208Rk\\u00d7m.\\nThe intuitive interpretation of this property is that each entry in M\\ncan be explained by only k numbers.\\nIn matrix completion this means,\\nintuitively, that there are only k factors that determine a persons preference\\nover movies, such as genre, director, actors and so on.\\nNow the simplistic matrix completion problem can be well-formulated\\nas in the following mathematical program. Denote by \\u2225\\u00b7 \\u2225OB the Euclidean\\nnorm only on the observed (non starred) entries of M, i.e.,\\n\\u2225X\\u22252\\nOB =\\nX\\nMij\\u0338=\\u2217\\nX2\\nij.\\nThe mathematical program for matrix completion is given by\\nmin\\nX\\u2208Rn\\u00d7m\\n1\\n2\\u2225X \\u2212M\\u22252\\nOB\\ns.t.\\nrank(X) \\u2264k.\\nSince the constraint over the rank of a matrix is non-convex, it is stan-\\ndard to consider a relaxation that replaces the rank constraint by the nuclear\\nnorm. It is known that the nuclear norm is a lower bound on the matrix\\n88\\nCHAPTER 9. THE CONDITIONAL GRADIENT METHOD\\nrank if the singular values are bounded by one (see exercises). Thus, we\\narrive at the following convex program for matrix completion:\\nmin\\nX\\u2208Rn\\u00d7m\\n1\\n2\\u2225X \\u2212M\\u22252\\nOB\\n(9.1)\\ns.t.\\n\\u2225X\\u2225\\u2217\\u2264k.\\nWe consider algorithms to solve this convex optimization problem next.\\n9.3\\nThe Frank-Wolfe method\\nIn this section we consider minimization of a convex function over a convex\\ndomain.\\nThe conditional gradient (CG) method, or Frank-Wolfe algorithm, is a\\nsimple algorithm for minimizing a smooth convex function f over a convex\\nset K \\u2286Rn. The appeal of the method is that it is a \\ufb01rst order interior point\\nmethod - the iterates always lie inside the convex set, and thus no projections\\nare needed, and the update step on each iteration simply requires minimizing\\na linear objective over the set. The basic method is given in Algorithm 13.\\nAlgorithm 13 Conditional gradient\\n1: Input: step sizes {\\u03b7t \\u2208(0, 1], t \\u2208[T]}, initial point x1 \\u2208K.\\n2: for t = 1 to T do\\n3:\\nvt \\u2190arg minx\\u2208K\\n\\b\\nx\\u22a4\\u2207f(xt)\\n\\t\\n.\\n4:\\nxt+1 \\u2190xt + \\u03b7t(vt \\u2212xt).\\n5: end for\\nNote that in the CG method, the update to the iterate xt may be not be\\nin the direction of the gradient, as vt is the result of a linear optimization\\nprocedure in the direction of the negative gradient.\\nThis is depicted in\\nFigure 9.1.\\nThe following theorem gives an essentially tight performance guarantee\\nof this algorithm over smooth functions. Recall our notation from Chapter\\n2: x\\u22c6denotes the global minimizer of f over K, D denotes the diameter of\\nthe set K, and ht = f(xt)\\u2212f(x\\u22c6) denotes the suboptimality of the objective\\nvalue in iteration t.\\nTheorem 9.1. The CG algorithm applied to \\u03b2-smooth functions with step\\nsizes \\u03b7t = min{2H\\nt , 1}, for H \\u2265max{1, h1}, attains the following conver-\\ngence guarantee:\\nht \\u22642\\u03b2HD2\\nt\\n9.3. THE FRANK-WOLFE METHOD\\n89\\nFigure 9.1: Direction of progression of the conditional gradient algorithm.\\nProof. As done before in this manuscript, we denote \\u2207t = \\u2207f(xt), and also\\ndenote H \\u2265max{h1, 1}, such that \\u03b7t = min{2H\\nt , 1}. For any set of step\\nsizes, we have\\nf(xt+1) \\u2212f(x\\u22c6) = f(xt + \\u03b7t(vt \\u2212xt)) \\u2212f(x\\u22c6)\\n\\u2264f(xt) \\u2212f(x\\u22c6) + \\u03b7t(vt \\u2212xt)\\u22a4\\u2207t + \\u03b72\\nt\\n\\u03b2\\n2 \\u2225vt \\u2212xt\\u22252\\n\\u03b2-smoothness\\n\\u2264f(xt) \\u2212f(x\\u22c6) + \\u03b7t(x\\u22c6\\u2212xt)\\u22a4\\u2207t + \\u03b72\\nt\\n\\u03b2\\n2 \\u2225vt \\u2212xt\\u22252\\nvt optimality\\n\\u2264f(xt) \\u2212f(x\\u22c6) + \\u03b7t(f(x\\u22c6) \\u2212f(xt)) + \\u03b72\\nt\\n\\u03b2\\n2 \\u2225vt \\u2212xt\\u22252\\nconvexity of f\\n\\u2264(1 \\u2212\\u03b7t)(f(xt) \\u2212f(x\\u22c6)) + \\u03b72\\nt \\u03b2\\n2 D2.\\n(9.2)\\n90\\nCHAPTER 9. THE CONDITIONAL GRADIENT METHOD\\nWe reached the recursion ht+1 \\u2264(1 \\u2212\\u03b7t)ht + \\u03b72\\nt\\n\\u03b2D2\\n2 , and by induction,\\nht+1 \\u2264(1 \\u2212\\u03b7t)ht + \\u03b72\\nt\\n\\u03b2D2\\n2\\n\\u2264(1 \\u2212\\u03b7t)2\\u03b2HD2\\nt\\n+ \\u03b72\\nt\\n\\u03b2D2\\n2\\ninduction hypothesis\\n\\u2264(1 \\u22122H\\nt )2\\u03b2HD2\\nt\\n+ 4H2\\nt2\\n\\u03b2D2\\n2\\nvalue of \\u03b7t\\n= 2\\u03b2HD2\\nt\\n\\u22122H2\\u03b2D2\\nt2\\n\\u22642\\u03b2HD2\\nt\\n(1 \\u22121\\nt )\\nsince H \\u22651\\n\\u22642\\u03b2HD2\\nt + 1 .\\nt\\u22121\\nt\\n\\u2264\\nt\\nt+1\\n9.4\\nProjections vs. linear optimization\\nThe conditional gradient (Frank-Wolfe) algorithm described before does not\\nresort to projections, but rather computes a linear optimization problem of\\nthe form\\narg min\\nx\\u2208K\\nn\\nx\\u22a4u\\no\\n.\\n(9.3)\\nWhen is the CG method computationally preferable? The overall compu-\\ntational complexity of an iterative optimization algorithm is the product\\nof the number of iterations and the computational cost per iteration. The\\nCG method does not converge as well as the most e\\ufb03cient gradient descent\\nalgorithms, meaning it requires more iterations to produce a solution of a\\ncomparable level of accuracy. However, for many interesting scenarios the\\ncomputational cost of a linear optimization step (9.3) is signi\\ufb01cantly lower\\nthan that of a projection step.\\nLet us point out several examples of problems for which we have very e\\ufb03-\\ncient linear optimization algorithms, whereas our state-of-the-art algorithms\\nfor computing projections are signi\\ufb01cantly slower.\\nRecommendation systems and matrix prediction.\\nIn the example\\npointed out in the preceding section of matrix completion, known methods\\n9.4. PROJECTIONS VS. LINEAR OPTIMIZATION\\n91\\nfor projection onto the spectahedron, or more generally the bounded nuclear-\\nnorm ball, require singular value decompositions, which take superlinear\\ntime via our best known methods. In contrast, the CG method requires\\nmaximal eigenvector computations which can be carried out in linear time\\nvia the power method (or the more sophisticated Lanczos algorithm).\\nNetwork routing and convex graph problems.\\nVarious routing and\\ngraph problems can be modeled as convex optimization problems over a\\nconvex set called the \\ufb02ow polytope.\\nConsider a directed acyclic graph with m edges, a source node marked\\ns and a target node marked t. Every path from s to t in the graph can be\\nrepresented by its identifying vector, that is a vector in {0, 1}m in which the\\nentries that are set to 1 correspond to edges of the path. The \\ufb02ow polytope\\nof the graph is the convex hull of all such identifying vectors of the simple\\npaths from s to t. This polytope is also exactly the set of all unit s\\u2013t \\ufb02ows\\nin the graph if we assume that each edge has a unit \\ufb02ow capacity (a \\ufb02ow\\nis represented here as a vector in Rm in which each entry is the amount of\\n\\ufb02ow through the corresponding edge).\\nSince the \\ufb02ow polytope is just the convex hull of s\\u2013t paths in the graph,\\nminimizing a linear objective over it amounts to \\ufb01nding a minimum weight\\npath given weights for the edges. For the shortest path problem we have\\nvery e\\ufb03cient combinatorial optimization algorithms, namely Dijkstra\\u2019s al-\\ngorithm.\\nThus, applying the CG algorithm to solve any convex optimization prob-\\nlem over the \\ufb02ow polytope will only require iterative shortest path compu-\\ntations.\\nRanking and permutations.\\nA common way to represent a permutation\\nor ordering is by a permutation matrix.\\nSuch are square matrices over\\n{0, 1}n\\u00d7n that contain exactly one 1 entry in each row and column.\\nDoubly-stochastic matrices are square, real-valued matrices with non-\\nnegative entries, in which the sum of entries of each row and each column\\namounts to 1.\\nThe polytope that de\\ufb01nes all doubly-stochastic matrices\\nis called the Birkho\\ufb00-von Neumann polytope. The Birkho\\ufb00-von Neumann\\ntheorem states that this polytope is the convex hull of exactly all n \\u00d7 n\\npermutation matrices.\\nSince a permutation matrix corresponds to a perfect matching in a fully\\nconnected bipartite graph, linear minimization over this polytope corre-\\nsponds to \\ufb01nding a minimum weight perfect matching in a bipartite graph.\\n92\\nCHAPTER 9. THE CONDITIONAL GRADIENT METHOD\\nConsider a convex optimization problem over the Birkho\\ufb00-von Neumann\\npolytope.\\nThe CG algorithm will iteratively solve a linear optimization\\nproblem over the BVN polytope, thus iteratively solving a minimum weight\\nperfect matching in a bipartite graph problem, which is a well-studied com-\\nbinatorial optimization problem for which we know of e\\ufb03cient algorithms.\\nIn contrast, other gradient based methods will require projections, which\\nare quadratic optimization problems over the BVN polytope.\\nMatroid polytopes.\\nA matroid is pair (E, I) where E is a set of elements\\nand I is a set of subsets of E called the independent sets which satisfy vari-\\nous interesting proprieties that resemble the concept of linear independence\\nin vector spaces. Matroids have been studied extensively in combinatorial\\noptimization and a key example of a matroid is the graphical matroid in\\nwhich the set E is the set of edges of a given graph and the set I is the set of\\nall subsets of E which are cycle-free. In this case, I contains all the spanning\\ntrees of the graph. A subset S \\u2208I could be represented by its identifying\\nvector which lies in {0, 1}|E| which also gives rise to the matroid polytope\\nwhich is just the convex hull of all identifying vectors of sets in I. It can\\nbe shown that some matroid polytopes are de\\ufb01ned by exponentially many\\nlinear inequalities (exponential in |E|), which makes optimization over them\\ndi\\ufb03cult.\\nOn the other hand, linear optimization over matroid polytopes is easy\\nusing a simple greedy procedure which runs in nearly linear time. Thus, the\\nCG method serves as an e\\ufb03cient algorithm to solve any convex optimization\\nproblem over matroids iteratively using only a simple greedy procedure.\\n9.5. EXERCISES\\n93\\n9.5\\nExercises\\n1. Prove that if the singular values are smaller than or equal to one, then\\nthe nuclear norm is a lower bound on the rank, i.e., show\\nrank(X) \\u2265\\u2225X\\u2225\\u2217.\\n2. Prove that the trace is related to the nuclear norm via\\n\\u2225X\\u2225\\u2217= Tr(\\n\\u221a\\nXX\\u22a4) = Tr(\\n\\u221a\\nX\\u22a4X).\\n3. Show that maximizing a linear function over the spectahedron is equiv-\\nalent to a maximal eigenvector computation. That is, show that the\\nfollowing mathematical program:\\nmin X \\u2022 C\\nX \\u2208Sd = {X \\u2208Rd\\u00d7d , X \\u227d0 , Tr(X) \\u22641},\\nis equivalent to the following:\\nmin\\nx\\u2208Rd x\\u22a4Cx\\ns.t. \\u2225x\\u22252 \\u22641.\\n4. Download the MovieLens dataset from the web. Implement an online\\nrecommendation system based on the matrix completion model: im-\\nplement the OCG and OGD algorithms for matrix completion. Bench-\\nmark your results.\\n94\\nCHAPTER 9. THE CONDITIONAL GRADIENT METHOD\\n9.6\\nBibliographic Remarks\\nThe matrix completion model has been extremely popular since its inception\\nin the context of recommendation systems [80, 66, 69, 50, 14, 75].\\nThe conditional gradient algorithm was devised in the seminal paper\\nby Frank and Wolfe [21]. Due to the applicability of the FW algorithm to\\nlarge-scale constrained problems, it has been a method of choice in recent\\nmachine learning applications, to name a few: [42, 49, 41, 20, 30, 36, 72, 7,\\n82, 22, 23, 8].\\nThe online conditional gradient algorithm is due to [36]. An optimal\\nregret algorithm, attaining the O(\\n\\u221a\\nT) bound, for the special case of poly-\\nhedral sets was devised in [23].\\nChapter 10\\nSecond order methods for\\nmachine learning\\nAt this point in our course, we have exhausted the main techniques in\\n\\ufb01rst-order (or gradient-based) optimization.\\nWe have studied the main\\nworkhorse - stochastic gradient descent, the three acceleration techniques,\\nand projection-free gradient methods. Have we exhausted optimization for\\nML?\\nIn this section we discuss using higher derivatives of the objective func-\\ntion to accelerate optimization. The canonical method is Newton\\u2019s method,\\nwhich involves the second derivative or Hessian in high dimensions. The\\nvanilla approach is computationally expensive since it involves matrix inver-\\nsion in high dimensions that machine learning problems usually require.\\nHowever, recent progress in random estimators gives rise to linear-time\\nsecond order methods, for which each iteration is as computationally cheap\\nas gradient descent.\\n10.1\\nMotivating example: linear regression\\nIn the problem of linear regression we are given a set of measurements {ai \\u2208\\nRd, bi \\u2208R}, and the goal is to \\ufb01nd a set of weights that explains them best\\nin the mean squared error sense. As a mathematical program, the goal is to\\noptimize:\\nmin\\nx\\u2208Rd\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1\\n2\\nX\\ni\\u2208[m]\\n\\u0010\\na\\u22a4\\ni x \\u2212bi\\n\\u00112\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe,\\n95\\n96\\nCHAPTER 10. SECOND ORDER METHODS\\nor in matrix form,\\nmin\\nx f(x) =\\n\\u001a1\\n2\\u2225Ax \\u2212b\\u22252\\n\\u001b\\n.\\nHere A \\u2208Rm\\u00d7d, b \\u2208Rm. Notice that the objective function f is smooth,\\nbut not necessarily strongly convex. Therefore, all algorithms that we have\\nstudied so far without exception, which are all \\ufb01rst order methods, attain\\nrates which are poly( 1\\n\\u03b5).\\nHowever, the linear regression problem has a closed form solution that\\ncan be computed by taking the gradient to be zero, i.e. (Ax \\u2212b)\\u22a4A = 0,\\nwhich gives\\nx = (A\\u22a4A)\\u22121A\\u22a4b.\\nThe Newton direction is given by the inverse Hessian multiplied by the\\ngradient, \\u2207\\u22122f(x)\\u2207f(x). Observe that a single Newton step, i.e. moving in\\nthe Newton direction with step size one, from any direction gets us directly\\nto the optimal solution in one iteration! (see exercises)\\nMore generally, Newton\\u2019s method yields O(log 1\\n\\u03b5) convergence rates for\\na large class of functions without dependence on the condition number of\\nthe function! We study this property next.\\n10.2\\nSelf-Concordant Functions\\nIn this section we de\\ufb01ne and collect some of the properties of a special class\\nof functions, called self-concordant functions. These functions allow New-\\nton\\u2019s method to run in time which is independent of the condition number.\\nThe class of self-concordant functions is expressive and includes quadratic\\nfunctions, logarithms of inner products, a variety of barriers such as the log\\ndeterminant, and many more.\\nAn excellent reference for this material is the lecture notes on this subject\\nby Nemirovski [55]. We begin by de\\ufb01ning self-concordant functions.\\nDe\\ufb01nition 10.1 (Self-Concordant Functions). Let K \\u2286Rn be a non-empty\\nopen convex set, and and let f : K 7\\u2192R be a C3 convex function. Then, f\\nis said to be self-concordant if\\n|\\u22073f(x)[h, h, h]| \\u22642(h\\u22a4\\u22072f(x)h)3/2,\\nwhere we have\\n\\u2207kf(x)[h1, . . . , hk] \\u225c\\n\\u2202k\\n\\u2202t1 . . . \\u2202tk\\n|t1=\\u00b7\\u00b7\\u00b7=tkf(x + t1h1 + \\u00b7 \\u00b7 \\u00b7 + tkhk).\\n10.2. NEWTON\\u2019S METHOD\\n97\\nAnother key object in the analysis of self concordant functions is the\\nnotion of a Dikin Ellipsoid, which is the unit ball around a point in the\\nnorm given by the Hessian \\u2225\\u00b7 \\u2225\\u22072f at the point. We will refer to this norm\\nas the local norm around a point and denote it as \\u2225\\u00b7 \\u2225x. Formally,\\nDe\\ufb01nition 10.2 (Dikin ellipsoid). The Dikin ellipsoid of radius r centered\\nat a point x is de\\ufb01ned as\\nEr(x) \\u225c{y | \\u2225y \\u2212x\\u2225\\u22072f(x) \\u2264r}\\nOne of the key properties of self-concordant functions that we use is that\\ninside the Dikin ellipsoid, the function is well conditioned with respect to\\nthe local norm at the center. The next lemma makes this formal. The proof\\nof this lemma can be found in [55].\\nLemma 10.3 (See [55]). For all h such that \\u2225h\\u2225x < 1 we have that\\n(1 \\u2212\\u2225h\\u2225x)2\\u22072f(x) \\u2aaf\\u22072f(x + h) \\u2aaf\\n1\\n(1 \\u2212\\u2225h\\u2225x)2 \\u22072f(x)\\nAnother key quantity, which is used both as a potential function as well\\nas a dampening for the step size in the analysis of Newton\\u2019s method, is the\\nNewton Decrement:\\n\\u03bbx \\u225c\\u2225\\u2207f(x)\\u2225\\u2217\\nx =\\nq\\n\\u2207f(x)\\u22a4\\u2207\\u22122f(x)\\u2207f(x).\\nThe following lemma quanti\\ufb01es how \\u03bbx behaves as a potential by showing\\nthat once it drops below 1, it ensures that the minimum of the function lies\\nin the current Dikin ellipsoid. This is the property which we use crucially\\nin our analysis. The proof can be found in [55].\\nLemma 10.4 (See [55]). If \\u03bbx < 1 then\\n\\u2225x \\u2212x\\u2217\\u2225x \\u2264\\n\\u03bbx\\n1 \\u2212\\u03bbx\\n10.3\\nNewton\\u2019s method for self-concordant func-\\ntions\\nBefore introducing the linear time second order methods, we start by intro-\\nducing a robust Newton\\u2019s method and its properties. The pseudo-code is\\ngiven in Algorithm 14.\\n98\\nCHAPTER 10. SECOND ORDER METHODS\\nThe usual analysis of Newton\\u2019s method allows for quadratic convergence,\\ni.e. error \\u03b5 in O(log log 1\\n\\u03b5) iterations for convex objectives. However, we\\nprefer to present a version of Newton\\u2019s method which is robust to certain\\nrandom estimators of the Newton direction. This yields a slower rate of\\nO(log 1\\n\\u03b5).\\nThe faster running time per iteration, which does not require\\nmatrix manipulations, more than makes up for this.\\nAlgorithm 14 Robust Newton\\u2019s method\\nInput: T, x1\\nfor t = 1 to T do\\nSet c = 1\\n8, \\u03b7 = min{c,\\nc\\n8\\u03bbxt }. Let 1\\n2\\u2207\\u22122f(xt) \\u2aaf\\u02dc\\u2207\\u22122\\nt\\n\\u2aaf2\\u2207\\u22122f(xt).\\nxt+1 = xt \\u2212\\u03b7 \\u02dc\\u2207\\u22122\\nt \\u2207f(xt)\\nend for\\nreturn xT+1\\nIt is important to notice that every two consecutive points are within\\nthe same Dikin ellipsoid of radius 1\\n2. Denote \\u2207t = \\u2207xt, and similarly for\\nthe Hessian. Then we have:\\n\\u2225xt \\u2212xt+1\\u22252\\nxt = \\u03b72\\u2207\\u22a4\\nt \\u02dc\\u2207\\u22122\\nt \\u22072\\nt \\u02dc\\u2207\\u22122\\nt \\u2207t \\u22644\\u03b72\\u03bb2\\nt \\u22641\\n2.\\nThe advantage of Newton\\u2019s method as applied to self-concordant func-\\ntions is its linear convergence rate, as given in the following theorem.\\nTheorem 10.5. Let f be self-concordant, and f(x1) \\u2264M, then\\nht = f(xt) \\u2212f(x\\u2217) \\u2264O(M + log 1\\n\\u03b5)\\nThe proof of this theorem is composed of two steps, according to the\\nmagnitude of the Newton decrement.\\nPhase 1: damped Newton\\nLemma 10.6. As long as \\u03bbx \\u22651\\n8, we have that\\nht \\u2264\\u22121\\n4c\\n10.3. NEWTON\\u2019S METHOD FOR SELF-CONCORDANT FUNCTIONS99\\nProof. Using similar analysis to the descent lemma we have that\\nf(xt+1) \\u2212f(xt)\\n\\u2264\\u2207\\u22a4\\nt (xt+1 \\u2212xt) + 1\\n2(xt \\u2212xt+1)\\u22a4\\u22072(\\u03b6)(xt \\u2212xt+1)\\nTaylor\\n\\u2264\\u2207\\u22a4\\nt (xt+1 \\u2212xt) + 1\\n4(xt \\u2212xt+1)\\u22a4\\u22072(xt)(xt \\u2212xt+1)\\nxt+1 \\u2208E1/2(xt)\\n= \\u2212\\u03b7\\u2207\\u22a4\\nt \\u02dc\\u2207\\u22122\\nt \\u2207t + 1\\n4\\u03b72\\u2207\\u22a4\\nt \\u02dc\\u2207\\u22122\\nt \\u22072\\nt \\u02dc\\u2207\\u22122\\nt \\u2207t\\n= \\u2212\\u03b7\\u03bb2\\nt + 1\\n4\\u03b72\\u03bb2\\nt \\u2264\\u22121\\n16c\\nThe conclusion from this step is that after O(M) steps, Algorithm 14\\nreaches a point for which \\u03bbx \\u22641\\n8. According to Lemma 10.4, we also have\\nthat \\u2225x \\u2212x\\u2217\\u2225x \\u22641\\n4, that is, the optimum is in the same Dikin ellipsoid as\\nthe current point.\\nPhase 2: pure Newton\\nIn the second phase our step size is changed to\\nbe larger. In this case, we are guaranteed that the Newton decrement is less\\nthan one, and thus we know that the global optimum is in the same Dikin\\nellipsoid as the current point. In this ellipsoid, all Hessians are equivalent\\nup to a factor of two, and thus Mirrored-Descent with the inverse Hessian\\nas preconditioner becomes gradient descent. We make this formal below.\\nAlgorithm 15 Preconditioned Gradient Descent\\nInput: P, T\\nfor t = 1 to T do\\nxt+1 = xt \\u2212\\u03b7P \\u22121\\u2207f(xt)\\nend for\\nreturn xT+1\\nLemma 10.7. Suppose that 1\\n2P \\u2aaf\\u22072f(x) \\u2aaf2P, and \\u2225x1 \\u2212x\\u2217\\u2225P \\u22641\\n2, then\\nAlgorithm 15 converges as\\nht+1 \\u2264h1e\\u22121\\n8 t.\\nThis theorem follows from noticing that the function g(z) = f(P \\u22121/2x)\\nis 1\\n2-strongly convex and 2-smooth, and using Theorem 3.2. It can be shown\\nthat gradient descent on g is equivalent to Newton\\u2019s method in f. Details\\nare left as an exercise.\\nAn immediate corollary is that Newton\\u2019s method converges at a rate of\\nO(log 1\\n\\u03b5) in this phase.\\n100\\nCHAPTER 10. SECOND ORDER METHODS\\n10.4\\nLinear-time second-order methods\\nNewton\\u2019s algorithm is of foundational importance in the study of mathemat-\\nical programming in general. A major application are interior point methods\\nfor convex optimization, which are the most important polynomial-time al-\\ngorithms for general constrained convex optimization.\\nHowever, the main downside of this method is the need to maintain and\\nmanipulate matrices - namely the Hessians. This is completely impractical\\nfor machine learning applications in which the dimension is huge.\\nAnother signi\\ufb01cant downside is the non-robust nature of the algorithm,\\nwhich makes applying it in stochastic environments challenging.\\nIn this section we show how to apply Newton\\u2019s method to machine\\nlearning problems.\\nThis involves relatively new developments that allow\\nfor linear-time per-iteration complexity, similar to SGD, and theoretically\\nsuperior running times.\\nAt the time of writing, however, these methods\\nare practical only for convex optimization, and have not shown superior\\nperformance on optimization tasks involving deep neural networks.\\nThe \\ufb01rst step to developing a linear time Newton\\u2019s method is an e\\ufb03cient\\nstochastic estimator for the Newton direction, and the Hessian inverse.\\n10.4.1\\nEstimators for the Hessian Inverse\\nThe key idea underlying the construction is the following well known fact\\nabout the Taylor series expansion of the matrix inverse.\\nLemma 10.8. For a matrix A \\u2208Rd\\u00d7d such that A \\u2ab00 and \\u2225A\\u2225\\u22641, we\\nhave that\\nA\\u22121 =\\n\\u221e\\nX\\ni=0\\n(I \\u2212A)i\\nWe propose two unbiased estimators based on the above series. To de\\ufb01ne\\nthe \\ufb01rst estimator pick a probability distribution over non-negative integers\\n{pi} and sample \\u02c6i from the above distribution. Let X1, . . . X\\u02c6i be independent\\nsamples of the Hessian \\u22072f and de\\ufb01ne the estimator as\\nDe\\ufb01nition 10.9 (Estimator 1).\\n\\u02dc\\u2207\\u22122f = 1\\np\\u02c6i\\n\\u02c6iY\\nj=1\\n(I \\u2212Xj)\\n10.4. LINEAR-TIME SECOND-ORDER METHODS\\n101\\nObserve that our estimator of the Hessian inverse is unbiased, i.e. E[ \\u02c6X] =\\n\\u2207\\u22122f at any point. Estimator 1 has the disadvantage that in a single sample\\nit incorporates only one term of the Taylor series.\\nThe second estimator below is based on the observation that the above\\nseries has the following succinct recursive de\\ufb01nition, and is more e\\ufb03cient.\\nFor a matrix A de\\ufb01ne\\nA\\u22121\\nj\\n=\\nj\\nX\\ni=0\\n(I \\u2212A)i\\ni.e. the \\ufb01rst j terms of the above Taylor expansion. It is easy to see that\\nthe following recursion holds for A\\u22121\\nj\\nA\\u22121\\nj\\n= I + (I \\u2212A)A\\u22121\\nj\\u22121\\nUsing the above recursive formulation, we now describe an unbiased\\nestimator of \\u2207\\u22122f by deriving an unbiased estimator \\u02dc\\u2207\\u22122fj for \\u2207\\u22122fj.\\nDe\\ufb01nition 10.10 (Estimator 2). Given j independent and unbiased samples\\n{X1 . . . Xj} of the hessian \\u22072f. De\\ufb01ne { \\u02dc\\u2207\\u22122f0 . . . \\u02dc\\u2207\\u22122fj} recursively as\\nfollows\\n\\u02dc\\u2207\\u22122f0 = I\\n\\u02dc\\u2207\\u22122ft = I + (I \\u2212Xj) \\u02dc\\u2207\\u22122ft\\u22121\\nIt can be readily seen that E[ \\u02dc\\u2207\\u22122fj] = \\u2207\\u22122fj and therefore E[ \\u02dc\\u2207\\u22122fj] \\u2192\\n\\u2207\\u22122f as j \\u2192\\u221egiving us an unbiased estimator in the limit.\\n10.4.2\\nIncorporating the estimator\\nBoth of the above estimators can be computed using only Hessian-vector\\nproducts, rather than matrix manipulations. For many machine learning\\nproblems, Hessian-vector products can be computed in linear time. Exam-\\nples include:\\n1. Convex regression and SVM objectives over training data have the\\nform\\nmin\\nw f(w) = E\\ni [\\u2113(w\\u22a4xi)],\\nwhere \\u2113is a convex function. The Hessian can thus be written as\\n\\u22072f(w) = E\\ni [\\u2113\\u2032\\u2032(w\\u22a4xi)xix\\u22a4\\ni ]\\n102\\nCHAPTER 10. SECOND ORDER METHODS\\nThus, the \\ufb01rst Newton direction estimator can now be written as\\n\\u02dc\\u22072f(w)\\u2207w = E\\nj\\u223cD[\\njY\\ni=1\\n(I \\u2212\\u2113\\u2032\\u2032(w\\u22a4xi)xix\\u22a4\\ni )]\\u2207w.\\nNotice that this estimator can be computed using j vector-vector prod-\\nucts if the ordinal j was randomly chosen.\\n2. Non-convex optimization over neural networks: a similar derivation as\\nabove shows that the estimator can be computed only using Hessian-\\nvector products. The special structure of neural networks allow this\\ncomputation in a constant number of backpropagation steps, i.e. linear\\ntime in the network size, this is called the \\u201cPearlmutter trick\\u201d, see [61].\\nWe note that non-convex optimization presents special challenges for\\nsecond order methods, since the Hessian need not be positive semi-\\nde\\ufb01nite.\\nNevertheless, the techniques presented hereby can still be\\nused to provide theoretical speedups for second order methods over\\n\\ufb01rst order methods in terms of convergence to local minima.\\nThe\\ndetails are beyond our scope, and can be found in [2].\\nPutting everything together.\\nThese estimators we have studied can\\nbe used to create unbiased estimators to the Newton direction of the form\\n\\u02dc\\u2207\\u22122\\nx \\u2207x for \\u02dc\\u2207\\u22122\\nx\\nwhich satis\\ufb01es\\n1\\n2\\u2207\\u22122f(xt) \\u2aaf\\u02dc\\u2207\\u22122\\nt\\n\\u2aaf2\\u2207\\u22122f(xt).\\nThese can be incorporated into Algorithm 14, which we proved is capable\\nof obtaining fast convergence with approximate Newton directions of this\\nform.\\n10.5. EXERCISES\\n103\\n10.5\\nExercises\\n1. Prove that a single Newton step for linear regression yields the optimal\\nsolution.\\n2. Let f : Rd 7\\u2192R, and consider the a\\ufb03ne transformation y = Ax, for\\nA \\u2208Rd\\u00d7d being a symmetric matrix. Prove that\\nyt+1 \\u2190yt \\u2212\\u03b7\\u2207f(yt)\\nis equivalent to\\nxt+1 \\u2190xt \\u2212\\u03b7A\\u22122\\u2207f(xt).\\n3. Prove that the function g(z) de\\ufb01ned in phase 2 of the robust Newton\\nalgorithm is 1\\n2-strongly convex and 2-smooth. Conclude with a proof\\nof Theorem 10.7.\\n104\\nCHAPTER 10. SECOND ORDER METHODS\\n10.6\\nBibliographic Remarks\\nThe modern application of Newton\\u2019s method to convex optimization was\\nput forth in the seminal work of Nesterov and Nemirovski [58] on interior\\npoint methods. A wonderful exposition is Nemirovski\\u2019s lecture notes [55].\\nThe fact that Hessian-vector products can be computed in linear time\\nfor feed forward neural networks was described in [61]. Linear time second\\norder methods for machine learning and the Hessian-vector product model\\nin machine learning was introduced in [4]. This was extended to non-convex\\noptimization for deep learning in [2].\\nChapter 11\\nHyperparameter\\nOptimization\\nThus far in this class, we have been talking about continuous mathematical\\noptimization, where the search space of our optimization problem is continu-\\nous and mostly convex. For example, we have learned about how to optimize\\nthe weights of a deep neural network, which take continuous real values, via\\nvarious optimization algorithms (SGD, AdaGrad, Newton\\u2019s method, etc.).\\nHowever, in the process of training a neural network, there are some meta\\nparameters, which we call hyperparameters, that have a profound e\\ufb00ect on\\nthe \\ufb01nal outcome. These are global, mostly discrete, parameters that are\\ntreated di\\ufb00erently by algorithm designers as well as by engineers. Examples\\ninclude the architecture of the neural network (number of layers, width of\\neach layer, type of activation function, ...), the optimization scheme for up-\\ndating weights (SGD/AdaGrad, initial learning rate, decay rate of learning\\nrate, momentum parameter, ...), and many more. Roughly speaking, these\\nhyperparameters are chosen before the training starts.\\nThe purpose of this chapter is to formalize this problem as an optimiza-\\ntion problem in machine learning, which requires a di\\ufb00erent methodology\\nthan we have treated in the rest of this course. We remark that hyperpa-\\nrameter optimization is still an active area of research and its theoretical\\nproperties are not well understood as of this time.\\n11.1\\nFormalizing the problem\\nWhat makes hyperparameters di\\ufb00erent from \\u201cregular\\u201d parameters?\\n105\\n106\\nCHAPTER 11. HYPERPARAMETER OPTIMIZATION\\n1. The search space is often discrete (for example, number of layers). As\\nsuch, there is no natural notion of gradient or di\\ufb00erentials and it is\\nnot clear how to apply the iterative methods we have studied thus far.\\n2. Even evaluating the objective function is extremely expensive (think\\nof evaluating the test error of the trained neural network). Thus it is\\ncrucial to minimize the number of function evaluations, whereas other\\ncomputations are signi\\ufb01cantly less expensive.\\n3. Evaluating the function can be done in parallel. As an example, train-\\ning feedforward deep neural networks over di\\ufb00erent architectures can\\nbe done in parallel.\\nMore formally, we consider the following optimization problem\\nmin\\nxi\\u2208GF(qi)\\nf(x),\\nwhere x is the representation of discrete hyperparameters, each taking value\\nfrom qi \\u22652 possible discrete values and thus in GF(q), the Galois \\ufb01eld of\\norder q. The example to keep in mind is that the objective f(x) is the test\\nerror of the neural network trained with hyperparameters x. Note that x\\nhas a search space of size Q\\ni qi \\u22652n, exponentially large in the number of\\ndi\\ufb00erent hyperparameters.\\n11.2\\nHyperparameter optimization algorithms\\nThe properties of the problem mentioned before prohibits the use of the\\nalgorithms we have studied thus far, which are all suitable for continuous\\noptimization. A naive method is to perform a grid search over all hyperpa-\\nrameters, but this quickly becomes infeasible. An emerging \\ufb01eld of research\\nin recent years, called AutoML, aims to choose hyperparameters automati-\\ncally. The following techniques are in common use:\\n\\u2022 Grid search, try all possible assignments of hyperparameters and\\nreturn the best. This becomes infeasible very quickly with n - the\\nnumber of hyperparameters.\\n\\u2022 Random search, where one randomly picks some choices of hyper-\\nparameters, evaluates their function objective, and chooses the one\\nchoice of hyperparameters giving best performance. An advantage of\\nthis method is that it is easy to implement in parallel.\\n11.3. A SPECTRAL METHOD\\n107\\n\\u2022 Successive Halving and Hyperband, random search combined\\nwith early stopping using multi-armed bandit techniques. These gain\\na small constant factor improvement over random search.\\n\\u2022 Bayesian optimization, a statistical approach which has a prior over\\nthe objective and tries to iteratively pick an evaluation point which\\nreduces the variance in objective value.\\nFinally it picks the point\\nthat attains the lowest objective objective with highest con\\ufb01dence.\\nThis approach is sequential in nature and thus di\\ufb03cult to parallelize.\\nAnother important question is how to choose a good prior.\\nThe hyperparameter optimization problem is essentially a combinato-\\nrial optimization problem with exponentially large search space. Without\\nfurther assumptions, this optimization problem is information-theoretically\\nhard. Such assumptions are explored in the next section with an accompa-\\nnying algorithm.\\nFinally, we note that a simple but hard-to-beat benchmark is random\\nsearch with double budget. That is, compare the performance of a method\\nto that of random search, but allow random search double the query budget\\nof your own method.\\n11.3\\nA Spectral Method\\nFor simplicity, in this section we consider the case in which hyperparam-\\neters are binary. This retains the di\\ufb03culty of the setting, but makes the\\nmathematical derivation simpler. The optimization problem now becomes\\nmin\\nx\\u2208{\\u22121,1}n\\nf(x).\\n(11.1)\\nThe method we describe in this section is inspired by the following key\\nobservation: although the whole search space of hyperparameters is exponen-\\ntially large, it is often the case in practice that only a few hyperparameters\\ntogether play a signi\\ufb01cant role in the performance of a deep neural network.\\nTo make this intuition more precise, we need some de\\ufb01nitions and facts\\nfrom Fourier analysis of Boolean functions.\\nFact 11.1. Any function f : {\\u22121, 1}n \\u2192[\\u22121, 1] can be uniquely represented\\nin the Fourier basis\\nf(x) =\\nX\\nS\\u2286[n]\\n\\u03b1s \\u02c6\\u03c7S(x),\\n108\\nCHAPTER 11. HYPERPARAMETER OPTIMIZATION\\nwhere each Fourier basis function\\n\\u02c6\\u03c7S(x) =\\nY\\ni\\u2208S\\nxi.\\nis a monomial, and thus f(x) has a polynomial representation.\\nNow we are ready to formalize our key observation in the following as-\\nsumption:\\nAssumption 11.2. The objective function f in the hyperparameter opti-\\nmization problem (11.1) is low degree and sparse in the Fourier basis, i.e.\\nf(x) \\u2248\\nX\\n|S|\\u2264d\\n\\u03b1S \\u02c6\\u03c7S(x),\\n\\u2225\\u03b1\\u03b1\\u03b1\\u22251 \\u2264k,\\n(11.2)\\nwhere d is the upper bound of polynomial degree, and k is the sparsity of\\nFourier coe\\ufb03cient \\u03b1\\u03b1\\u03b1 (indexed by S) in \\u21131 sense (which is a convex relaxation\\nof \\u2225\\u03b1\\u03b1\\u03b1\\u22250, the true sparsity).\\nRemark 11.3. Clearly this assumption does not always hold. For example,\\nmany deep reinforcement learning algorithms nowadays rely heavily on the\\nchoice of the random seed, which can also be seen as a hyperparameter. If\\nx \\u2208{\\u22121, 1}32 is the bit representation of a int32 random seed, then there is\\nno reason to assume that a few of these bits should play a more signi\\ufb01cant\\nrole than the others.\\nUnder this assumption, all we need to do now is to \\ufb01nd out the few im-\\nportant sets of variables S\\u2019s, as well as their coe\\ufb03cients \\u03b1S\\u2019s, in the approx-\\nimation (11.2). Fortunately, there is already a whole area of research, called\\ncompressed sensing, that aims to recover a high-dimensional but sparse vec-\\ntor, using only a few linear measurements. Next, we will brie\\ufb02y introduce\\nthe problem of compressed sensing, and one useful result from the litera-\\nture. After that, we will introduce the Harmonica algorithm, which applies\\ncompressed sensing techniques to solve the hyperparameter optimization\\nproblem (11.1).\\n11.3.1\\nBackground: Compressed Sensing\\nThe problem of compressed sensing is as follows. Suppose there is a hidden\\nsignal x \\u2208Rn that we cannot observe. In order to recover x, we design a\\nmeasurement matrix A \\u2208Rm\\u00d7n, and obtain noisy linear measurements y =\\nAx + \\u03b7\\u03b7\\u03b7 \\u2208Rm, where \\u03b7\\u03b7\\u03b7 is some random noise. The di\\ufb03culty arises when we\\n11.3. A SPECTRAL METHOD\\n109\\nhave a limited budget for measurements, i.e. m \\u226an. Note that even without\\nnoise, recovering x is non-trivial since y = Ax is an underdetermined linear\\nsystem, therefore if there is one solution x that solves this linear system,\\nthere will be in\\ufb01nitely many solutions. The key to this problem is to assume\\nthat x is k-sparse, that is, \\u2225x\\u22250 \\u2264k. This assumption has been justi\\ufb01ed\\nin various real-world applications; for example, natural images tend to be\\nsparse in the Fourier/wavelet domain, a property which forms the bases of\\nmany image compression algorithms.\\nUnder the assumption of sparsity, the natural way to recover x is to\\nsolve a least squares problem, subject to some sparsity constraint \\u2225x\\u22250 \\u2264k.\\nHowever, \\u21130 norm is di\\ufb03cult to handle, and it is often replaced by \\u21131 norm,\\nits convex relaxation. One useful result from the literature of compressed\\nsensing is the following.\\nProposition 11.4 (Informal statement of Theorem 4.4 in [63]). Assume\\nthe ground-truth signal x \\u2208Rn is k-sparse.\\nThen, with high probability,\\nusing a randomly designed A \\u2208Rm\\u00d7n that is \\u201cnear-orthogonal\\u201d (random\\nGaussian matrix, subsampled Fourier basis, etc.), with m = O(k log(n)/\\u03b5)\\nand \\u2225\\u03b7\\u03b7\\u03b7\\u22252 = O(\\u221am), x can be recovered by a convex program\\nmin\\nz\\u2208Rn \\u2225y \\u2212Az\\u22252\\n2\\ns.t.\\n\\u2225z\\u22251 \\u2264k,\\n(11.3)\\nwith accuracy \\u2225x \\u2212z\\u22252 \\u2264\\u03b5.\\nThis result is remarkable; in particular, it says that the number of mea-\\nsurements needed to recover a sparse signal is independent of the dimension\\nn (up to a logarithm term), but only depends on the sparsity k and the\\ndesired accuracy \\u03b5. 1\\nRemark 11.5. The convex program (11.3) is equivalent to the following\\nLASSO problem\\nmin\\nz\\u2208Rn \\u2225y \\u2212Az\\u22252\\n2 + \\u03bb\\u2225z\\u22251,\\nwith a proper choice of regularization parameter \\u03bb. The LASSO problem\\nis an unconstrained convex program, and has e\\ufb03cient solvers, as per the\\nalgorithms we have studied in this course.\\n1It also depends on the desired high-probability bound, which is omitted in this informal\\nstatement.\\n110\\nCHAPTER 11. HYPERPARAMETER OPTIMIZATION\\n11.3.2\\nThe Spectral Algorithm\\nThe main idea is that, under Assumption 11.2, we can view the problem\\nof hyperparameter optimization as recovering the sparse signal \\u03b1\\u03b1\\u03b1 from lin-\\near measurements. More speci\\ufb01cally, we need to query T random samples,\\nf(x1), . . . , f(xT ), and then solve the LASSO problem\\nmin\\n\\u03b1\\u03b1\\u03b1\\nT\\nX\\nt=1\\n(\\nX\\n|S|\\u2264d\\n\\u03b1S \\u02c6\\u03c7S(xt) \\u2212f(xt))2 + \\u03bb\\u2225\\u03b1\\u03b1\\u03b1\\u22251,\\n(11.4)\\nwhere the regularization term \\u03bb\\u2225\\u03b1\\u03b1\\u03b1\\u22251 controls the sparsity of \\u03b1\\u03b1\\u03b1. Also note\\nthat the constraint |S| \\u2264d not only implies that the solution is a low-degree\\npolynomial, but also helps to reduce the \\u201ce\\ufb00ective\\u201d dimension of \\u03b1\\u03b1\\u03b1 from 2n\\nto O(nd), which makes it feasible to solve this LASSO problem.\\nDenote by S1, . . . , Ss the indices of the s largest coe\\ufb03cients of the LASSO\\nsolution, and de\\ufb01ne\\ng(x) =\\nX\\ni\\u2208[s]\\n\\u03b1Si \\u02c6\\u03c7Si(x),\\nwhich involves only a few dimensions of x since the LASSO solution is sparse\\nand low-degree.\\nThe next step is to set the variables outside \\u222ai\\u2208[s]Si to\\narbitrary values, and compute a minimizer x\\u2217\\u2208arg min g(x).\\nIn other\\nwords, we have reduced the original problem of optimizing f(x) over n\\nvariables, to the problem of optimizing g(x) (an approximation of f(x))\\nover only a few variables (which is now feasible to solve). One remarkable\\nfeature of this algorithm is that the returned solution x\\u2217may not belong to\\nthe samples {x1, . . . , xT }, which is not the case for other existing methods\\n(such as random search).\\nUsing theoretical results from compressed sensing (e.g.\\nProposition\\n11.4), we can derive the following guarantee for the sparse recovery of \\u03b1\\u03b1\\u03b1\\nvia LASSO.\\nTheorem 11.6 (Informal statement of Lemma 7 in [38]). Assume f is k-\\nsparse in the Fourier expansion. Then, with T = O(k2 log(n)/\\u03b5) samples,\\nthe solution of the LASSO problem (11.4) achieves \\u03b5 accuracy.\\nFinally, the above derivation can be considered as only one stage in a\\nmulti-stage process, each iteratively setting the value of a few more variables\\nthat are the most signi\\ufb01cant.\\n11.4. BIBLIOGRAPHIC REMARKS\\n111\\n11.4\\nBibliographic Remarks\\nFor a nice exposition on hyperparameter optimization see [64, 65], in which\\nthe the benchmark of comparing to Random Search with double queries was\\nproposed.\\nPerhaps the simplest approach to HPO is random sampling of di\\ufb00erent\\nchoices of parameters and picking the best amongst the chosen evaluations\\n[9].\\nSuccessive Halving (SH) algorithm was introduced [43].\\nHyperband\\nfurther improves SH by automatically tuning the hyperparameters in SH\\n[51].\\nThe Bayesian optimization (BO) methodology is currently the most stud-\\nied in HPO. For recent studies and algorithms of this \\ufb02avor see [10, 78, 81,\\n79, 24, 84, 40].\\nThe spectral approach for hyperparameter optimization was introduced\\nin [38]. For an in-depth treatment of compressed sensing see the survey of\\n[63], and for Fourier analysis of Boolean functions see [59].\\n112\\nCHAPTER 11. HYPERPARAMETER OPTIMIZATION\\nBibliography\\n[1] Jacob Abernethy, Elad Hazan, and Alexander Rakhlin.\\nCompeting\\nin the dark: An e\\ufb03cient algorithm for bandit linear optimization. In\\nProceedings of the 21st Annual Conference on Learning Theory, pages\\n263\\u2013274, 2008.\\n[2] Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and\\nTengyu Ma. Finding approximate local minima faster than gradient\\ndescent. In Proceedings of the 49th Annual ACM SIGACT Symposium\\non Theory of Computing, pages 1195\\u20131199. ACM, 2017.\\n[3] Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh,\\nCyril Zhang, and Yi Zhang. The case for full-matrix adaptive regular-\\nization. arXiv preprint arXiv:1806.02958, 2018.\\n[4] Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochas-\\ntic optimization for machine learning in linear time. The Journal of\\nMachine Learning Research, 18(1):4148\\u20134187, 2017.\\n[5] Zeyuan Allen-Zhu and Lorenzo Orecchia.\\nLinear coupling:\\nAn ul-\\ntimate uni\\ufb01cation of gradient and mirror descent.\\narXiv preprint\\narXiv:1407.1537, 2014.\\n[6] Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory-\\ne\\ufb03cient adaptive optimization for large-scale learning. arXiv preprint\\narXiv:1901.11150, 2019.\\n[7] Francis Bach, Simon Lacoste-Julien, and Guillaume Obozinski.\\nOn\\nthe equivalence between herding and conditional gradient algorithms.\\nIn John Langford and Joelle Pineau, editors, Proceedings of the 29th\\nInternational Conference on Machine Learning (ICML-12), ICML \\u201912,\\npages 1359\\u20131366, New York, NY, USA, July 2012. Omnipress.\\n113\\n114\\nBIBLIOGRAPHY\\n[8] Aur\\u00b4elien Bellet, Yingyu Liang, Alireza Bagheri Garakani, Maria-\\nFlorina Balcan, and Fei Sha.\\nDistributed frank-wolfe algorithm: A\\nuni\\ufb01ed framework for communication-e\\ufb03cient sparse learning. CoRR,\\nabs/1404.2644, 2014.\\n[9] James Bergstra and Yoshua Bengio.\\nRandom search for hyper-\\nparameter optimization. J. Mach. Learn. Res., 13:281\\u2013305, February\\n2012.\\n[10] James S. Bergstra, R\\u00b4emi Bardenet, Yoshua Bengio, and Bal\\u00b4azs K\\u00b4egl.\\nAlgorithms for hyper-parameter optimization. In J. Shawe-Taylor, R. S.\\nZemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Ad-\\nvances in Neural Information Processing Systems 24, pages 2546\\u20132554.\\nCurran Associates, Inc., 2011.\\n[11] J.M. Borwein and A.S. Lewis. Convex Analysis and Nonlinear Opti-\\nmization: Theory and Examples. CMS Books in Mathematics. Springer,\\n2006.\\n[12] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge Uni-\\nversity Press, March 2004.\\n[13] S\\u00b4ebastien Bubeck. Convex optimization: Algorithms and complexity.\\nFoundations and Trends in Machine Learning, 8(3\\u20134):231\\u2013357, 2015.\\n[14] E. Candes and B. Recht. Exact matrix completion via convex optimiza-\\ntion. Foundations of Computational Mathematics, 9:717\\u2013772, 2009.\\n[15] Nicol`o Cesa-Bianchi and G\\u00b4abor Lugosi.\\nPrediction, Learning, and\\nGames. Cambridge University Press, 2006.\\n[16] Xinyi Chen, Naman Agarwal, Elad Hazan, Cyril Zhang, and Yi Zhang.\\nExtreme tensoring for low-memory preconditioning.\\narXiv preprint\\narXiv:1902.04620, 2019.\\n[17] Qi Deng, Yi Cheng, and Guanghui Lan. Optimal adaptive and accel-\\nerated stochastic gradient descent. arXiv preprint arXiv:1810.00553,\\n2018.\\n[18] John Duchi, Elad Hazan, and Yoram Singer.\\nAdaptive subgradient\\nmethods for online learning and stochastic optimization. The Journal\\nof Machine Learning Research, 12:2121\\u20132159, 2011.\\nBIBLIOGRAPHY\\n115\\n[19] John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient\\nmethods for online learning and stochastic optimization. In COLT 2010\\n- The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29,\\n2010, pages 257\\u2013269, 2010.\\n[20] Miroslav Dud\\u00b4\\u0131k, Za\\u00a8\\u0131d Harchaoui, and J\\u00b4er\\u02c6ome Malick. Lifted coordinate\\ndescent for learning with trace-norm regularization. Journal of Machine\\nLearning Research - Proceedings Track, 22:327\\u2013336, 2012.\\n[21] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval\\nResearch Logistics Quarterly, 3:149\\u2013154, 1956.\\n[22] Dan Garber and Elad Hazan. Approximating semide\\ufb01nite programs in\\nsublinear time. In NIPS, pages 1080\\u20131088, 2011.\\n[23] Dan Garber and Elad Hazan.\\nPlaying non-linear games with linear\\noracles. In FOCS, pages 420\\u2013428, 2013.\\n[24] Jacob R. Gardner, Matt J. Kusner, Zhixiang Eddie Xu, Kilian Q. Wein-\\nberger, and John P. Cunningham. Bayesian optimization with inequal-\\nity constraints.\\nIn Proceedings of the 31th International Conference\\non Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014,\\npages 937\\u2013945, 2014.\\n[25] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning.\\nMIT Press, 2016. http://www.deeplearningbook.org.\\n[26] A .J. Grove, N. Littlestone, and D. Schuurmans. General convergence\\nresults for linear discriminant updates. Machine Learning, 43(3):173\\u2013\\n210, 2001.\\n[27] Vineet Gupta, Tomer Koren, and Yoram Singer. A uni\\ufb01ed approach\\nto adaptive regularization in online and stochastic optimization. arXiv\\npreprint arXiv:1706.06569, 2017.\\n[28] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Precondi-\\ntioned stochastic tensor optimization. arXiv preprint arXiv:1802.09568,\\n2018.\\n[29] James Hannan. Approximation to bayes risk in repeated play. In M.\\nDresher, A. W. Tucker, and P. Wolfe, editors, Contributions to the\\nTheory of Games, volume 3, pages 97\\u2013139, 1957.\\n116\\nBIBLIOGRAPHY\\n[30] Za\\u00a8\\u0131d Harchaoui, Matthijs Douze, Mattis Paulin, Miroslav Dud\\u00b4\\u0131k, and\\nJ\\u00b4er\\u02c6ome Malick. Large-scale image classi\\ufb01cation with trace-norm regu-\\nlarization. In CVPR, pages 3386\\u20133393, 2012.\\n[31] Elad Hazan. Introduction to online convex optimization. Foundations\\nand Trends \\u02c6A R\\u20ddin Optimization, 2(3-4):157\\u2013325, 2016.\\n[32] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret al-\\ngorithms for online convex optimization. In Machine Learning, volume\\n69(2\\u20133), pages 169\\u2013192, 2007.\\n[33] Elad Hazan and Sham Kakade. Revisiting the polyak step size. arXiv\\npreprint arXiv:1905.00313, 2019.\\n[34] Elad Hazan and Satyen Kale. Extracting certainty from uncertainty:\\nRegret bounded by variation in costs. In The 21st Annual Conference\\non Learning Theory (COLT), pages 57\\u201368, 2008.\\n[35] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier:\\nan optimal algorithm for stochastic strongly-convex optimization. Jour-\\nnal of Machine Learning Research - Proceedings Track, pages 421\\u2013436,\\n2011.\\n[36] Elad Hazan and Satyen Kale. Projection-free online learning. In ICML,\\n2012.\\n[37] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier:\\noptimal algorithms for stochastic strongly-convex optimization.\\nThe\\nJournal of Machine Learning Research, 15(1):2489\\u20132512, 2014.\\n[38] Elad Hazan, Adam Klivans, and Yang Yuan. Hyperparameter opti-\\nmization: A spectral approach. ICLR, 2018.\\n[39] Geo\\ufb00rey Hinton, Nitish Srivastava, and Kevin Swersky. Neural net-\\nworks for machine learning lecture 6a overview of mini-batch gradient\\ndescent. Cited on, 14, 2012.\\n[40] Ilija Ilievski, Taimoor Akhtar, Jiashi Feng, and Christine Annette Shoe-\\nmaker.\\nE\\ufb03cient hyperparameter optimization for deep learning al-\\ngorithms using deterministic RBF surrogates.\\nIn Proceedings of the\\nThirty-First AAAI Conference on Arti\\ufb01cial Intelligence, February 4-9,\\n2017, San Francisco, California, USA., pages 822\\u2013829, 2017.\\nBIBLIOGRAPHY\\n117\\n[41] Martin Jaggi.\\nRevisiting frank-wolfe: Projection-free sparse convex\\noptimization. In ICML, 2013.\\n[42] Martin Jaggi and Marek Sulovsk\\u00b4y. A simple algorithm for nuclear norm\\nregularized problems. In ICML, pages 471\\u2013478, 2010.\\n[43] Kevin G. Jamieson and Ameet Talwalkar.\\nNon-stochastic best arm\\nidenti\\ufb01cation and hyperparameter optimization. In Proceedings of the\\n19th International Conference on Arti\\ufb01cial Intelligence and Statistics,\\nAISTATS 2016, Cadiz, Spain, May 9-11, 2016, pages 240\\u2013248, 2016.\\n[44] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent\\nusing predictive variance reduction. In Advances in neural information\\nprocessing systems, pages 315\\u2013323, 2013.\\n[45] Adam Kalai and Santosh Vempala. E\\ufb03cient algorithms for online de-\\ncision problems. Journal of Computer and System Sciences, 71(3):291\\u2013\\n307, 2005.\\n[46] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic\\noptimization. arXiv preprint arXiv:1412.6980, 2014.\\n[47] Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient ver-\\nsus gradient descent for linear predictors. Inf. Comput., 132(1):1\\u201363,\\n1997.\\n[48] Jyrki Kivinen and Manfred K. Warmuth.\\nRelative loss bounds for\\nmultidimensional regression problems. Machine Learning, 45(3):301\\u2013\\n329, 2001.\\n[49] Simon Lacoste-Julien, Martin Jaggi, Mark W. Schmidt, and Patrick\\nPletscher.\\nBlock-coordinate frank-wolfe optimization for structural\\nsvms. In Proceedings of the 30th International Conference on Machine\\nLearning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pages 53\\u2013\\n61, 2013.\\n[50] J. Lee, B. Recht, R. Salakhutdinov, N. Srebro, and J. A. Tropp. Prac-\\ntical large-scale optimization for max-norm regularization. In NIPS,\\npages 1297\\u20131305, 2010.\\n[51] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar.\\nHyperband: A Novel Bandit-Based Approach to Hyperparameter Op-\\ntimization. ArXiv e-prints, March 2016.\\n118\\nBIBLIOGRAPHY\\n[52] H. Brendan McMahan and Matthew J. Streeter. Adaptive bound op-\\ntimization for online convex optimization. In COLT 2010 - The 23rd\\nConference on Learning Theory, Haifa, Israel, June 27-29, 2010, pages\\n244\\u2013256, 2010.\\n[53] Arkadi S. Nemirovski and David B. Yudin. Problem Complexity and\\nMethod E\\ufb03ciency in Optimization. John Wiley UK/USA, 1983.\\n[54] A.S. Nemirovskii. Interior point polynomial time methods in convex\\nprogramming, 2004. Lecture Notes.\\n[55] AS Nemirovskii.\\nInterior point polynomial time methods in convex\\nprogramming. Lecture Notes, 2004.\\n[56] Y. Nesterov. A method of solving a convex programming problem with\\nconvergence rate O(1/k2). Soviet Mathematics Doklady, 27(2):372\\u2013376,\\n1983.\\n[57] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic\\nCourse. Applied Optimization. Springer, 2004.\\n[58] Y. E. Nesterov and A. S. Nemirovskii. Interior Point Polynomial Al-\\ngorithms in Convex Programming. SIAM, Philadelphia, 1994.\\n[59] Ryan O\\u2019Donnell. Analysis of Boolean Functions. Cambridge University\\nPress, New York, NY, USA, 2014.\\n[60] Francesco Orabona and Koby Crammer. New adaptive algorithms for\\nonline classi\\ufb01cation. In Proceedings of the 24th Annual Conference on\\nNeural Information Processing Systems 2010., pages 1840\\u20131848, 2010.\\n[61] Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural\\ncomputation, 6(1):147\\u2013160, 1994.\\n[62] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.\\nMaking\\ngradient descent optimal for strongly convex stochastic optimization.\\nIn ICML, 2012.\\n[63] Holger Rauhut. Compressive sensing and structured random matrices.\\nTheoretical foundations and numerical methods for sparse recovery, 9:1\\u2013\\n92, 2010.\\n[64] Benjamin Recht. Embracing the random. http://www.argmin.net/\\n2016/06/23/hyperband/, 2016.\\nBIBLIOGRAPHY\\n119\\n[65] Benjamin Recht. The news on auto-tuning. http://www.argmin.net/\\n2016/06/20/hypertuning/, 2016.\\n[66] Jasson D. M. Rennie and Nathan Srebro. Fast maximum margin matrix\\nfactorization for collaborative prediction. In Proceedings of the 22Nd\\nInternational Conference on Machine Learning, ICML \\u201905, pages 713\\u2013\\n719, New York, NY, USA, 2005. ACM.\\n[67] Herbert Robbins and Sutton Monro.\\nA stochastic approximation\\nmethod. The Annals of Mathematical Statistics, 22(3):400\\u2013407, 09 1951.\\n[68] R.T. Rockafellar. Convex Analysis. Convex Analysis. Princeton Uni-\\nversity Press, 1997.\\n[69] R. Salakhutdinov and N. Srebro.\\nCollaborative \\ufb01ltering in a non-\\nuniform world: Learning with the weighted trace norm. In NIPS, pages\\n2056\\u20132064, 2010.\\n[70] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing \\ufb01nite\\nsums with the stochastic average gradient. Mathematical Programming,\\n162(1-2):83\\u2013112, 2017.\\n[71] Shai Shalev-Shwartz. Online Learning: Theory, Algorithms, and Ap-\\nplications. PhD thesis, The Hebrew University of Jerusalem, 2007.\\n[72] Shai Shalev-Shwartz, Alon Gonen, and Ohad Shamir. Large-scale con-\\nvex minimization with a low-rank constraint. In ICML, pages 329\\u2013336,\\n2011.\\n[73] Shai Shalev-Shwartz and Yoram Singer. A primal-dual perspective of\\nonline learning algorithms. Machine Learning, 69(2-3):115\\u2013142, 2007.\\n[74] Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cot-\\nter.\\nPegasos: primal estimated sub-gradient solver for svm.\\nMath.\\nProgram., 127(1):3\\u201330, 2011.\\n[75] O. Shamir and S. Shalev-Shwartz.\\nCollaborative \\ufb01ltering with the\\ntrace norm: Learning, bounding, and transducing. JMLR - Proceedings\\nTrack, 19:661\\u2013678, 2011.\\n[76] Ohad Shamir and Tong Zhang.\\nStochastic gradient descent for\\nnon-smooth optimization: Convergence results and optimal averaging\\nschemes. In ICML, 2013.\\n120\\nBIBLIOGRAPHY\\n[77] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates\\nwith sublinear memory cost. arXiv preprint arXiv:1804.04235, 2018.\\n[78] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian\\noptimization of machine learning algorithms. In Advances in Neural\\nInformation Processing Systems 25: 26th Annual Conference on Neural\\nInformation Processing Systems 2012. Proceedings of a meeting held\\nDecember 3-6, 2012, Lake Tahoe, Nevada, United States., pages 2960\\u2013\\n2968, 2012.\\n[79] Jasper Snoek, Kevin Swersky, Richard S. Zemel, and Ryan P. Adams.\\nInput warping for bayesian optimization of non-stationary functions. In\\nProceedings of the 31th International Conference on Machine Learning,\\nICML 2014, Beijing, China, 21-26 June 2014, pages 1674\\u20131682, 2014.\\n[80] Nathan Srebro. Learning with Matrix Factorizations. PhD thesis, Mas-\\nsachusetts Institute of Technology, 2004.\\n[81] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task\\nbayesian optimization. In Advances in Neural Information Processing\\nSystems 26: 27th Annual Conference on Neural Information Processing\\nSystems 2013. Proceedings of a meeting held December 5-8, 2013, Lake\\nTahoe, Nevada, United States., pages 2004\\u20132012, 2013.\\n[82] Ambuj Tewari, Pradeep D. Ravikumar, and Inderjit S. Dhillon. Greedy\\nalgorithms for structurally constrained high dimensional problems. In\\nNIPS, pages 882\\u2013890, 2011.\\n[83] A. M. Turing.\\nComputing machinery and intelligence.\\nMind,\\n59(236):433\\u2013460, 1950.\\n[84] Ziyu Wang, Masrour Zoghi, Frank Hutter, David Matheson, and Nando\\nde Freitas. Bayesian optimization in high dimensions via random em-\\nbeddings. In IJCAI 2013, Proceedings of the 23rd International Joint\\nConference on Arti\\ufb01cial Intelligence, Beijing, China, August 3-9, 2013,\\npages 1778\\u20131784, 2013.\\n[85] Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp\\nconvergence over nonconvex landscapes, from any initialization. arXiv\\npreprint arXiv:1806.01811, 2018.\\n[86] Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. Linear convergence\\nwith condition number independent access of full gradients. In Advances\\nin Neural Information Processing Systems, pages 980\\u2013988, 2013.\\nBIBLIOGRAPHY\\n121\\n[87] Martin Zinkevich.\\nOnline convex programming and generalized in-\\n\\ufb01nitesimal gradient ascent.\\nIn Proceedings of the 20th International\\nConference on Machine Learning, pages 928\\u2013936, 2003.\\n\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        " arxiv_full_path = 'data/raw/arxiv_papers_full.json'\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(arxiv_full_path):\n",
        "    print(f\"File {arxiv_full_path} does not exist.\")\n",
        "else:\n",
        "    # Load the data\n",
        "    with open(arxiv_full_path, 'r') as f:\n",
        "        arxiv_papers_full = json.load(f)\n",
        "    print(f\"Loaded {len(arxiv_papers_full)} ArXiv papers.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gURFrCxyv-EY",
        "outputId": "8aa27fbe-005b-4144-a93a-cc4b7c17ddc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 ArXiv papers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK data if not already done\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove non-ASCII characters\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to all papers\n",
        "for paper in arxiv_papers_full:\n",
        "    if 'full_text' in paper and paper['full_text']:\n",
        "        paper['cleaned_text'] = clean_text(paper['full_text'])\n",
        "    else:\n",
        "        paper['cleaned_text'] = \"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSLcn122wGkH",
        "outputId": "c49c7956-2fb5-467d-951b-d5a3f7b156a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_chunks(text, max_words=500, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = start + max_words\n",
        "        chunk = ' '.join(words[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start += max_words - overlap  # Overlap to maintain context\n",
        "    return chunks\n",
        "\n",
        "# Apply segmentation to all papers\n",
        "arxiv_chunks = []\n",
        "for paper in arxiv_papers_full:\n",
        "    if 'cleaned_text' in paper and paper['cleaned_text']:\n",
        "        chunks = split_into_chunks(paper['cleaned_text'], max_words=500, overlap=50)\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            arxiv_chunks.append({\n",
        "                'paper_id': paper['id'],\n",
        "                'title': paper['title'],\n",
        "                'authors': paper['authors'],\n",
        "                'published': paper['published'],\n",
        "                'categories': paper['categories'],\n",
        "                'chunk_id': f\"{paper['id']}_chunk_{idx+1}\",\n",
        "                'text': chunk\n",
        "            })\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "print(f\"Created {len(arxiv_chunks)} text chunks from ArXiv papers.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM5K56I-wQ8L",
        "outputId": "97b7b318-74eb-4984-a2fe-e6842914e0e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 1861 text chunks from ArXiv papers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_arxiv_path = 'data/processed/arxiv_chunks.json'\n",
        "\n",
        "# Save the chunks to a JSON file\n",
        "with open(processed_arxiv_path, 'w') as f:\n",
        "    json.dump(arxiv_chunks, f, indent=2)\n",
        "\n",
        "print(f\"Saved processed data to {processed_arxiv_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_YurPYjwXXx",
        "outputId": "d533246d-81d0-4ffa-9473-7c3921c0b765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved processed data to data/processed/arxiv_chunks.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the processed data\n",
        "with open(processed_arxiv_path, 'r') as f:\n",
        "    arxiv_chunks = json.load(f)\n",
        "\n",
        "# Display a sample chunk\n",
        "sample_chunk = arxiv_chunks[0]\n",
        "print(\"Sample Processed Chunk:\")\n",
        "print(json.dumps(sample_chunk, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiXulH1wwa_D",
        "outputId": "47fe29f4-27de-4ed4-8928-0163cfddaa1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Processed Chunk:\n",
            "{\n",
            "  \"paper_id\": \"1909.03550v1\",\n",
            "  \"title\": \"Lecture Notes: Optimization for Machine Learning\",\n",
            "  \"authors\": [\n",
            "    \"Elad Hazan\"\n",
            "  ],\n",
            "  \"published\": \"2019-09-08\",\n",
            "  \"categories\": [\n",
            "    \"cs.LG\",\n",
            "    \"stat.ML\"\n",
            "  ],\n",
            "  \"chunk_id\": \"1909.03550v1_chunk_1\",\n",
            "  \"text\": \"lecture notes optimization for machine learning version all rights reserved elad hazan wwwcsprincetoneduehazan arxivv cslg sep ii preface this text was written to accompany a series of lectures given at the machine learning summer school buenos aires following a lecture series at the simons center for theoretical computer science berkeley it was extended for the course cos d optimization for machine learning princeton university spring i am grateful to paula gradu for proofreading parts of this manuscript im also thankful for the help of the following students and colleagues for corrections and suggestions to this text udaya ghai john hallman noe pion xinyi chen iii iv preface figure professor arkadi nemirovski pioneer of mathematical optimiza tion contents preface iii introduction examples of optimization problems in machine learning empirical risk minimization matrix completion and recommender systems learning in linear dynamical systems why is mathematical programming hard the computational model hardness of constrained mathematical programming basic concepts in optimization and analysis basic denitions and the notion of convexity projections onto convex sets introduction to optimality conditions solution concepts for nonconvex optimization potentials for distance to optimality gradient descent and the polyak stepsize exercises bibliographic remarks stochastic gradient descent training feedforward neural networks gradient descent for smooth optimization stochastic gradient descent bibliographic remarks generalization and nonsmooth optimization a note on nonsmooth optimization minimizing regret v vi contents regret implies generalization online gradient descent lower bounds online gradient descent for strongly convex functions online gradient descent implies sgd exercises bibliographic remarks regularization motivation prediction from expert advice the weighted majority algorithm randomized weighted majority hedge the regularization framework the rftl algorithm mirrored descent deriving online gradient descent deriving multiplicative updates technical background regularization functions regret bounds for mirrored descent exercises bibliographic remarks adaptive regularization adaptive learning rates intuition a regularization viewpoint tools from matrix calculus the adagrad algorithm and its analysis diagonal adagrad stateoftheart from adam to shampoo and beyond exercises bibliographic remarks variance reduction variance reduction intuition setting and denitions the variance reduction advantage a simple variancereduced algorithm bibliographic remarks contents vii nesterov acceleration algorithm and implementation analysis bibliographic remarks the conditional gradient method review relevant concepts from linear algebra motivation matrix completion and recommendation systems the frankwolfe method projections vs linear optimization exercises bibliographic remarks second order methods for machine learning motivating example linear regression selfconcordant functions newtons method for selfconcordant functions lineartime secondorder methods estimators for the hessian inverse incorporating the estimator exercises bibliographic remarks hyperparameter optimization formalizing the problem hyperparameter optimization algorithms a spectral method background compressed sensing the spectral algorithm bibliographic remarks viii contents notation we use the following mathematical notation in this writeup ddimensional euclidean space is denoted rd vectors are denoted by boldface lowercase letters such as x rd co ordinates of vectors are denoted by underscore notation xi or regular brackets xi matrices are denoted by boldface uppercase letters such as x rmn their coordinates by xi j or xij functions are denoted by lower case letters f rd r the kth dierential of function f\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Embedding and Indexing"
      ],
      "metadata": {
        "id": "llzZjopKwozh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the processed ArXiv chunks\n",
        "with open('data/processed/arxiv_chunks.json', 'r') as f:\n",
        "    arxiv_chunks = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(arxiv_chunks)} ArXiv text chunks for embedding.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh_jVKC9wdrc",
        "outputId": "6f6158e2-71c8-4a8a-b342-75410df3b59e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1861 ArXiv text chunks for embedding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443,
          "referenced_widgets": [
            "ccd810977aff4e36b22941f81a664764",
            "015c96fa392645b09ed3a5bff868dbce",
            "bec78f1d2bf1400a91ff08c302ff4809",
            "951a75647b4049c78a8f2aa1eaa952ba",
            "8b5135dbace34a848660e7aa708606d4",
            "956c0a3e67d64f11b80d1cd86f6953d0",
            "02eeb6a388524180af15734587914f18",
            "5f610ba908674f9b823f44dd66941899",
            "5db03be6c0e0484796939a49b76342fd",
            "6e6e353fc2c4402996e0ece6c9f8e5f3",
            "57bafc2f9d39427dbf4cdba75e328aba",
            "0e9f6565775a43e2a260c933850fbe96",
            "2d39cbd9b4354b15853513e5e1b2c128",
            "91b96ce1fd384a0ebcbefb54ec86164b",
            "fc1ef955bdea49f0a7fa229dfddeea8c",
            "a62873a12afe43c3a684b58960d4df14",
            "6f3e3e05b2fe4c538d7b1ef832c139f3",
            "3f81c9efaf9f4db389cbd5fb0bc9c818",
            "e1ae7ef4189442c58573d511906b4953",
            "42d7ccdd24a947639c09ba8055eca79c",
            "f5723c3aba9043a89ec35039a93cb15b",
            "9e0f6c0c9dab429395e874874ab30a0d",
            "aba576a7e9e74d00995e9727b7d5fb9b",
            "d7ca3d74b23a4169a00eff976ab4e0d4",
            "741dfcf62025495884a7016975751661",
            "45c93db61dcc44c4bee432046b2c699b",
            "699165a630aa4b8e9cb9bc55a50d474d",
            "9d9d5ae6998748378ac43f22abb9f00f",
            "9ee242c7a84a4c3399e608ef308f670e",
            "ed30e22f807e4bb495ae6d65bca72e94",
            "fad340d07f174c46833ed4f75acebb55",
            "6704ef11432c45369f87791edff59e97",
            "1c0dbd967eda4f179923d70c7068963d",
            "8538e490d4ea41fba1f10e1000c34c33",
            "1dabc0d0daf24fb7bcac405b160d3fb8",
            "1aa3828f3aad4336a10a8f094a0a9cdb",
            "1e9823864033440ab065b19a18dd6bdb",
            "2784d4a33ba34117850edf485ea14460",
            "0ca142aa8dd042958ff0b652cb90b528",
            "f67e7b41da8b4cf29e21d6073426d13a",
            "29631e282963437c8b0a09337a6000d7",
            "d8385a88289c43588296fa7e7598b6d6",
            "fadef56086554e89bb8a18aaecbc8bd3",
            "262a954787964d28943a148f1aabf78d",
            "01be710bc46e4feb812204019152a181",
            "163491a1435f49aebfe62a4c30cf588b",
            "2078ed8477c74b549ad6ec31074ec6f6",
            "c748bfdf1b354facbe1816a365bf298f",
            "ecb06e81b4e446edae1c8fef6870a997",
            "875c3f35fb0f42808d1d9dc8f6c8c5da",
            "8710b9fb793a47379e70c9915a59f63b",
            "662f05663c3d460991414891e2bcf0a2",
            "8d809c05b0ed47fa9eb12900cff541ac",
            "a42189b8fb884e47a0e0e7ca2a4dcac5",
            "4c3ea191f02c4b31b3101a1bcd34199b",
            "98c340f017b64c5f9a81569163953e2e",
            "14e3c40ac2494c24bef7eb1c5b925f5c",
            "44402fbb61a64919be96d53508fd650e",
            "2cafad12786944e2b7133e78e883daba",
            "9c43340f53e94788adf2e166a069159f",
            "b8596f99da4e4bec900a72e984f2e7a1",
            "f2283a2378f34cb9b62e8135b338bdd9",
            "3a58308a67ac471d80bc0d54c3585f8e",
            "be675f5885ac43c28284e99279654476",
            "fd65d563759c43bab8d28f6589399e86",
            "ffedbbc5b3224b71a3f7e53241ff7f32",
            "fe7b3d78c3414474ad38346c0f3b1de2",
            "3b7098322a8f4917baca2548d5de34d2",
            "a73a1871dfe64e369d84d57eb5e3ecbe",
            "c4ad9eb919d14505823ee5f5d6dccfb6",
            "10bc5d87f19145f6819cc660f25f797b",
            "112d9bd7abbc488b9490277f7ec5c608",
            "936a004588264d92bc2b91c9e160b6ee",
            "beebcd05cb244b03a58dc97557bb4d99",
            "8eb1f3ddc9334e9b924ec4ef7abc65e5",
            "2d9b1aa0f00d4d118c470bbabafa53c9",
            "7f45308dec824f03a4f6e5da673c6594",
            "1707c98fb9b54709a43c779a3b068daa",
            "bac89e8a09244410912f4a1f2b8858c8",
            "12a77f5ac1f24861926dc58cc3cc3f97",
            "9e5e1e7e80004cd2b4a07ec26a62fd64",
            "4b34097c53cc41dba9c638a1ad68ea68",
            "6d3293d7a2324f73a8695e5d7a8995a3",
            "25dc09fb82eb486f98dc22010ce24423",
            "672752a0a3014c11aeffd70897a650bb",
            "7160c25bce3a45409976457793e795a6",
            "cbf167dbb15d4b77a8bb2409bdfdab7f",
            "cca3ebddbd9f420ca425daf3bd8db5bb",
            "5a18f5eeaa53411ab5ee709a834096b4",
            "e076d0594c7b415294b3a83344b92fdd",
            "7690ec64ee5a404c988911c735de0f57",
            "a424b15459d54475a500f8f9df0cf35e",
            "1d301535b1e24f54b82c54b2050d5d81",
            "2f99a798834d4f34b690282b040d0856",
            "54c76d2ec1da450080770071581373c1",
            "d60c86f42c324740accba4e799abfecc",
            "6d7a61e58ab04619ba1d470e41583b16",
            "78a4197df4764132953e386fc3b5e648",
            "6df0f4fd03eb44d699721ab35cee30ff",
            "83971072b55f43f1ae80c5de4328a22c",
            "3c06627c1d674a84b7126426423df22e",
            "69db3b67c83d435ab6691fa85ef52dad",
            "751416d1cee049a59c5235f73560613c",
            "173536b69d514e1997bdecdeeb6023e1",
            "0d48dd256cfb451baf5ef53e782d28e9",
            "a2b052b6feea4a0fb35aa9afaa1fdaa7",
            "fdd82cbcffe849c1903f67528b390a74",
            "c3dd235972f34622bf0152a2b1bd86f1",
            "96b953d6b9334198b16bfd56acfa4ce1",
            "cbed670feaf24548985afba796845fba",
            "1a0abd5e63804187a66544a21c04242d",
            "ad89408186cb423d8624af1ccd58437a",
            "3cc5ff13d7f84098991a94df50b025e1",
            "aa01140e575e4fb1adde1a97e563d193",
            "0d4722646606439e9addfb3f29979d44",
            "25a522ee0ec54325b70c00f0d5397ffa",
            "b8daa0553b624470b241db503cc5e3f6",
            "e263bd2d8f174dfb800fe0abdd2dff6d",
            "76880089511d41a8b779a5efa5118da7",
            "646f94414de44ca08bcbc5a898bc001f",
            "fd699c4e161b47dd8da518979df3e7f9"
          ]
        },
        "id": "zxMCWrRPwzIk",
        "outputId": "e5df7d48-fc15-46aa-fb82-9199d07da4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccd810977aff4e36b22941f81a664764"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e9f6565775a43e2a260c933850fbe96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aba576a7e9e74d00995e9727b7d5fb9b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8538e490d4ea41fba1f10e1000c34c33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01be710bc46e4feb812204019152a181"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98c340f017b64c5f9a81569163953e2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe7b3d78c3414474ad38346c0f3b1de2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1707c98fb9b54709a43c779a3b068daa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a18f5eeaa53411ab5ee709a834096b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83971072b55f43f1ae80c5de4328a22c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a0abd5e63804187a66544a21c04242d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_embeddings(chunks, model, batch_size=32):\n",
        "    texts = [chunk['text'] for chunk in chunks]\n",
        "    embeddings = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating Embeddings\"):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        batch_embeddings = model.encode(batch_texts, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=False)\n",
        "        embeddings.append(batch_embeddings)\n",
        "\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    return embeddings\n",
        "\n",
        "# Generate embeddings\n",
        "arxiv_embeddings = generate_embeddings(arxiv_chunks, embedding_model, batch_size=32)\n",
        "print(f\"Generated embeddings with shape: {arxiv_embeddings.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQN4PugSw0xW",
        "outputId": "465013ed-098b-4033-dea9-0bbaa9198c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Embeddings: 100%|██████████| 59/59 [00:09<00:00,  6.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings with shape: (1861, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "embedding_dim = arxiv_embeddings.shape[1]\n",
        "\n",
        "# Initialize a FAISS index\n",
        "index = faiss.IndexFlatL2(embedding_dim)  # Using L2 distance\n",
        "\n",
        "# Add embeddings to the index\n",
        "index.add(arxiv_embeddings)\n",
        "print(f\"FAISS index contains {index.ntotal} vectors.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ACJUscvw-hG",
        "outputId": "0442ebd9-4a05-43e0-9ea6-41800cdfb02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index contains 1861 vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the FAISS index\n",
        "faiss.write_index(index, 'models/arxiv_faiss.index')\n",
        "print(\"Saved FAISS index to models/arxiv_faiss.index.\")\n",
        "\n",
        "# Save the embeddings\n",
        "np.save('models/arxiv_embeddings.npy', arxiv_embeddings)\n",
        "print(\"Saved embeddings to models/arxiv_embeddings.npy.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lip9oZIFxF6f",
        "outputId": "c908e4b6-39f6-4bd0-a832-db62c9d03e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved FAISS index to models/arxiv_faiss.index.\n",
            "Saved embeddings to models/arxiv_embeddings.npy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_index = faiss.read_index('models/arxiv_faiss.index')\n",
        "\n",
        "# Example: Querying the first chunk's embedding to find similar chunks\n",
        "query_embedding = arxiv_embeddings[0].reshape(1, -1)\n",
        "k = 5  # Number of nearest neighbors\n",
        "\n",
        "distances, indices = loaded_index.search(query_embedding, k)\n",
        "print(f\"Top {k} nearest neighbors for the first chunk:\")\n",
        "for i in range(k):\n",
        "    neighbor_idx = indices[0][i]\n",
        "    neighbor_chunk = arxiv_chunks[neighbor_idx]\n",
        "    print(f\"{i+1}. Chunk ID: {neighbor_chunk['chunk_id']}, Title: {neighbor_chunk['title']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07_6KmFoxJFc",
        "outputId": "f7ddc8a5-e322-40cf-e1d3-3759079a3815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 nearest neighbors for the first chunk:\n",
            "1. Chunk ID: 1909.03550v1_chunk_1, Title: Lecture Notes: Optimization for Machine Learning\n",
            "2. Chunk ID: 1909.03550v1_chunk_44, Title: Lecture Notes: Optimization for Machine Learning\n",
            "3. Chunk ID: 1909.03550v1_chunk_53, Title: Lecture Notes: Optimization for Machine Learning\n",
            "4. Chunk ID: 1906.06821v2_chunk_1, Title: A Survey of Optimization Methods from a Machine Learning Perspective\n",
            "5. Chunk ID: 1906.06821v2_chunk_49, Title: A Survey of Optimization Methods from a Machine Learning Perspective\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Module Development"
      ],
      "metadata": {
        "id": "jHd4KHbJxTyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.read_index('models/arxiv_faiss.index')\n",
        "\n",
        "# Load the processed ArXiv chunks\n",
        "with open('data/processed/arxiv_chunks.json', 'r') as f:\n",
        "    arxiv_chunks = json.load(f)\n",
        "\n",
        "print(\"Loaded FAISS index and ArXiv chunks for retrieval.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEt4jE6VxPJG",
        "outputId": "b7cf9986-8fc9-475c-bff4-4a1dc9e709d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded FAISS index and ArXiv chunks for retrieval.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_similar_chunks(query, model, index, chunks, top_k=5):\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
        "\n",
        "    # Search in FAISS index\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # Retrieve the corresponding chunks\n",
        "    retrieved_chunks = [chunks[idx] for idx in indices[0]]\n",
        "\n",
        "    return retrieved_chunks\n"
      ],
      "metadata": {
        "id": "I2SLDGyPxc56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_query = \"What is Gradient Descent algorithm?\"\n",
        "\n",
        "# Retrieve similar chunks\n",
        "retrieved = retrieve_similar_chunks(sample_query, embedding_model, index, arxiv_chunks, top_k=5)\n",
        "\n",
        "# Display the retrieved chunks\n",
        "for i, chunk in enumerate(retrieved, 1):\n",
        "    print(f\"--- Chunk {i} ---\")\n",
        "    print(f\"Title: {chunk['title']}\")\n",
        "    print(f\"Authors: {', '.join(chunk['authors'])}\")\n",
        "    print(f\"Published: {chunk['published']}\")\n",
        "    print(f\"Categories: {', '.join(chunk['categories'])}\")\n",
        "    print(f\"Chunk ID: {chunk['chunk_id']}\")\n",
        "    print(f\"Text: {chunk['text'][:200]}...\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZt6Ej5YxgNZ",
        "outputId": "faac8d44-fabf-4460-81cb-ecfd80a32f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chunk 1 ---\n",
            "Title: A comprehensive review of Quantum Machine Learning: from NISQ to Fault Tolerance\n",
            "Authors: Yunfei Wang, Junyu Liu\n",
            "Published: 2024-01-21\n",
            "Categories: quant-ph, cs.AI, cs.LG, stat.ML\n",
            "Chunk ID: 2401.11351v2_chunk_13\n",
            "Text: more computationally practical the essence of this section can be summarized as follows when presented with an objective function or loss function denoted as l our goal is to identify its minima the s...\n",
            "\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Title: Category Theory in Machine Learning\n",
            "Authors: Dan Shiebler, Bruno Gavranović, Paul Wilson\n",
            "Published: 2021-06-13\n",
            "Categories: cs.LG\n",
            "Chunk ID: 2106.07032v1_chunk_4\n",
            "Text: parameter updates and learning finally we discuss how lensbased formalisms for learning capture the various machine learning algorithms used in practice since pixels are not actually realvalued we may...\n",
            "\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Title: A Survey of Optimization Methods from a Machine Learning Perspective\n",
            "Authors: Shiliang Sun, Zehui Cao, Han Zhu, Jing Zhao\n",
            "Published: 2019-06-17\n",
            "Categories: cs.LG, math.OC, stat.ML\n",
            "Chunk ID: 1906.06821v2_chunk_8\n",
            "Text: descent next we give the formal expression of gradient descent method for a linear regression model we assume that fx is the function to be learned l is the loss function and is the parameter to be op...\n",
            "\n",
            "\n",
            "--- Chunk 4 ---\n",
            "Title: Lecture Notes: Optimization for Machine Learning\n",
            "Authors: Elad Hazan\n",
            "Published: 2019-09-08\n",
            "Categories: cs.LG, stat.ML\n",
            "Chunk ID: 1909.03550v1_chunk_12\n",
            "Text: much more indepth treatment of the topics surveyed in this background chapter for background in convex analysis see the texts the classic text book gives a broad introduction to convex optimization wi...\n",
            "\n",
            "\n",
            "--- Chunk 5 ---\n",
            "Title: Distributed Multi-Task Learning with Shared Representation\n",
            "Authors: Jialei Wang, Mladen Kolar, Nathan Srebro\n",
            "Published: 2016-03-07\n",
            "Categories: cs.LG, stat.ML\n",
            "Chunk ID: 1603.02185v1_chunk_7\n",
            "Text: gradient maybe the simplest distributed optimization algo rithm for is the proximal gradient descent it is not hard to see that computation of the gradient lnw can be easily done in a distributed way ...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation Module Integration"
      ],
      "metadata": {
        "id": "I8km9M9Dx4Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Initialize the tokenizer and model\n",
        "generation_model_name = 't5-base'  # We can use 't5-large' for better performance\n",
        "generation_tokenizer = AutoTokenizer.from_pretrained(generation_model_name)\n",
        "generation_model = AutoModelForSeq2SeqLM.from_pretrained(generation_model_name).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "378f09a0c24e4ad7b1a9b7a9cb830cb3",
            "818eb89c1a2a47a28814376264f4e398",
            "eb8e7640ae2044feb652241d903cfb37",
            "adeea763bb024c62b1145ae06dc470d5",
            "a24a9c37a8634e6e8887d5db7da465c6",
            "c8cdb0b859694d048417ebda64dfa903",
            "c91a069c6bc14080b2781977e38006e5",
            "b9888e3d93a34b6ca6b66dfba142711f",
            "5271dd63aee64460b663720017dc71a5",
            "cecddcef45684b2d934d58c7b77e0166",
            "cc98b649a49e4193a5a464c58eb7c93b",
            "b96e9329e0f540a0af652b8fd9558710",
            "c850a350678e4d75aee994d7d0542001",
            "4b0b07aadcaf41d498bc05424ece1cad",
            "aadea8868d8348618eb220fdad71530f",
            "ea607c52e4454f4283745c042b1c724f",
            "7d72cf60824447b5bfd7fb67ee833907",
            "06fb4db5bc1f40a0af04791be0a72197",
            "c5ae7e3a71154fb09b3ad9137f78bc84",
            "1973fe281f4f4b70a6d48b8bb3c587ea",
            "cdf44c22b1094dd1955a088cce438509",
            "5339e3edab084d5db73bac717ad7ba62",
            "39340122f257475fb385c09400d96ec5",
            "159d5cbeae514a64adee19eb3be7ef3e",
            "97a672aa9ac4438aa79b951a490b7127",
            "46b9d2bf19264da3bfe23914a6a8ee07",
            "90b5ecaff4ca4c7ea85bfdaf90db2462",
            "b67437a93fc4472e8205c9c1e96ce195",
            "f3636bd920994340a1496a68fe0f9e2b",
            "674061a1b100474ba1500c47520b2215",
            "2666dfc96355414a9f89839e5fbf693e",
            "db8dcc13d46a40908a64bd8b06a6c7bd",
            "2dcf4dcc1d2b4e75bb1ba348ea653996",
            "fdc0996dc31248bba002cb5269c02df6",
            "8c3a9596957e48cdbe8808ac762ad7e7",
            "eaa48e8fdd9045f78f0bd177996b194e",
            "343858b0afd445fd8b5bf2ab2b0347b1",
            "b210d6fe81084b839d2593e80fa995b8",
            "4966132c2523425eb5da497b9f66a61c",
            "3ce58743e6644489a0c308c9063c45e5",
            "b69ef220262a4b82866114b652917c9b",
            "5105e058932942958f1c09a162237ef2",
            "9ac0b2eee03a4588b1634263d92eb2d9",
            "4b56a6221e2f4f76a537e0d0f746b7a6",
            "8d4c233461b74d38a810fa1ad83a54be",
            "b95f711b50df44a3a607ef70219224ae",
            "7fdf0bcf1bcc49d0826d0a61ad1c6e6a",
            "3bd8e69b77f1447b99c4ec4714ed60c8",
            "687bb6f07fc14bed945afcf130d7a012",
            "d7a52d54f461420cba7b8ad98b991975",
            "ae57023746e940f9ab793ad5402883a7",
            "a34826ae523842be88b2e9bfe988df2f",
            "9ebc0cd9d3af49b4a5232b571d06a8cd",
            "67b084f8a3c547d4aee22eeb5ff45a2c",
            "fdf47df7714c41f99365a4f24b38b6a2"
          ]
        },
        "id": "b8MRh2Z_xiWE",
        "outputId": "19e5895a-a07c-4abf-b0fc-17f56c472295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "378f09a0c24e4ad7b1a9b7a9cb830cb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b96e9329e0f540a0af652b8fd9558710"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39340122f257475fb385c09400d96ec5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdc0996dc31248bba002cb5269c02df6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d4c233461b74d38a810fa1ad83a54be"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query, retrieved_chunks, tokenizer, model, max_length=200):\n",
        "    # Concatenate the texts from retrieved chunks\n",
        "    context = \" \".join([chunk['text'] for chunk in retrieved_chunks])\n",
        "\n",
        "    # Add the retrieved context to the question\n",
        "    input_text = f\"question: {query} context: {context}\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
        "\n",
        "    # Generate the answer\n",
        "    outputs = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True) #We can use OpenAI's GPT models for better performance\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "Wx5YP_Lnx-AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample query\n",
        "sample_query = \"What is machine learning?\"\n",
        "\n",
        "# Retrieve similar chunks\n",
        "retrieved_chunks = retrieve_similar_chunks(sample_query, embedding_model, index, arxiv_chunks, top_k=5)\n",
        "\n",
        "# Generate an answer\n",
        "answer = generate_answer(sample_query, retrieved_chunks, generation_tokenizer, generation_model)\n",
        "\n",
        "print(f\"Generated Answer:\\n{answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGuU-sdvyFEe",
        "outputId": "ab26c77d-e88d-44a7-fb79-13db14b0241e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Answer:\n",
            "the process in which computers learn to make decisions based on the given data set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r rag_pipeline_project.zip data/ models/ outputs/ app.py\n",
        "# Add any other directories or files as needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-5JPwHhyHaa",
        "outputId": "79026caa-c789-43cd-d6bb-ca0f705609cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tzip warning: name not matched: app.py\n",
            "  adding: data/ (stored 0%)\n",
            "  adding: data/raw/ (stored 0%)\n",
            "  adding: data/raw/pubmed_pdfs/ (stored 0%)\n",
            "  adding: data/raw/pubmed_pdfs/39341632.pdf (deflated 57%)\n",
            "  adding: data/raw/pubmed_pdfs/39341153.pdf (deflated 56%)\n",
            "  adding: data/raw/pubmed_pdfs/39341210.pdf (deflated 77%)\n",
            "  adding: data/raw/pubmed_pdfs/39340586.pdf (deflated 81%)\n",
            "  adding: data/raw/pubmed_pdfs/39341304.pdf (deflated 57%)\n",
            "  adding: data/raw/pubmed_pdfs/39338970.pdf (deflated 85%)\n",
            "  adding: data/raw/pubmed_pdfs/39341638.pdf (deflated 57%)\n",
            "  adding: data/raw/pubmed_pdfs/39341637.pdf (deflated 57%)\n",
            "  adding: data/raw/pubmed_pdfs/39340756.pdf (deflated 77%)\n",
            "  adding: data/raw/pubmed_pdfs/39341499.pdf (deflated 57%)\n",
            "  adding: data/raw/pubmed_pdfs/39341043.pdf (deflated 57%)\n",
            "  adding: data/raw/pubmed_pdfs/39340015.pdf (deflated 81%)\n",
            "  adding: data/raw/pubmed_pdfs/39340739.pdf (deflated 78%)\n",
            "  adding: data/raw/pubmed_pdfs/39341876.pdf (deflated 79%)\n",
            "  adding: data/raw/pubmed_papers.json (deflated 68%)\n",
            "  adding: data/raw/arxiv_papers.json (deflated 70%)\n",
            "  adding: data/raw/arxiv_papers_full.json (deflated 68%)\n",
            "  adding: data/raw/arxiv_pdfs/ (stored 0%)\n",
            "  adding: data/raw/arxiv_pdfs/0904.3664v1.pdf (deflated 18%)\n",
            "  adding: data/raw/arxiv_pdfs/2209.02057v3.pdf (deflated 11%)\n",
            "  adding: data/raw/arxiv_pdfs/2407.05520v1.pdf (deflated 16%)\n",
            "  adding: data/raw/arxiv_pdfs/2407.05526v1.pdf (deflated 19%)\n",
            "  adding: data/raw/arxiv_pdfs/2407.19890v1.pdf (deflated 2%)\n",
            "  adding: data/raw/arxiv_pdfs/1911.06612v1.pdf (deflated 19%)\n",
            "  adding: data/raw/arxiv_pdfs/1810.03548v1.pdf (deflated 35%)\n",
            "  adding: data/raw/arxiv_pdfs/2104.05314v2.pdf (deflated 34%)\n",
            "  adding: data/raw/arxiv_pdfs/1909.01866v1.pdf (deflated 1%)\n",
            "  adding: data/raw/arxiv_pdfs/1612.04858v1.pdf (deflated 5%)\n",
            "  adding: data/raw/arxiv_pdfs/1907.03010v1.pdf (deflated 6%)\n",
            "  adding: data/raw/arxiv_pdfs/2007.07981v1.pdf (deflated 13%)\n",
            "  adding: data/raw/arxiv_pdfs/2003.05155v2.pdf (deflated 12%)\n",
            "  adding: data/raw/arxiv_pdfs/2105.03726v4.pdf (deflated 10%)\n",
            "  adding: data/raw/arxiv_pdfs/2310.11470v1.pdf (deflated 8%)\n",
            "  adding: data/raw/arxiv_pdfs/1501.04309v1.pdf (deflated 11%)\n",
            "  adding: data/raw/arxiv_pdfs/2106.07032v1.pdf (deflated 32%)\n",
            "  adding: data/raw/arxiv_pdfs/1910.12387v2.pdf (deflated 6%)\n",
            "  adding: data/raw/arxiv_pdfs/2306.14624v2.pdf (deflated 34%)\n",
            "  adding: data/raw/arxiv_pdfs/1702.08608v2.pdf (deflated 7%)\n",
            "  adding: data/raw/arxiv_pdfs/2004.05366v2.pdf (deflated 18%)\n",
            "  adding: data/raw/arxiv_pdfs/1802.03830v1.pdf (deflated 10%)\n",
            "  adding: data/raw/arxiv_pdfs/1807.06722v2.pdf (deflated 13%)\n",
            "  adding: data/raw/arxiv_pdfs/2204.07492v2.pdf (deflated 16%)\n",
            "  adding: data/raw/arxiv_pdfs/1908.04710v3.pdf (deflated 7%)\n",
            "  adding: data/raw/arxiv_pdfs/2312.03120v1.pdf (deflated 19%)\n",
            "  adding: data/raw/arxiv_pdfs/2001.04942v2.pdf (deflated 24%)\n",
            "  adding: data/raw/arxiv_pdfs/2401.11351v2.pdf (deflated 16%)\n",
            "  adding: data/raw/arxiv_pdfs/1611.03969v1.pdf (deflated 14%)\n",
            "  adding: data/raw/arxiv_pdfs/2405.03720v1.pdf (deflated 8%)\n",
            "  adding: data/raw/arxiv_pdfs/1908.01262v1.pdf (deflated 8%)\n",
            "  adding: data/raw/arxiv_pdfs/1909.09248v1.pdf (deflated 32%)\n",
            "  adding: data/raw/arxiv_pdfs/2102.05639v1.pdf (deflated 13%)\n",
            "  adding: data/raw/arxiv_pdfs/1405.1304v1.pdf (deflated 25%)\n",
            "  adding: data/raw/arxiv_pdfs/2002.12364v1.pdf (deflated 11%)\n",
            "  adding: data/raw/arxiv_pdfs/1907.08908v1.pdf (deflated 12%)\n",
            "  adding: data/raw/arxiv_pdfs/2103.03122v1.pdf (deflated 11%)\n",
            "  adding: data/raw/arxiv_pdfs/2001.09608v1.pdf (deflated 14%)\n",
            "  adding: data/raw/arxiv_pdfs/2201.06921v1.pdf (deflated 12%)\n",
            "  adding: data/raw/arxiv_pdfs/2007.05479v1.pdf (deflated 19%)\n",
            "  adding: data/raw/arxiv_pdfs/1705.07538v2.pdf (deflated 9%)\n",
            "  adding: data/raw/arxiv_pdfs/1803.10311v2.pdf (deflated 11%)\n",
            "  adding: data/raw/arxiv_pdfs/2009.11087v1.pdf (deflated 18%)\n",
            "  adding: data/raw/arxiv_pdfs/1212.2686v1.pdf (deflated 19%)\n",
            "  adding: data/raw/arxiv_pdfs/1707.04849v1.pdf (deflated 21%)\n",
            "  adding: data/raw/arxiv_pdfs/2012.04105v1.pdf (deflated 14%)\n",
            "  adding: data/raw/arxiv_pdfs/2103.11249v1.pdf (deflated 5%)\n",
            "  adding: data/raw/arxiv_pdfs/1908.00868v2.pdf (deflated 11%)\n",
            "  adding: data/raw/arxiv_pdfs/1909.09246v1.pdf (deflated 15%)\n",
            "  adding: data/raw/arxiv_pdfs/1612.04251v1.pdf (deflated 16%)\n",
            "  adding: data/raw/arxiv_pdfs/2110.12773v1.pdf (deflated 10%)\n",
            "  adding: data/raw/arxiv_pdfs/1911.08587v1.pdf (deflated 1%)\n",
            "  adding: data/raw/arxiv_pdfs/2007.01503v1.pdf (deflated 11%)\n",
            "  adding: data/raw/arxiv_pdfs/2212.12303v1.pdf (deflated 14%)\n",
            "  adding: data/raw/arxiv_pdfs/1510.00633v1.pdf (deflated 39%)\n",
            "  adding: data/raw/arxiv_pdfs/2303.09491v1.pdf (deflated 18%)\n",
            "  adding: data/raw/arxiv_pdfs/1711.06552v1.pdf (deflated 12%)\n",
            "  adding: data/raw/arxiv_pdfs/2108.08712v1.pdf (deflated 13%)\n",
            "  adding: data/raw/arxiv_pdfs/2001.11489v1.pdf (deflated 12%)\n",
            "  adding: data/raw/arxiv_pdfs/2202.10564v1.pdf (deflated 9%)\n",
            "  adding: data/raw/arxiv_pdfs/2312.14050v1.pdf (deflated 16%)\n",
            "  adding: data/raw/arxiv_pdfs/1906.06821v2.pdf (deflated 25%)\n",
            "  adding: data/raw/arxiv_pdfs/1811.04422v1.pdf (deflated 20%)\n",
            "  adding: data/raw/arxiv_pdfs/1707.03184v1.pdf (deflated 8%)\n",
            "  adding: data/raw/arxiv_pdfs/1810.11383v2.pdf (deflated 14%)\n",
            "  adding: data/raw/arxiv_pdfs/1507.02188v1.pdf (deflated 16%)\n",
            "  adding: data/raw/arxiv_pdfs/2108.07915v1.pdf (deflated 25%)\n",
            "  adding: data/raw/arxiv_pdfs/1707.09562v3.pdf (deflated 17%)\n",
            "  adding: data/raw/arxiv_pdfs/2305.15410v1.pdf (deflated 15%)\n",
            "  adding: data/raw/arxiv_pdfs/1812.01410v1.pdf (deflated 9%)\n",
            "  adding: data/raw/arxiv_pdfs/2011.11819v1.pdf (deflated 8%)\n",
            "  adding: data/raw/arxiv_pdfs/1706.08001v1.pdf (deflated 7%)\n",
            "  adding: data/raw/arxiv_pdfs/1605.07805v2.pdf (deflated 17%)\n",
            "  adding: data/raw/arxiv_pdfs/2301.09753v1.pdf (deflated 5%)\n",
            "  adding: data/raw/arxiv_pdfs/1610.08251v1.pdf (deflated 15%)\n",
            "  adding: data/raw/arxiv_pdfs/2007.01977v1.pdf (deflated 12%)\n",
            "  adding: data/raw/arxiv_pdfs/2303.18087v1.pdf (deflated 24%)\n",
            "  adding: data/raw/arxiv_pdfs/1910.02544v1.pdf (deflated 10%)\n",
            "  adding: data/raw/arxiv_pdfs/2206.13446v1.pdf (deflated 18%)\n",
            "  adding: data/raw/arxiv_pdfs/2011.03733v1.pdf (deflated 11%)\n",
            "  adding: data/raw/arxiv_pdfs/1808.00033v3.pdf (deflated 7%)\n",
            "  adding: data/raw/arxiv_pdfs/2006.15680v1.pdf (deflated 32%)\n",
            "  adding: data/raw/arxiv_pdfs/2003.10146v2.pdf (deflated 18%)\n",
            "  adding: data/raw/arxiv_pdfs/1902.04622v1.pdf (deflated 8%)\n",
            "  adding: data/raw/arxiv_pdfs/1603.02185v1.pdf (deflated 18%)\n",
            "  adding: data/raw/arxiv_pdfs/2008.08080v2.pdf (deflated 22%)\n",
            "  adding: data/raw/arxiv_pdfs/2004.00993v2.pdf (deflated 2%)\n",
            "  adding: data/raw/arxiv_pdfs/1911.00776v1.pdf (deflated 25%)\n",
            "  adding: data/raw/arxiv_pdfs/1912.09630v1.pdf (deflated 5%)\n",
            "  adding: data/raw/arxiv_pdfs/1602.00198v1.pdf (deflated 10%)\n",
            "  adding: data/raw/arxiv_pdfs/2103.00742v4.pdf (deflated 19%)\n",
            "  adding: data/raw/arxiv_pdfs/1909.03550v1.pdf (deflated 9%)\n",
            "  adding: data/raw/arxiv_pdfs/1509.00913v3.pdf (deflated 49%)\n",
            "  adding: data/raw/arxiv_pdfs/2409.03632v1.pdf (deflated 18%)\n",
            "  adding: data/raw/arxiv_pdfs/1903.08801v1.pdf (deflated 11%)\n",
            "  adding: data/raw/arxiv_pdfs/1607.01400v1.pdf (deflated 13%)\n",
            "  adding: data/raw/arxiv_pdfs/2007.14206v1.pdf (deflated 7%)\n",
            "  adding: data/processed/ (stored 0%)\n",
            "  adding: data/processed/arxiv_chunks.json (deflated 75%)\n",
            "  adding: models/ (stored 0%)\n",
            "  adding: models/arxiv_faiss.index (deflated 7%)\n",
            "  adding: models/arxiv_embeddings.npy (deflated 7%)\n",
            "  adding: outputs/ (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('rag_pipeline_project.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "e9U7trtYzDiD",
        "outputId": "caeb234c-1b4b-4ce0-c55a-bc232c112ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e5a696e3-60ef-47cd-ba7c-6327819bd576\", \"rag_pipeline_project.zip\", 126771983)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c2_qcg-5zNS3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}